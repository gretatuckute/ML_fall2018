{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "The purpose of this application is to solve relevant classification and regression problems for the prostate dataset for use in the project in 02450 Intro to Machine Learning\n",
    "\n",
    "Author: Naia Wright\n",
    "\n",
    "Reviewed by:  \n",
    "\n",
    "Last modified: 28/10/18, 09:39\n",
    "\n",
    "#### Change-log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import of required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from scipy.linalg import svd\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn.linear_model as lm\n",
    "from matplotlib.pylab import figure, plot, xlabel, ylabel, legend, ylim, show\n",
    "\n",
    "from matplotlib.pyplot import figure, boxplot, xlabel, ylabel, show\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn import model_selection, tree\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import model_selection\n",
    "\n",
    "from matplotlib.pyplot import figure, plot, subplot, title, xlabel, ylabel, show, clim\n",
    "from scipy.io import loadmat\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn import model_selection\n",
    "from toolbox_02450 import feature_selector_lr, bmplot\n",
    "import numpy as np\n",
    "\n",
    "from statistics import mean\n",
    "import graphviz\n",
    "from numpy import array\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a method for importing a spread_sheet using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataLoader(path, sheet):\n",
    "    \"\"\"\n",
    "    Method for importing data from a spreadsheet.\n",
    "\n",
    "    :param path: full path to the spreadsheet to load\n",
    "    :param sheet: name of the sheet in the workbook that is loaded\n",
    "    :return: pandas dataFrame with imported data\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    out = pd.read_excel(path, sheet_name=sheet)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify path and sheet name in the prostate workbook\n",
    "#filePath = 'C:/Users/PeterBakke/Documents/git/ML_fall2018/Data/Prostate.xlsx'\n",
    "#filePath = 'C:/Users/Greta/Documents/Github/ML_fall2018/Data/Prostate.xlsx'\n",
    "filePath = 'C:/Users/narisa/Documents/GitHub/ML_fall2018/Data/Prostate.xlsx'\n",
    "sheet = 'Sheet1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load prostate data into dataFrame\n",
    "myData = DataLoader(path=filePath, sheet=sheet)\n",
    "\n",
    "# delete irrelevant columns\n",
    "del myData['ID']\n",
    "del myData['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lCaVol', 'lWeight', 'Age', 'lBPH', 'SVI', 'lCP', 'Gleason', 'pgg45', 'lPSA']\n",
      "{6: 0, 7: 1, 8: 2, 9: 3}\n"
     ]
    }
   ],
   "source": [
    "# extract class names and encode with integers (dict)\n",
    "\n",
    "classLabels = myData['Gleason'].values.tolist()\n",
    "classNames = sorted(set(classLabels))\n",
    "classDict = dict(zip(classNames, range(4)))\n",
    "\n",
    "#del myData['Gleason']\n",
    "\n",
    "attributeNames = list(myData.columns.values)\n",
    "\n",
    "print(attributeNames)\n",
    "print(classDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract vector y, convert to NumPy array\n",
    "y = np.asarray([classDict[value] for value in classLabels])\n",
    "\n",
    "# Convert dataFrame to numpy array\n",
    "X = myData.values\n",
    "\n",
    "# Compute values of N, M and C\n",
    "N = len(y)\n",
    "M = len(attributeNames)\n",
    "C = len(classNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.64586143 -2.01663373 -1.87210098 -1.03002898 -0.52565748 -0.86765522\n",
      "  -1.04757113 -0.86895727 -2.53331785]\n",
      " [-1.9993129  -0.72575948 -0.79198919 -1.03002898 -0.52565748 -0.86765522\n",
      "  -1.04757113 -0.86895727 -2.29971238]\n",
      " [-1.58702059 -2.20015441  1.36823439 -1.03002898 -0.52565748 -0.86765522\n",
      "   0.34440695 -0.15615511 -2.29971238]\n",
      " [-2.17817387 -0.8121913  -0.79198919 -1.03002898 -0.52565748 -0.86765522\n",
      "  -1.04757113 -0.86895727 -2.29971238]\n",
      " [-0.5105128  -0.46121762 -0.25193329 -1.03002898 -0.52565748 -0.86765522\n",
      "  -1.04757113 -0.86895727 -1.83463099]\n",
      " [-2.04670586 -0.93880639 -1.87210098 -1.03002898 -0.52565748 -0.86765522\n",
      "  -1.04757113 -0.86895727 -1.49161747]\n",
      " [-0.5226677  -0.3646778   0.01809466  0.35670122 -0.52565748 -0.86765522\n",
      "  -1.04757113 -0.86895727 -1.49161747]\n",
      " [-0.56020767 -0.20984103 -0.79198919  0.99529051 -0.52565748 -0.86765522\n",
      "  -1.04757113 -0.86895727 -1.4141616 ]\n",
      " [-1.81362657 -0.20984103 -2.2771429  -1.03002898 -0.52565748 -0.86765522\n",
      "  -1.04757113 -0.86895727 -1.24618021]\n",
      " [-0.9610521  -0.90192675 -0.11691932 -1.03002898 -0.52565748 -0.86765522\n",
      "  -1.04757113 -0.86895727 -1.24618021]\n",
      " [-0.93418834 -0.05819996  0.15310863 -1.03002898 -0.52565748 -0.86765522\n",
      "  -1.04757113 -0.86895727 -1.05492666]\n",
      " [-2.30021799 -0.07100389 -0.11691932  0.80827605 -0.52565748 -0.86765522\n",
      "  -1.04757113 -0.86895727 -1.05492666]\n",
      " [ 0.22465908 -1.4220686  -0.11691932 -1.03002898 -0.52565748 -0.30083707\n",
      "   0.34440695  0.20024597 -1.05492666]\n",
      " [ 0.10834583 -1.47986344  0.42313658 -1.03002898 -0.52565748 -0.86765522\n",
      "   0.34440695 -0.69075673 -0.98428221]\n",
      " [-0.12284403 -0.4385849  -0.92700316 -1.03002898 -0.52565748 -0.1807427\n",
      "   0.34440695 -0.69075673 -0.94018137]\n",
      " [ 0.16302259 -1.33245984  0.2881226  -1.03002898 -0.52565748 -0.86765522\n",
      "  -1.04757113 -0.86895727 -0.89820677]\n",
      " [-1.50573493 -0.26497044  0.8281785   0.79248386 -0.52565748 -0.30083707\n",
      "   0.34440695  0.20024597 -0.87795465]\n",
      " [ 0.80038343  0.04790351  0.2881226  -1.03002898 -0.52565748  0.39606027\n",
      "  -1.04757113 -0.86895727 -0.85816274]\n",
      " [-1.63076627 -0.84767487 -3.08722674 -1.03002898 -0.52565748 -0.86765522\n",
      "  -1.04757113 -0.86895727 -0.80135103]\n",
      " [-0.9958673   0.46089542  0.8281785   1.07937567 -0.52565748 -0.86765522\n",
      "  -1.04757113 -0.86895727 -0.76543644]\n",
      " [-0.17279428 -0.4917387  -0.65697521 -1.03002898 -0.52565748 -0.86765522\n",
      "  -1.04757113 -0.86895727 -0.73094466]\n",
      " [ 0.60486894 -0.30009502 -0.52196124  0.95226146 -0.52565748  1.09806826\n",
      "   0.34440695 -0.15615511 -0.71419787]\n",
      " [-1.61593366 -0.59376893 -0.65697521 -0.62277959 -0.52565748 -0.86765522\n",
      "  -1.04757113 -0.86895727 -0.68164068]\n",
      " [ 0.36817665 -0.4161657  -0.11691932  0.23411435 -0.52565748  0.97627438\n",
      "   0.34440695  1.26944921 -0.66580745]\n",
      " [-0.82278841  0.09023368  0.69316453  1.03860789 -0.52565748 -0.86765522\n",
      "  -1.04757113 -0.86895727 -0.65025697]\n",
      " [ 0.08264956 -1.18343728  0.55815055  0.13839656 -0.52565748 -0.86765522\n",
      "  -1.04757113 -0.86895727 -0.619965  ]\n",
      " [-0.71399732  0.21283185  0.15310863 -1.03002898 -0.52565748 -0.44509826\n",
      "   0.34440695  1.62585029 -0.59069151]\n",
      " [-1.49290981  0.55616587  0.42313658  1.18900155 -0.52565748 -0.86765522\n",
      "   0.34440695 -0.15615511 -0.57641572]\n",
      " [-0.26415689 -1.17314627  0.42313658  0.08507392 -0.52565748  0.16402005\n",
      "   0.34440695  1.98225137 -0.54854763]\n",
      " [ 0.9037135  -0.59376893  0.15310863 -1.03002898 -0.52565748  1.29311536\n",
      "  -1.04757113 -0.86895727 -0.50834947]\n",
      " [-0.90814498  1.08218997  0.15310863  1.29047369 -0.52565748 -0.44509826\n",
      "  -1.04757113 -0.86895727 -0.48254597]\n",
      " [-0.9958673   0.41177028  0.15310863  1.11160717 -0.52565748 -0.86765522\n",
      "  -1.04757113 -0.86895727 -0.40942861]\n",
      " [-0.0636628  -1.38806321  0.96319247  0.80827605 -0.52565748 -0.86765522\n",
      "  -1.04757113 -0.86895727 -0.40942861]\n",
      " [-1.14287478 -0.84767487 -1.33204508 -1.03002898 -0.52565748 -0.86765522\n",
      "  -1.04757113 -0.86895727 -0.39781765]\n",
      " [-1.15993242 -0.96684975 -0.11691932 -1.03002898 -0.52565748 -0.44509826\n",
      "  -1.04757113 -0.86895727 -0.3750503 ]\n",
      " [-0.03554419  1.15183144  0.01809466  1.43488428 -0.52565748 -0.86765522\n",
      "   0.34440695 -0.69075673 -0.34197776]\n",
      " [ 0.06234256  0.0661392   1.23322042 -0.47126026 -0.52565748  1.32103713\n",
      "   1.73638502 -0.33435565 -0.27937807]\n",
      " [-0.76124438 -2.94238594  0.01809466 -1.03002898 -0.52565748 -0.86765522\n",
      "   0.34440695 -0.33435565 -0.24968869]\n",
      " [ 1.118048    1.07038088  0.55815055  0.8822505   1.90237946  1.44637892\n",
      "   0.34440695  0.37844651 -0.23044356]\n",
      " [-0.47120382 -1.44501572 -1.06201713  0.5790429  -0.52565748  0.01211097\n",
      "   0.34440695 -0.69075673 -0.17513581]\n",
      " [-0.62209987 -1.14254072 -0.52196124 -1.03002898 -0.52565748 -0.86765522\n",
      "   3.1283631   1.98225137 -0.15745387]\n",
      " [ 0.07862666  0.12592138  0.55815055 -1.03002898 -0.52565748 -0.86765522\n",
      "   0.34440695 -0.51255619 -0.14874583]\n",
      " [-0.65481608  0.55616587 -0.25193329  1.11787737 -0.52565748 -0.1807427\n",
      "  -1.04757113 -0.86895727 -0.13158654]\n",
      " [ 0.35951816  0.62873791 -0.38694727 -1.03002898 -0.52565748  0.71191882\n",
      "   0.34440695 -0.65511662 -0.09011178]\n",
      " [ 0.1160991  -0.51489465  0.2881226   1.14240568 -0.52565748 -0.1807427\n",
      "   0.34440695 -0.15615511  0.0377352 ]\n",
      " [ 0.26772493 -0.55400096 -0.38694727  0.35670122 -0.52565748 -0.86765522\n",
      "   0.34440695 -0.33435565  0.06527282]\n",
      " [ 1.17509901  0.85993605  2.04330426  1.23266023  1.90237946  2.03887464\n",
      "   3.1283631   2.69505353  0.07872178]\n",
      " [-0.15936323  0.95303849  0.55815055  1.11787737 -0.52565748 -0.1807427\n",
      "   0.34440695  0.55664705  0.07872178]\n",
      " [ 0.33747937 -0.30718329 -2.81719879 -1.03002898 -0.52565748 -0.86765522\n",
      "  -1.04757113 -0.86895727  0.09851369]\n",
      " [-0.11017138 -0.14270309  0.8281785   0.8822505  -0.52565748 -0.44509826\n",
      "  -1.04757113 -0.86895727  0.09851369]\n",
      " [-0.22010989  0.85561411  0.55815055 -1.03002898 -0.52565748 -0.86765522\n",
      "   0.34440695  0.91304813  0.1553254 ]\n",
      " [ 0.26448829  1.421615    0.01809466  1.36687051 -0.52565748 -0.86765522\n",
      "  -1.04757113 -0.86895727  0.17346783]\n",
      " [-0.71399732  0.0110004   0.01809466  0.96483055 -0.52565748  0.16402005\n",
      "   0.34440695  1.62585029  0.17943223]\n",
      " [ 0.66269388  1.15563954  0.55815055  1.15435171 -0.52565748  1.16912805\n",
      "   0.34440695  0.55664705  0.18535613]\n",
      " [ 1.53819093 -0.26497044 -0.65697521 -1.03002898 -0.52565748 -0.86765522\n",
      "   0.34440695 -0.69075673  0.1970843 ]\n",
      " [-0.07083973  1.52790617  0.2881226   1.40088236 -0.52565748 -0.86765522\n",
      "   0.34440695 -0.33435565  0.2086566 ]\n",
      " [-0.32020395 -1.79233616 -2.2771429  -1.03002898 -0.52565748  0.48894996\n",
      "   0.34440695 -0.72639684  0.26969337]\n",
      " [-0.75586358  0.31848951 -2.00711495  0.91647239 -0.52565748 -0.86765522\n",
      "  -1.04757113 -0.86895727  0.27503575]\n",
      " [-0.68883756  1.2888009   0.8281785   0.23411435 -0.52565748 -0.86765522\n",
      "   0.34440695 -0.15615511  0.28562317]\n",
      " [-0.24626419  0.52151525 -0.38694727  0.82752319 -0.52565748 -0.86765522\n",
      "   0.34440695  0.55664705  0.29086898]\n",
      " [-0.76124438  2.10127925  1.23322042  1.54225202 -0.52565748 -0.86765522\n",
      "  -1.04757113 -0.86895727  0.31663434]\n",
      " [ 0.55214455  0.21283185 -0.11691932  1.05246539  1.90237946  1.50170584\n",
      "   0.34440695  0.55664705  0.32673071]\n",
      " [ 1.2159132  -0.2441444   1.09820645 -1.03002898 -0.52565748  1.24908762\n",
      "   3.1283631   2.51685299  0.32673071]\n",
      " [ 0.58394572  0.67590387  0.2881226   1.32186427  1.90237946  1.64596703\n",
      "   0.34440695  1.26944921  0.35147113]\n",
      " [ 0.61675184 -0.01392703  0.01809466 -1.03002898 -0.52565748 -0.86765522\n",
      "  -1.04757113 -0.86895727  0.35147113]\n",
      " [ 0.09262458  0.48634373 -0.38694727  0.84625007 -0.52565748 -0.1807427\n",
      "   0.34440695 -0.15615511  0.35633597]\n",
      " [ 0.57385266  0.58546452  0.55815055  1.16609525 -0.52565748  1.07914887\n",
      "   0.34440695  1.62585029  0.38496775]\n",
      " [ 0.72349772  0.99008707  1.09820645  1.52927559 -0.52565748 -0.1807427\n",
      "   0.34440695 -0.51255619  0.42173538]\n",
      " [-1.53197866  1.82921036  0.69316453 -1.03002898 -0.52565748 -0.86765522\n",
      "  -1.04757113 -0.86895727  0.42173538]\n",
      " [-0.1331195   2.70166094  1.09820645  1.54225202 -0.52565748 -0.44509826\n",
      "   0.34440695 -0.69075673  0.43068977]\n",
      " [ 0.43842708 -0.08387821 -0.52196124 -1.03002898  1.90237946  1.07914887\n",
      "   0.34440695  1.26944921  0.46561391]\n",
      " [-0.16203258 -0.67539077  1.77327632  1.14240568 -0.52565748 -0.86765522\n",
      "   0.34440695  0.02204543  0.48675094]\n",
      " [-0.11521787  0.46089542  0.69316453 -1.03002898  1.90237946  0.28936185\n",
      "   0.34440695 -0.15615511  0.50329884]\n",
      " [ 0.41700419 -0.92029384 -0.52196124  0.23411435  1.90237946  0.97627438\n",
      "   3.1283631   2.33865245  0.51953812]\n",
      " [ 1.40654082  0.51652225  0.69316453 -1.03002898  1.90237946  1.50170584\n",
      "   0.34440695 -0.15615511  0.69391731]\n",
      " [ 1.52756447 -0.85663082  0.55815055 -0.1050703   1.90237946  1.8689359\n",
      "   0.34440695  0.91304813  0.74816076]\n",
      " [ 0.56363872  1.88843646  1.09820645  1.40088236 -0.52565748  0.48894996\n",
      "   0.34440695  1.26944921  0.79630031]\n",
      " [ 1.01288993  1.70306453  1.90829029  1.54225202 -0.52565748 -0.86765522\n",
      "   0.34440695 -0.51255619  0.83354436]\n",
      " [ 1.10725223 -0.10984037  0.69316453 -1.03002898  1.90237946  1.98656829\n",
      "   0.34440695  1.62585029  0.85295798]\n",
      " [ 1.2190955   0.45577338 -0.11691932 -1.03002898 -0.52565748  0.39606027\n",
      "   0.34440695  0.91304813  0.90097779]\n",
      " [ 0.10052143 -1.31058265  0.2881226   0.31819952 -0.52565748  0.28936185\n",
      "   0.34440695  0.55664705  0.90356948]\n",
      " [ 0.99242046 -0.3646778  -0.92700316  0.23411435 -0.52565748  1.80201365\n",
      "   0.34440695  1.26944921  0.91641341]\n",
      " [ 1.077152    0.60960358  1.77327632 -0.43510323  1.90237946  0.5312501\n",
      "   0.34440695  0.20024597  0.94648734]\n",
      " [ 1.132233    0.49140008  0.15310863  0.7030969  -0.52565748  1.38643629\n",
      "   3.1283631   1.62585029  0.95140024]\n",
      " [ 0.18109221  0.1899692  -0.52196124  1.10527971 -0.52565748  0.71191882\n",
      "   0.34440695  0.20024597  0.96597463]\n",
      " [ 1.66548696 -0.25800887  0.01809466 -1.03002898  1.90237946  1.80201365\n",
      "   0.34440695  1.26944921  1.00368795]\n",
      " [ 0.57498003  0.24110046 -0.79198919  1.06605117 -0.52565748 -0.86765522\n",
      "  -1.04757113 -0.86895727  1.04644915]\n",
      " [ 0.32548825 -0.60986946 -0.25193329 -1.03002898  1.90237946  0.34468877\n",
      "   0.34440695  0.20024597  1.07454209]\n",
      " [ 1.24310643  2.55541174  0.15310863 -1.03002898  1.90237946  1.90019713\n",
      "   0.34440695  1.26944921  1.31139383]\n",
      " [ 0.18109221  0.15525053  1.63826234  0.5790429   1.90237946  0.71191882\n",
      "   0.34440695  1.80405083  1.31945687]\n",
      " [ 1.61742159  1.10952004  0.55815055 -1.03002898 -0.52565748 -0.86765522\n",
      "  -1.04757113 -0.86895727  1.3509826 ]\n",
      " [ 1.00883515  0.11408648 -0.38694727  0.86448408  1.90237946 -0.86765522\n",
      "   0.34440695 -0.33435565  1.43784081]\n",
      " [ 1.26244405  0.58060761  0.55815055 -1.03002898  1.90237946  1.07914887\n",
      "   0.34440695  1.26944921  1.66041493]\n",
      " [ 2.10739693  0.62873791 -2.68218482 -1.03002898  1.90237946  1.68826718\n",
      "   0.34440695  0.55664705  1.92104373]\n",
      " [ 1.3282669  -0.54612667 -1.60207303 -1.03002898  1.90237946  1.90019713\n",
      "   0.34440695 -0.51255619  2.32046525]\n",
      " [ 1.30704467  0.34014146  0.55815055  1.0100326   1.90237946  1.24908762\n",
      "   0.34440695  1.98225137  2.61164875]\n",
      " [ 1.80971922  0.81196061  0.55815055  0.23411435  1.90237946  2.21673517\n",
      "   0.34440695 -0.15615511  2.70345173]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize data with mean and std\n",
    "Y = (X - np.ones((N,1))*X.mean(axis=0)) / X.std(axis=0)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.64586143 -2.01663373 -1.87210098 -1.03002898 -0.86765522 -1.04757113\n",
      "  -0.86895727 -2.53331785]\n",
      " [-1.9993129  -0.72575948 -0.79198919 -1.03002898 -0.86765522 -1.04757113\n",
      "  -0.86895727 -2.29971238]\n",
      " [-1.58702059 -2.20015441  1.36823439 -1.03002898 -0.86765522  0.34440695\n",
      "  -0.15615511 -2.29971238]\n",
      " [-2.17817387 -0.8121913  -0.79198919 -1.03002898 -0.86765522 -1.04757113\n",
      "  -0.86895727 -2.29971238]\n",
      " [-0.5105128  -0.46121762 -0.25193329 -1.03002898 -0.86765522 -1.04757113\n",
      "  -0.86895727 -1.83463099]\n",
      " [-2.04670586 -0.93880639 -1.87210098 -1.03002898 -0.86765522 -1.04757113\n",
      "  -0.86895727 -1.49161747]\n",
      " [-0.5226677  -0.3646778   0.01809466  0.35670122 -0.86765522 -1.04757113\n",
      "  -0.86895727 -1.49161747]\n",
      " [-0.56020767 -0.20984103 -0.79198919  0.99529051 -0.86765522 -1.04757113\n",
      "  -0.86895727 -1.4141616 ]\n",
      " [-1.81362657 -0.20984103 -2.2771429  -1.03002898 -0.86765522 -1.04757113\n",
      "  -0.86895727 -1.24618021]\n",
      " [-0.9610521  -0.90192675 -0.11691932 -1.03002898 -0.86765522 -1.04757113\n",
      "  -0.86895727 -1.24618021]\n",
      " [-0.93418834 -0.05819996  0.15310863 -1.03002898 -0.86765522 -1.04757113\n",
      "  -0.86895727 -1.05492666]\n",
      " [-2.30021799 -0.07100389 -0.11691932  0.80827605 -0.86765522 -1.04757113\n",
      "  -0.86895727 -1.05492666]\n",
      " [ 0.22465908 -1.4220686  -0.11691932 -1.03002898 -0.30083707  0.34440695\n",
      "   0.20024597 -1.05492666]\n",
      " [ 0.10834583 -1.47986344  0.42313658 -1.03002898 -0.86765522  0.34440695\n",
      "  -0.69075673 -0.98428221]\n",
      " [-0.12284403 -0.4385849  -0.92700316 -1.03002898 -0.1807427   0.34440695\n",
      "  -0.69075673 -0.94018137]\n",
      " [ 0.16302259 -1.33245984  0.2881226  -1.03002898 -0.86765522 -1.04757113\n",
      "  -0.86895727 -0.89820677]\n",
      " [-1.50573493 -0.26497044  0.8281785   0.79248386 -0.30083707  0.34440695\n",
      "   0.20024597 -0.87795465]\n",
      " [ 0.80038343  0.04790351  0.2881226  -1.03002898  0.39606027 -1.04757113\n",
      "  -0.86895727 -0.85816274]\n",
      " [-1.63076627 -0.84767487 -3.08722674 -1.03002898 -0.86765522 -1.04757113\n",
      "  -0.86895727 -0.80135103]\n",
      " [-0.9958673   0.46089542  0.8281785   1.07937567 -0.86765522 -1.04757113\n",
      "  -0.86895727 -0.76543644]\n",
      " [-0.17279428 -0.4917387  -0.65697521 -1.03002898 -0.86765522 -1.04757113\n",
      "  -0.86895727 -0.73094466]\n",
      " [ 0.60486894 -0.30009502 -0.52196124  0.95226146  1.09806826  0.34440695\n",
      "  -0.15615511 -0.71419787]\n",
      " [-1.61593366 -0.59376893 -0.65697521 -0.62277959 -0.86765522 -1.04757113\n",
      "  -0.86895727 -0.68164068]\n",
      " [ 0.36817665 -0.4161657  -0.11691932  0.23411435  0.97627438  0.34440695\n",
      "   1.26944921 -0.66580745]\n",
      " [-0.82278841  0.09023368  0.69316453  1.03860789 -0.86765522 -1.04757113\n",
      "  -0.86895727 -0.65025697]\n",
      " [ 0.08264956 -1.18343728  0.55815055  0.13839656 -0.86765522 -1.04757113\n",
      "  -0.86895727 -0.619965  ]\n",
      " [-0.71399732  0.21283185  0.15310863 -1.03002898 -0.44509826  0.34440695\n",
      "   1.62585029 -0.59069151]\n",
      " [-1.49290981  0.55616587  0.42313658  1.18900155 -0.86765522  0.34440695\n",
      "  -0.15615511 -0.57641572]\n",
      " [-0.26415689 -1.17314627  0.42313658  0.08507392  0.16402005  0.34440695\n",
      "   1.98225137 -0.54854763]\n",
      " [ 0.9037135  -0.59376893  0.15310863 -1.03002898  1.29311536 -1.04757113\n",
      "  -0.86895727 -0.50834947]\n",
      " [-0.90814498  1.08218997  0.15310863  1.29047369 -0.44509826 -1.04757113\n",
      "  -0.86895727 -0.48254597]\n",
      " [-0.9958673   0.41177028  0.15310863  1.11160717 -0.86765522 -1.04757113\n",
      "  -0.86895727 -0.40942861]\n",
      " [-0.0636628  -1.38806321  0.96319247  0.80827605 -0.86765522 -1.04757113\n",
      "  -0.86895727 -0.40942861]\n",
      " [-1.14287478 -0.84767487 -1.33204508 -1.03002898 -0.86765522 -1.04757113\n",
      "  -0.86895727 -0.39781765]\n",
      " [-1.15993242 -0.96684975 -0.11691932 -1.03002898 -0.44509826 -1.04757113\n",
      "  -0.86895727 -0.3750503 ]\n",
      " [-0.03554419  1.15183144  0.01809466  1.43488428 -0.86765522  0.34440695\n",
      "  -0.69075673 -0.34197776]\n",
      " [ 0.06234256  0.0661392   1.23322042 -0.47126026  1.32103713  1.73638502\n",
      "  -0.33435565 -0.27937807]\n",
      " [-0.76124438 -2.94238594  0.01809466 -1.03002898 -0.86765522  0.34440695\n",
      "  -0.33435565 -0.24968869]\n",
      " [ 1.118048    1.07038088  0.55815055  0.8822505   1.44637892  0.34440695\n",
      "   0.37844651 -0.23044356]\n",
      " [-0.47120382 -1.44501572 -1.06201713  0.5790429   0.01211097  0.34440695\n",
      "  -0.69075673 -0.17513581]\n",
      " [-0.62209987 -1.14254072 -0.52196124 -1.03002898 -0.86765522  3.1283631\n",
      "   1.98225137 -0.15745387]\n",
      " [ 0.07862666  0.12592138  0.55815055 -1.03002898 -0.86765522  0.34440695\n",
      "  -0.51255619 -0.14874583]\n",
      " [-0.65481608  0.55616587 -0.25193329  1.11787737 -0.1807427  -1.04757113\n",
      "  -0.86895727 -0.13158654]\n",
      " [ 0.35951816  0.62873791 -0.38694727 -1.03002898  0.71191882  0.34440695\n",
      "  -0.65511662 -0.09011178]\n",
      " [ 0.1160991  -0.51489465  0.2881226   1.14240568 -0.1807427   0.34440695\n",
      "  -0.15615511  0.0377352 ]\n",
      " [ 0.26772493 -0.55400096 -0.38694727  0.35670122 -0.86765522  0.34440695\n",
      "  -0.33435565  0.06527282]\n",
      " [ 1.17509901  0.85993605  2.04330426  1.23266023  2.03887464  3.1283631\n",
      "   2.69505353  0.07872178]\n",
      " [-0.15936323  0.95303849  0.55815055  1.11787737 -0.1807427   0.34440695\n",
      "   0.55664705  0.07872178]\n",
      " [ 0.33747937 -0.30718329 -2.81719879 -1.03002898 -0.86765522 -1.04757113\n",
      "  -0.86895727  0.09851369]\n",
      " [-0.11017138 -0.14270309  0.8281785   0.8822505  -0.44509826 -1.04757113\n",
      "  -0.86895727  0.09851369]\n",
      " [-0.22010989  0.85561411  0.55815055 -1.03002898 -0.86765522  0.34440695\n",
      "   0.91304813  0.1553254 ]\n",
      " [ 0.26448829  1.421615    0.01809466  1.36687051 -0.86765522 -1.04757113\n",
      "  -0.86895727  0.17346783]\n",
      " [-0.71399732  0.0110004   0.01809466  0.96483055  0.16402005  0.34440695\n",
      "   1.62585029  0.17943223]\n",
      " [ 0.66269388  1.15563954  0.55815055  1.15435171  1.16912805  0.34440695\n",
      "   0.55664705  0.18535613]\n",
      " [ 1.53819093 -0.26497044 -0.65697521 -1.03002898 -0.86765522  0.34440695\n",
      "  -0.69075673  0.1970843 ]\n",
      " [-0.07083973  1.52790617  0.2881226   1.40088236 -0.86765522  0.34440695\n",
      "  -0.33435565  0.2086566 ]\n",
      " [-0.32020395 -1.79233616 -2.2771429  -1.03002898  0.48894996  0.34440695\n",
      "  -0.72639684  0.26969337]\n",
      " [-0.75586358  0.31848951 -2.00711495  0.91647239 -0.86765522 -1.04757113\n",
      "  -0.86895727  0.27503575]\n",
      " [-0.68883756  1.2888009   0.8281785   0.23411435 -0.86765522  0.34440695\n",
      "  -0.15615511  0.28562317]\n",
      " [-0.24626419  0.52151525 -0.38694727  0.82752319 -0.86765522  0.34440695\n",
      "   0.55664705  0.29086898]\n",
      " [-0.76124438  2.10127925  1.23322042  1.54225202 -0.86765522 -1.04757113\n",
      "  -0.86895727  0.31663434]\n",
      " [ 0.55214455  0.21283185 -0.11691932  1.05246539  1.50170584  0.34440695\n",
      "   0.55664705  0.32673071]\n",
      " [ 1.2159132  -0.2441444   1.09820645 -1.03002898  1.24908762  3.1283631\n",
      "   2.51685299  0.32673071]\n",
      " [ 0.58394572  0.67590387  0.2881226   1.32186427  1.64596703  0.34440695\n",
      "   1.26944921  0.35147113]\n",
      " [ 0.61675184 -0.01392703  0.01809466 -1.03002898 -0.86765522 -1.04757113\n",
      "  -0.86895727  0.35147113]\n",
      " [ 0.09262458  0.48634373 -0.38694727  0.84625007 -0.1807427   0.34440695\n",
      "  -0.15615511  0.35633597]\n",
      " [ 0.57385266  0.58546452  0.55815055  1.16609525  1.07914887  0.34440695\n",
      "   1.62585029  0.38496775]\n",
      " [ 0.72349772  0.99008707  1.09820645  1.52927559 -0.1807427   0.34440695\n",
      "  -0.51255619  0.42173538]\n",
      " [-1.53197866  1.82921036  0.69316453 -1.03002898 -0.86765522 -1.04757113\n",
      "  -0.86895727  0.42173538]\n",
      " [-0.1331195   2.70166094  1.09820645  1.54225202 -0.44509826  0.34440695\n",
      "  -0.69075673  0.43068977]\n",
      " [ 0.43842708 -0.08387821 -0.52196124 -1.03002898  1.07914887  0.34440695\n",
      "   1.26944921  0.46561391]\n",
      " [-0.16203258 -0.67539077  1.77327632  1.14240568 -0.86765522  0.34440695\n",
      "   0.02204543  0.48675094]\n",
      " [-0.11521787  0.46089542  0.69316453 -1.03002898  0.28936185  0.34440695\n",
      "  -0.15615511  0.50329884]\n",
      " [ 0.41700419 -0.92029384 -0.52196124  0.23411435  0.97627438  3.1283631\n",
      "   2.33865245  0.51953812]\n",
      " [ 1.40654082  0.51652225  0.69316453 -1.03002898  1.50170584  0.34440695\n",
      "  -0.15615511  0.69391731]\n",
      " [ 1.52756447 -0.85663082  0.55815055 -0.1050703   1.8689359   0.34440695\n",
      "   0.91304813  0.74816076]\n",
      " [ 0.56363872  1.88843646  1.09820645  1.40088236  0.48894996  0.34440695\n",
      "   1.26944921  0.79630031]\n",
      " [ 1.01288993  1.70306453  1.90829029  1.54225202 -0.86765522  0.34440695\n",
      "  -0.51255619  0.83354436]\n",
      " [ 1.10725223 -0.10984037  0.69316453 -1.03002898  1.98656829  0.34440695\n",
      "   1.62585029  0.85295798]\n",
      " [ 1.2190955   0.45577338 -0.11691932 -1.03002898  0.39606027  0.34440695\n",
      "   0.91304813  0.90097779]\n",
      " [ 0.10052143 -1.31058265  0.2881226   0.31819952  0.28936185  0.34440695\n",
      "   0.55664705  0.90356948]\n",
      " [ 0.99242046 -0.3646778  -0.92700316  0.23411435  1.80201365  0.34440695\n",
      "   1.26944921  0.91641341]\n",
      " [ 1.077152    0.60960358  1.77327632 -0.43510323  0.5312501   0.34440695\n",
      "   0.20024597  0.94648734]\n",
      " [ 1.132233    0.49140008  0.15310863  0.7030969   1.38643629  3.1283631\n",
      "   1.62585029  0.95140024]\n",
      " [ 0.18109221  0.1899692  -0.52196124  1.10527971  0.71191882  0.34440695\n",
      "   0.20024597  0.96597463]\n",
      " [ 1.66548696 -0.25800887  0.01809466 -1.03002898  1.80201365  0.34440695\n",
      "   1.26944921  1.00368795]\n",
      " [ 0.57498003  0.24110046 -0.79198919  1.06605117 -0.86765522 -1.04757113\n",
      "  -0.86895727  1.04644915]\n",
      " [ 0.32548825 -0.60986946 -0.25193329 -1.03002898  0.34468877  0.34440695\n",
      "   0.20024597  1.07454209]\n",
      " [ 1.24310643  2.55541174  0.15310863 -1.03002898  1.90019713  0.34440695\n",
      "   1.26944921  1.31139383]\n",
      " [ 0.18109221  0.15525053  1.63826234  0.5790429   0.71191882  0.34440695\n",
      "   1.80405083  1.31945687]\n",
      " [ 1.61742159  1.10952004  0.55815055 -1.03002898 -0.86765522 -1.04757113\n",
      "  -0.86895727  1.3509826 ]\n",
      " [ 1.00883515  0.11408648 -0.38694727  0.86448408 -0.86765522  0.34440695\n",
      "  -0.33435565  1.43784081]\n",
      " [ 1.26244405  0.58060761  0.55815055 -1.03002898  1.07914887  0.34440695\n",
      "   1.26944921  1.66041493]\n",
      " [ 2.10739693  0.62873791 -2.68218482 -1.03002898  1.68826718  0.34440695\n",
      "   0.55664705  1.92104373]\n",
      " [ 1.3282669  -0.54612667 -1.60207303 -1.03002898  1.90019713  0.34440695\n",
      "  -0.51255619  2.32046525]\n",
      " [ 1.30704467  0.34014146  0.55815055  1.0100326   1.24908762  0.34440695\n",
      "   1.98225137  2.61164875]\n",
      " [ 1.80971922  0.81196061  0.55815055  0.23411435  2.21673517  0.34440695\n",
      "  -0.15615511  2.70345173]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1.]\n",
      "['lCaVol' 'lWeight' 'Age' 'lBPH' 'lCP' 'Gleason' 'pgg45' 'lPSA']\n"
     ]
    }
   ],
   "source": [
    "# Remove attribute 5 (SVI) from X\n",
    "X_classification = Y[:,[0,1,2,3,5,6,7,8]]\n",
    "print(X_classification)\n",
    "# Use attribute 5 (SVI) as y\n",
    "y_classification = X[:,4]\n",
    "print(y_classification)\n",
    "# Remove attribute 5 (SVI) from attribute names\n",
    "\n",
    "attributeNames_classification = np.array(attributeNames)[[0, 1, 2, 3, 5, 6, 7, 8]]\n",
    "print(attributeNames_classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two level cross validation for KNN - Naia 2018-11-03 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV-fold 1 of 5\n",
      "Inner CV-fold 1 of 5\n",
      "Inner CV-fold 2 of 5\n",
      "Inner CV-fold 3 of 5\n",
      "Inner CV-fold 4 of 5\n",
      "Inner CV-fold 5 of 5\n",
      "Inner_error_values are:[18.083333333333332, 22.166666666666668, 22.083333333333336, 18.166666666666668, 18.166666666666668, 17.0, 13.25, 13.25, 13.25, 15.833333333333334, 14.5, 15.833333333333334, 15.75, 13.083333333333334, 15.75, 14.333333333333334, 16.916666666666668, 16.916666666666668, 16.916666666666668, 19.583333333333332, 19.583333333333332, 19.583333333333332, 19.583333333333332, 19.583333333333332, 19.583333333333332, 20.916666666666668, 20.916666666666668, 20.916666666666668, 20.916666666666668, 20.916666666666668, 20.916666666666668, 22.166666666666668, 22.166666666666668, 22.166666666666668, 22.166666666666668, 22.166666666666668, 22.166666666666668, 22.166666666666668, 22.166666666666668, 22.166666666666668]\n",
      "The index of optimal KNN value is: 13\n",
      "The optimal KNN value across inner CV folds is: 14\n",
      "Errors for each outer CV fold: [10.0]\n",
      "CV-fold 2 of 5\n",
      "Inner CV-fold 1 of 5\n",
      "Inner CV-fold 2 of 5\n",
      "Inner CV-fold 3 of 5\n",
      "Inner CV-fold 4 of 5\n",
      "Inner CV-fold 5 of 5\n",
      "Inner_error_values are:[20.75, 15.583333333333334, 18.0, 15.5, 16.833333333333332, 15.5, 12.833333333333334, 12.916666666666668, 13.0, 15.583333333333334, 16.833333333333332, 14.333333333333334, 15.583333333333334, 13.0, 15.5, 15.5, 15.583333333333334, 19.416666666666668, 18.083333333333332, 20.75, 19.416666666666668, 22.083333333333336, 22.083333333333336, 22.166666666666668, 22.166666666666668, 20.916666666666668, 20.916666666666668, 21.0, 22.25, 22.25, 22.25, 22.25, 22.25, 22.25, 22.25, 22.25, 22.25, 22.25, 22.25, 22.25]\n",
      "The index of optimal KNN value is: 6\n",
      "The optimal KNN value across inner CV folds is: 7\n",
      "Errors for each outer CV fold: [10.0, 15.0]\n",
      "CV-fold 3 of 5\n",
      "Inner CV-fold 1 of 5\n",
      "Inner CV-fold 2 of 5\n",
      "Inner CV-fold 3 of 5\n",
      "Inner CV-fold 4 of 5\n",
      "Inner CV-fold 5 of 5\n",
      "Inner_error_values are:[22.833333333333332, 17.916666666666668, 14.0, 12.75, 13.916666666666666, 11.416666666666666, 10.166666666666666, 10.166666666666666, 10.166666666666666, 14.0, 12.666666666666666, 12.666666666666666, 12.666666666666666, 13.916666666666666, 13.916666666666666, 13.916666666666666, 13.916666666666666, 12.666666666666666, 13.916666666666666, 19.166666666666668, 17.833333333333332, 17.833333333333332, 19.083333333333332, 20.5, 20.5, 20.5, 20.5, 21.833333333333332, 21.833333333333332, 21.833333333333332, 21.833333333333332, 21.833333333333332, 21.833333333333332, 21.833333333333332, 21.833333333333332, 21.833333333333332, 21.833333333333332, 21.833333333333332, 21.833333333333332, 21.833333333333332]\n",
      "The index of optimal KNN value is: 6\n",
      "The optimal KNN value across inner CV folds is: 7\n",
      "Errors for each outer CV fold: [10.0, 15.0, 21.05263157894737]\n",
      "CV-fold 4 of 5\n",
      "Inner CV-fold 1 of 5\n",
      "Inner CV-fold 2 of 5\n",
      "Inner CV-fold 3 of 5\n",
      "Inner CV-fold 4 of 5\n",
      "Inner CV-fold 5 of 5\n",
      "Inner_error_values are:[20.5, 20.416666666666668, 12.75, 15.333333333333334, 15.25, 12.75, 14.0, 15.25, 15.25, 16.5, 16.583333333333332, 20.5, 20.5, 20.5, 19.25, 20.5, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 20.416666666666668, 20.416666666666668, 20.416666666666668, 20.416666666666668, 20.416666666666668, 20.416666666666668, 20.416666666666668, 20.416666666666668, 20.416666666666668, 20.416666666666668, 20.416666666666668, 20.416666666666668, 20.416666666666668, 20.416666666666668, 20.416666666666668]\n",
      "The index of optimal KNN value is: 2\n",
      "The optimal KNN value across inner CV folds is: 3\n",
      "Errors for each outer CV fold: [10.0, 15.0, 21.05263157894737, 15.789473684210526]\n",
      "CV-fold 5 of 5\n",
      "Inner CV-fold 1 of 5\n",
      "Inner CV-fold 2 of 5\n",
      "Inner CV-fold 3 of 5\n",
      "Inner CV-fold 4 of 5\n",
      "Inner CV-fold 5 of 5\n",
      "Inner_error_values are:[16.583333333333332, 14.0, 11.416666666666666, 15.333333333333334, 14.0, 14.083333333333334, 14.0, 12.75, 12.75, 11.5, 11.5, 10.333333333333334, 11.583333333333334, 15.416666666666666, 15.416666666666666, 14.166666666666666, 14.166666666666666, 14.166666666666666, 14.166666666666666, 17.916666666666668, 17.916666666666668, 19.25, 20.5, 23.0, 21.75, 21.666666666666668, 21.666666666666668, 21.666666666666668, 21.666666666666668, 21.666666666666668, 21.666666666666668, 21.666666666666668, 21.666666666666668, 21.666666666666668, 21.666666666666668, 21.666666666666668, 21.666666666666668, 21.666666666666668, 21.666666666666668, 21.666666666666668]\n",
      "The index of optimal KNN value is: 11\n",
      "The optimal KNN value across inner CV folds is: 12\n",
      "Errors for each outer CV fold: [10.0, 15.0, 21.05263157894737, 15.789473684210526, 15.789473684210526]\n"
     ]
    }
   ],
   "source": [
    "## Crossvalidation for KNN\n",
    "## The selection of optimal model is based on an average of the inner errors for each model\n",
    "\n",
    "# Create crossvalidation partition for evaluation\n",
    "K_outer = 5\n",
    "K_inner = 5\n",
    "CV_outer = model_selection.KFold(n_splits=K_outer,shuffle=True)\n",
    "#CV = model_selection.StratifiedKFold(n_splits=K)\n",
    "index_min_lst = []\n",
    "min_indices = []\n",
    "error_outer = [] # List for the errors in outer CV fold\n",
    "dict_inner = {}\n",
    "error_inner = {} # Dict with the errors in the innter CV fold for each tested model\n",
    "K_KNN = range(1,41) # Change here for different nearest neighbour crossvalidation - test of K=1-40\n",
    "\n",
    "for count, value in enumerate(K_KNN):\n",
    "    error_inner['K_KNN_of_{0}'.format(value)] = []\n",
    "\n",
    "k=0\n",
    "classifier_lst = []\n",
    "\n",
    "for train_outer_index, test_outer_index in CV_outer.split(X_classification,y_classification):\n",
    "    print('CV-fold {0} of {1}'.format(k+1,K_outer))\n",
    "    k += 1\n",
    "    \n",
    "    # extract training and test set for current CV fold\n",
    "    X_train_outer = X_classification[train_outer_index,:]\n",
    "    y_train_outer = y_classification[train_outer_index]\n",
    "    X_test_outer = X_classification[test_outer_index,:]\n",
    "    y_test_outer = y_classification[test_outer_index]\n",
    "    \n",
    "    CV_inner = model_selection.KFold(n_splits=K_inner,shuffle=True)\n",
    "    \n",
    "    kk=0\n",
    "    for train_inner_index, test_inner_index in CV_inner.split(X_train_outer,y_train_outer):\n",
    "        print('Inner CV-fold {0} of {1}'.format(kk+1,K_inner))\n",
    "\n",
    "        # extract training and test set for current CV fold\n",
    "        X_train_inner = X_train_outer[train_inner_index,:]\n",
    "        y_train_inner = y_train_outer[train_inner_index]\n",
    "        X_test_inner = X_train_outer[test_inner_index,:]\n",
    "        y_test_inner = y_train_outer[test_inner_index]\n",
    "        \n",
    "#         print(len(X_train_inner))\n",
    "#         print(len(y_train_inner))\n",
    "        \n",
    "        for count, value in enumerate(K_KNN):\n",
    "            dist=2 #euclidean_distance\n",
    "                       \n",
    "            knclassifier = KNeighborsClassifier(n_neighbors=value, p=dist);\n",
    "            knclassifier.fit(X_train_inner, y_train_inner);\n",
    "            classifier_lst.append(knclassifier)\n",
    "            \n",
    "            y_KNN = knclassifier.predict(X_test_inner);\n",
    "            errorKNN_inner = 100*(y_KNN!=y_test_inner).sum().astype(float)/len(y_test_inner)  \n",
    "            #index_min_lst.append(errorKNN_inner) #Append the error values to a list\n",
    "            error_inner['K_KNN_of_{0}'.format(value)].append(errorKNN_inner) # add errors for each fold to each model\n",
    "            \n",
    "        kk += 1\n",
    "        \n",
    "    # Find the KNN value with minimum average error value\n",
    "    for key in error_inner.keys():\n",
    "        index_min_lst.append(mean(error_inner[key]))\n",
    "        \n",
    "    print('Inner_error_values are:' + str(index_min_lst))\n",
    "    index_min = np.argmin(index_min_lst) #Find the index of the minimum error value\n",
    "    top_count = index_min\n",
    "    min_indices.append(index_min) \n",
    "        \n",
    "    index_min_lst = [] # Clear for next CV fold\n",
    "    \n",
    "    for key in error_inner.keys():\n",
    "        error_inner[key] = [] # Clear for next CV fold\n",
    "        \n",
    "      \n",
    "    print('The index of optimal KNN value is: ' + str(top_count))\n",
    "    \n",
    "    optimal_K = K_KNN[top_count]\n",
    "    \n",
    "    print('The optimal KNN value across inner CV folds is: ' + str(optimal_K))\n",
    "    \n",
    "    knclassifierOuter = KNeighborsClassifier(n_neighbors=optimal_K, p=dist); #Uses optimal_K, which was found in the inner CV loop\n",
    "    knclassifierOuter.fit(X_train_outer, y_train_outer);\n",
    "            \n",
    "    y_KNN_outer = knclassifierOuter.predict(X_test_outer);\n",
    "    errorKNN_outer = 100*(y_KNN_outer!=y_test_outer).sum().astype(float)/len(y_test_outer)\n",
    "    error_outer.append(errorKNN_outer)\n",
    "    print('Errors for each outer CV fold: ' + str(error_outer))\n",
    "error_KNN = error_outer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two level cross validation for decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "CV-fold 1 of 5\n",
      "Inner CV-fold 1 of 5\n",
      "Inner CV-fold 2 of 5\n",
      "Inner CV-fold 3 of 5\n",
      "Inner CV-fold 4 of 5\n",
      "Inner CV-fold 5 of 5\n",
      "Inner_error_values are:[19.5, 11.833333333333334, 14.416666666666668, 14.416666666666668, 13.083333333333334, 11.75, 14.416666666666668, 11.75, 14.5, 13.083333333333334, 11.75, 13.166666666666668, 13.0, 13.166666666666668, 15.666666666666668, 11.75, 10.5, 15.666666666666668, 17.0]\n",
      "The index of optimal tc value is: 16\n",
      "The optimal tc value across inner CV folds is: 18\n",
      "Errors for each outer tc fold: [0.0]\n",
      "CV-fold 2 of 5\n",
      "Inner CV-fold 1 of 5\n",
      "Inner CV-fold 2 of 5\n",
      "Inner CV-fold 3 of 5\n",
      "Inner CV-fold 4 of 5\n",
      "Inner CV-fold 5 of 5\n",
      "Inner_error_values are:[20.916666666666668, 13.0, 14.333333333333334, 14.25, 14.333333333333334, 13.0, 16.916666666666668, 16.916666666666668, 16.916666666666668, 15.75, 13.083333333333334, 14.333333333333334, 15.666666666666666, 18.166666666666668, 16.916666666666668, 15.666666666666666, 15.666666666666666, 13.0, 18.25]\n",
      "The index of optimal tc value is: 1\n",
      "The optimal tc value across inner CV folds is: 3\n",
      "Errors for each outer tc fold: [0.0, 15.0]\n",
      "CV-fold 3 of 5\n",
      "Inner CV-fold 1 of 5\n",
      "Inner CV-fold 2 of 5\n",
      "Inner CV-fold 3 of 5\n",
      "Inner CV-fold 4 of 5\n",
      "Inner CV-fold 5 of 5\n",
      "Inner_error_values are:[17.833333333333332, 16.583333333333332, 13.916666666666666, 15.333333333333334, 12.75, 12.75, 12.666666666666666, 15.25, 16.583333333333332, 15.416666666666666, 12.666666666666666, 14.083333333333334, 18.0, 14.0, 12.666666666666666, 11.416666666666666, 16.75, 11.416666666666666, 12.75]\n",
      "The index of optimal tc value is: 15\n",
      "The optimal tc value across inner CV folds is: 17\n",
      "Errors for each outer tc fold: [0.0, 15.0, 10.526315789473685]\n",
      "CV-fold 4 of 5\n",
      "Inner CV-fold 1 of 5\n",
      "Inner CV-fold 2 of 5\n",
      "Inner CV-fold 3 of 5\n",
      "Inner CV-fold 4 of 5\n",
      "Inner CV-fold 5 of 5\n",
      "Inner_error_values are:[24.333333333333332, 20.5, 21.75, 21.75, 23.083333333333332, 23.083333333333332, 23.083333333333332, 23.0, 21.833333333333332, 23.083333333333332, 24.333333333333332, 20.5, 21.75, 24.333333333333332, 24.333333333333332, 24.333333333333332, 25.583333333333332, 24.333333333333332, 21.75]\n",
      "The index of optimal tc value is: 1\n",
      "The optimal tc value across inner CV folds is: 3\n",
      "Errors for each outer tc fold: [0.0, 15.0, 10.526315789473685, 10.526315789473685]\n",
      "CV-fold 5 of 5\n",
      "Inner CV-fold 1 of 5\n",
      "Inner CV-fold 2 of 5\n",
      "Inner CV-fold 3 of 5\n",
      "Inner CV-fold 4 of 5\n",
      "Inner CV-fold 5 of 5\n",
      "Inner_error_values are:[14.083333333333334, 22.916666666666668, 14.166666666666666, 15.416666666666666, 15.416666666666666, 15.416666666666666, 20.416666666666668, 14.166666666666666, 20.416666666666668, 15.416666666666666, 14.166666666666666, 14.166666666666666, 22.916666666666668, 20.416666666666668, 14.166666666666666, 20.416666666666668, 15.416666666666666, 21.666666666666668, 20.416666666666668]\n",
      "The index of optimal tc value is: 0\n",
      "The optimal tc value across inner CV folds is: 2\n",
      "Errors for each outer tc fold: [0.0, 15.0, 10.526315789473685, 10.526315789473685, 10.526315789473685]\n"
     ]
    }
   ],
   "source": [
    "## Crossvalidation for decision trees\n",
    "## The selection of optimal model is based on an average of the inner errors for each model\n",
    "\n",
    "# Create crossvalidation partition for evaluation\n",
    "K_outer = 5\n",
    "K_inner = 5\n",
    "CV_outer = model_selection.KFold(n_splits=K_outer,shuffle=True)\n",
    "#CV = model_selection.StratifiedKFold(n_splits=K)\n",
    "index_min_lst = []\n",
    "min_indices = []\n",
    "error_outer = [] # List for the errors in outer CV fold\n",
    "dict_inner = {}\n",
    "error_inner = {} # Dict with the errors in the innter CV fold for each tested model\n",
    "# Tree complexity parameter - constraint on maximum depth\n",
    "tc = np.arange(2, 21, 1)\n",
    "print(tc)\n",
    "\n",
    "\n",
    "for count, value in enumerate(tc):\n",
    "    error_inner['tc_of_{0}'.format(value)] = []\n",
    "\n",
    "k=0\n",
    "classifier_lst = []\n",
    "\n",
    "for train_outer_index, test_outer_index in CV_outer.split(X_classification,y_classification):\n",
    "    print('CV-fold {0} of {1}'.format(k+1,K_outer))\n",
    "    k += 1\n",
    "    \n",
    "    # extract training and test set for current CV fold\n",
    "    X_train_outer = X_classification[train_outer_index,:]\n",
    "    y_train_outer = y_classification[train_outer_index]\n",
    "    X_test_outer = X_classification[test_outer_index,:]\n",
    "    y_test_outer = y_classification[test_outer_index]\n",
    "    \n",
    "    CV_inner = model_selection.KFold(n_splits=K_inner,shuffle=True)\n",
    "    \n",
    "    kk=0\n",
    "    for train_inner_index, test_inner_index in CV_inner.split(X_train_outer,y_train_outer):\n",
    "        print('Inner CV-fold {0} of {1}'.format(kk+1,K_inner))\n",
    "\n",
    "        # extract training and test set for current CV fold\n",
    "        X_train_inner = X_train_outer[train_inner_index,:]\n",
    "        y_train_inner = y_train_outer[train_inner_index]\n",
    "        X_test_inner = X_train_outer[test_inner_index,:]\n",
    "        y_test_inner = y_train_outer[test_inner_index]\n",
    "        \n",
    "#         print(len(X_train_inner))\n",
    "#         print(len(y_train_inner))\n",
    "        \n",
    "        for count, value in enumerate(tc):\n",
    "            dist=1\n",
    "            \n",
    "            # Fit decision tree classifier, Gini split criterion, different pruning levels\n",
    "            dtc = tree.DecisionTreeClassifier(criterion='gini', max_depth=value) \n",
    "            dtc.fit(X_train_inner, y_train_inner.ravel());\n",
    "            classifier_lst.append(dtc)\n",
    "            \n",
    "            y_dtc = dtc.predict(X_test_inner);\n",
    "            errordtc_inner = 100*(y_dtc!=y_test_inner).sum().astype(float)/len(y_test_inner)  \n",
    "            #index_min_lst.append(errorKNN_inner) #Append the error values to a list\n",
    "            error_inner['tc_of_{0}'.format(value)].append(errordtc_inner) # add errors for each fold to each model\n",
    "            \n",
    "        kk += 1\n",
    "        \n",
    "    # Find the KNN value with minimum average error value\n",
    "    for key in error_inner.keys():\n",
    "        index_min_lst.append(mean(error_inner[key]))\n",
    "        \n",
    "    print('Inner_error_values are:' + str(index_min_lst))\n",
    "    index_min = np.argmin(index_min_lst) #Find the index of the minimum error value\n",
    "    top_count = index_min\n",
    "    min_indices.append(index_min) \n",
    "        \n",
    "    index_min_lst = [] # Clear for next CV fold\n",
    "    \n",
    "    for key in error_inner.keys():\n",
    "        error_inner[key] = [] # Clear for next CV fold\n",
    "        \n",
    "      \n",
    "    print('The index of optimal tc value is: ' + str(top_count))\n",
    "    \n",
    "    optimal_tc = tc[top_count]\n",
    "    \n",
    "    print('The optimal tc value across inner CV folds is: ' + str(optimal_tc))\n",
    "    \n",
    "    \n",
    "    dtcclassifierOuter = tree.DecisionTreeClassifier(criterion='gini', max_depth=optimal_tc);  #Uses optimal_tc, which was found in the inner CV loop\n",
    "    dtcclassifierOuter.fit(X_train_outer, y_train_outer.ravel());\n",
    "            \n",
    "    y_dtc_outer = dtcclassifierOuter.predict(X_test_outer);\n",
    "    errordtc_outer = 100*(y_dtc_outer!=y_test_outer).sum().astype(float)/len(y_test_outer)\n",
    "    error_outer.append(errordtc_outer)\n",
    "    print('Errors for each outer tc fold: ' + str(error_outer))\n",
    "\n",
    "error_dct = error_outer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two level cross validation for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV-fold 1 of 5\n",
      "Inner CV-fold 1 of 5\n",
      "[[-2.17817387 -0.8121913  -0.79198919 -1.03002898 -0.86765522 -1.04757113\n",
      "  -0.86895727 -2.29971238]\n",
      " [-0.5105128  -0.46121762 -0.25193329 -1.03002898 -0.86765522 -1.04757113\n",
      "  -0.86895727 -1.83463099]\n",
      " [-2.04670586 -0.93880639 -1.87210098 -1.03002898 -0.86765522 -1.04757113\n",
      "  -0.86895727 -1.49161747]\n",
      " [-0.56020767 -0.20984103 -0.79198919  0.99529051 -0.86765522 -1.04757113\n",
      "  -0.86895727 -1.4141616 ]\n",
      " [-0.93418834 -0.05819996  0.15310863 -1.03002898 -0.86765522 -1.04757113\n",
      "  -0.86895727 -1.05492666]\n",
      " [-2.30021799 -0.07100389 -0.11691932  0.80827605 -0.86765522 -1.04757113\n",
      "  -0.86895727 -1.05492666]\n",
      " [ 0.22465908 -1.4220686  -0.11691932 -1.03002898 -0.30083707  0.34440695\n",
      "   0.20024597 -1.05492666]\n",
      " [ 0.10834583 -1.47986344  0.42313658 -1.03002898 -0.86765522  0.34440695\n",
      "  -0.69075673 -0.98428221]\n",
      " [-0.12284403 -0.4385849  -0.92700316 -1.03002898 -0.1807427   0.34440695\n",
      "  -0.69075673 -0.94018137]\n",
      " [ 0.16302259 -1.33245984  0.2881226  -1.03002898 -0.86765522 -1.04757113\n",
      "  -0.86895727 -0.89820677]\n",
      " [ 0.80038343  0.04790351  0.2881226  -1.03002898  0.39606027 -1.04757113\n",
      "  -0.86895727 -0.85816274]\n",
      " [-1.63076627 -0.84767487 -3.08722674 -1.03002898 -0.86765522 -1.04757113\n",
      "  -0.86895727 -0.80135103]\n",
      " [ 0.60486894 -0.30009502 -0.52196124  0.95226146  1.09806826  0.34440695\n",
      "  -0.15615511 -0.71419787]\n",
      " [ 0.36817665 -0.4161657  -0.11691932  0.23411435  0.97627438  0.34440695\n",
      "   1.26944921 -0.66580745]\n",
      " [-0.82278841  0.09023368  0.69316453  1.03860789 -0.86765522 -1.04757113\n",
      "  -0.86895727 -0.65025697]\n",
      " [ 0.08264956 -1.18343728  0.55815055  0.13839656 -0.86765522 -1.04757113\n",
      "  -0.86895727 -0.619965  ]\n",
      " [-0.71399732  0.21283185  0.15310863 -1.03002898 -0.44509826  0.34440695\n",
      "   1.62585029 -0.59069151]\n",
      " [ 0.9037135  -0.59376893  0.15310863 -1.03002898  1.29311536 -1.04757113\n",
      "  -0.86895727 -0.50834947]\n",
      " [-0.90814498  1.08218997  0.15310863  1.29047369 -0.44509826 -1.04757113\n",
      "  -0.86895727 -0.48254597]\n",
      " [-0.9958673   0.41177028  0.15310863  1.11160717 -0.86765522 -1.04757113\n",
      "  -0.86895727 -0.40942861]\n",
      " [-1.14287478 -0.84767487 -1.33204508 -1.03002898 -0.86765522 -1.04757113\n",
      "  -0.86895727 -0.39781765]\n",
      " [-0.03554419  1.15183144  0.01809466  1.43488428 -0.86765522  0.34440695\n",
      "  -0.69075673 -0.34197776]\n",
      " [ 0.06234256  0.0661392   1.23322042 -0.47126026  1.32103713  1.73638502\n",
      "  -0.33435565 -0.27937807]\n",
      " [-0.76124438 -2.94238594  0.01809466 -1.03002898 -0.86765522  0.34440695\n",
      "  -0.33435565 -0.24968869]\n",
      " [ 1.118048    1.07038088  0.55815055  0.8822505   1.44637892  0.34440695\n",
      "   0.37844651 -0.23044356]\n",
      " [-0.62209987 -1.14254072 -0.52196124 -1.03002898 -0.86765522  3.1283631\n",
      "   1.98225137 -0.15745387]\n",
      " [-0.65481608  0.55616587 -0.25193329  1.11787737 -0.1807427  -1.04757113\n",
      "  -0.86895727 -0.13158654]\n",
      " [ 0.35951816  0.62873791 -0.38694727 -1.03002898  0.71191882  0.34440695\n",
      "  -0.65511662 -0.09011178]\n",
      " [ 0.1160991  -0.51489465  0.2881226   1.14240568 -0.1807427   0.34440695\n",
      "  -0.15615511  0.0377352 ]\n",
      " [-0.15936323  0.95303849  0.55815055  1.11787737 -0.1807427   0.34440695\n",
      "   0.55664705  0.07872178]\n",
      " [ 0.33747937 -0.30718329 -2.81719879 -1.03002898 -0.86765522 -1.04757113\n",
      "  -0.86895727  0.09851369]\n",
      " [-0.22010989  0.85561411  0.55815055 -1.03002898 -0.86765522  0.34440695\n",
      "   0.91304813  0.1553254 ]\n",
      " [-0.71399732  0.0110004   0.01809466  0.96483055  0.16402005  0.34440695\n",
      "   1.62585029  0.17943223]\n",
      " [-0.07083973  1.52790617  0.2881226   1.40088236 -0.86765522  0.34440695\n",
      "  -0.33435565  0.2086566 ]\n",
      " [-0.32020395 -1.79233616 -2.2771429  -1.03002898  0.48894996  0.34440695\n",
      "  -0.72639684  0.26969337]\n",
      " [-0.75586358  0.31848951 -2.00711495  0.91647239 -0.86765522 -1.04757113\n",
      "  -0.86895727  0.27503575]\n",
      " [ 0.55214455  0.21283185 -0.11691932  1.05246539  1.50170584  0.34440695\n",
      "   0.55664705  0.32673071]\n",
      " [ 1.2159132  -0.2441444   1.09820645 -1.03002898  1.24908762  3.1283631\n",
      "   2.51685299  0.32673071]\n",
      " [ 0.58394572  0.67590387  0.2881226   1.32186427  1.64596703  0.34440695\n",
      "   1.26944921  0.35147113]\n",
      " [ 0.61675184 -0.01392703  0.01809466 -1.03002898 -0.86765522 -1.04757113\n",
      "  -0.86895727  0.35147113]\n",
      " [ 0.72349772  0.99008707  1.09820645  1.52927559 -0.1807427   0.34440695\n",
      "  -0.51255619  0.42173538]\n",
      " [-1.53197866  1.82921036  0.69316453 -1.03002898 -0.86765522 -1.04757113\n",
      "  -0.86895727  0.42173538]\n",
      " [-0.1331195   2.70166094  1.09820645  1.54225202 -0.44509826  0.34440695\n",
      "  -0.69075673  0.43068977]\n",
      " [ 0.43842708 -0.08387821 -0.52196124 -1.03002898  1.07914887  0.34440695\n",
      "   1.26944921  0.46561391]\n",
      " [-0.16203258 -0.67539077  1.77327632  1.14240568 -0.86765522  0.34440695\n",
      "   0.02204543  0.48675094]\n",
      " [-0.11521787  0.46089542  0.69316453 -1.03002898  0.28936185  0.34440695\n",
      "  -0.15615511  0.50329884]\n",
      " [ 0.41700419 -0.92029384 -0.52196124  0.23411435  0.97627438  3.1283631\n",
      "   2.33865245  0.51953812]\n",
      " [ 1.40654082  0.51652225  0.69316453 -1.03002898  1.50170584  0.34440695\n",
      "  -0.15615511  0.69391731]\n",
      " [ 1.52756447 -0.85663082  0.55815055 -0.1050703   1.8689359   0.34440695\n",
      "   0.91304813  0.74816076]\n",
      " [ 0.56363872  1.88843646  1.09820645  1.40088236  0.48894996  0.34440695\n",
      "   1.26944921  0.79630031]\n",
      " [ 1.01288993  1.70306453  1.90829029  1.54225202 -0.86765522  0.34440695\n",
      "  -0.51255619  0.83354436]\n",
      " [ 1.10725223 -0.10984037  0.69316453 -1.03002898  1.98656829  0.34440695\n",
      "   1.62585029  0.85295798]\n",
      " [ 0.99242046 -0.3646778  -0.92700316  0.23411435  1.80201365  0.34440695\n",
      "   1.26944921  0.91641341]\n",
      " [ 1.132233    0.49140008  0.15310863  0.7030969   1.38643629  3.1283631\n",
      "   1.62585029  0.95140024]\n",
      " [ 0.18109221  0.1899692  -0.52196124  1.10527971  0.71191882  0.34440695\n",
      "   0.20024597  0.96597463]\n",
      " [ 1.66548696 -0.25800887  0.01809466 -1.03002898  1.80201365  0.34440695\n",
      "   1.26944921  1.00368795]\n",
      " [ 0.57498003  0.24110046 -0.79198919  1.06605117 -0.86765522 -1.04757113\n",
      "  -0.86895727  1.04644915]\n",
      " [ 0.32548825 -0.60986946 -0.25193329 -1.03002898  0.34468877  0.34440695\n",
      "   0.20024597  1.07454209]\n",
      " [ 1.24310643  2.55541174  0.15310863 -1.03002898  1.90019713  0.34440695\n",
      "   1.26944921  1.31139383]\n",
      " [ 1.00883515  0.11408648 -0.38694727  0.86448408 -0.86765522  0.34440695\n",
      "  -0.33435565  1.43784081]\n",
      " [ 1.3282669  -0.54612667 -1.60207303 -1.03002898  1.90019713  0.34440695\n",
      "  -0.51255619  2.32046525]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X must be non-negative",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-af75c218e0c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mnbclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_prior\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mest_prior\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[0mnbclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_inner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_inner\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m             \u001b[0mclassifier_lst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbclassifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    602\u001b[0m         self.feature_count_ = np.zeros((n_effective_classes, n_features),\n\u001b[0;32m    603\u001b[0m                                        dtype=np.float64)\n\u001b[1;32m--> 604\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    605\u001b[0m         \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    706\u001b[0m         \u001b[1;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 708\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input X must be non-negative\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    709\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input X must be non-negative"
     ]
    }
   ],
   "source": [
    "### OBS. method can not use X values < 0\n",
    "\n",
    "## Crossvalidation for Naive Bayes\n",
    "## The selection of optimal model is based on an average of the inner errors for each model\n",
    "\n",
    "# Create crossvalidation partition for evaluation\n",
    "K_outer = 5\n",
    "K_inner = 5\n",
    "CV_outer = model_selection.KFold(n_splits=K_outer,shuffle=True)\n",
    "#CV = model_selection.StratifiedKFold(n_splits=K)\n",
    "index_min_lst = []\n",
    "min_indices = []\n",
    "error_outer = np.empty((1,1)) # List for the errors in outer CV fold\n",
    "dict_inner = {}\n",
    "error_inner = {} # Dict with the errors in the innter CV fold for each tested model\n",
    "\n",
    "# Naive Bayes classifier parameters\n",
    "alpha = [1]         # additive parameter (e.g. Laplace correction), lgges til da log bliver taget i MultinormilNB\n",
    "\n",
    "for count, value in enumerate(K_KNN):\n",
    "    error_inner['K_KNN_of_{0}'.format(value)] = []\n",
    "\n",
    "k=0\n",
    "classifier_lst = []\n",
    "\n",
    "for train_outer_index, test_outer_index in CV_outer.split(X_classification,y_classification):\n",
    "    print('CV-fold {0} of {1}'.format(k+1,K_outer))\n",
    "    k += 1\n",
    "    \n",
    "    # extract training and test set for current CV fold\n",
    "    X_train_outer = X_classification[train_outer_index,:]\n",
    "    y_train_outer = y_classification[train_outer_index]\n",
    "    X_test_outer = X_classification[test_outer_index,:]\n",
    "    y_test_outer = y_classification[test_outer_index]\n",
    "    \n",
    "    CV_inner = model_selection.KFold(n_splits=K_inner,shuffle=True)\n",
    "    \n",
    "    kk=0\n",
    "    for train_inner_index, test_inner_index in CV_inner.split(X_train_outer,y_train_outer):\n",
    "        print('Inner CV-fold {0} of {1}'.format(kk+1,K_inner))\n",
    "\n",
    "        # extract training and test set for current CV fold\n",
    "        X_train_inner = X_train_outer[train_inner_index,:]\n",
    "        y_train_inner = y_train_outer[train_inner_index]\n",
    "        X_test_inner = X_train_outer[test_inner_index,:]\n",
    "        y_test_inner = y_train_outer[test_inner_index]\n",
    "        \n",
    "#         print(len(X_train_inner))\n",
    "#         print(len(y_train_inner))\n",
    "        print(X_train_inner) \n",
    "        for count, value in enumerate(alpha):\n",
    "            est_prior = True  # uniform prior (change to True to estimate prior from data)    \n",
    "                       \n",
    "            nbclassifier = MultinomialNB(alpha=alpha, fit_prior=est_prior);\n",
    "            nbclassifier.fit(X_train_inner, y_train_inner);\n",
    "            classifier_lst.append(nbclassifier)\n",
    " \n",
    "            y_NB = nbclassifier.predict(X_test_inner);\n",
    "            errorNB_inner = 100*(y_NB!=y_test_inner).sum().astype(float)/len(y_test_inner)  \n",
    "            #index_min_lst.append(errorKNN_inner) #Append the error values to a list\n",
    "            error_inner['alpha_of_{0}'.format(value)].append(errorNB_inner) # add errors for each fold to each model\n",
    "            \n",
    "        kk += 1\n",
    "        \n",
    "    # Find the alpha value with minimum average error value\n",
    "    for key in error_inner.keys():\n",
    "        index_min_lst.append(mean(error_inner[key]))\n",
    "        \n",
    "    print('Inner_error_values are:' + str(index_min_lst))\n",
    "    index_min = np.argmin(index_min_lst) #Find the index of the minimum error value\n",
    "    top_count = index_min\n",
    "    min_indices.append(index_min) \n",
    "        \n",
    "    index_min_lst = [] # Clear for next CV fold\n",
    "    \n",
    "    for key in error_inner.keys():\n",
    "        error_inner[key] = [] # Clear for next CV fold\n",
    "        \n",
    "      \n",
    "    print('The index of optimal KNN value is: ' + str(top_count))\n",
    "    \n",
    "    optimal_alpha = alpha[top_count]\n",
    "    \n",
    "    print('The optimal KNN value across inner CV folds is: ' + str(optimal_K))\n",
    "    \n",
    "    nbclassifierOuter = MultinomialNB(alpha=optimal_alpha, fit_prior=est_prior); #Uses optimal_alpha, which was found in the inner CV loop\n",
    "    nbclassifierOuter.fit(X_train_outer, y_train_outer);\n",
    "            \n",
    "    y_NB_outer = nbclassifier.predict(X_test_outer);\n",
    "    errorNB_outer = 100*(y_NB_outer!=y_test_outer).sum().astype(float)/len(y_test_outer)\n",
    "    error_outer.append(errorNB_outer)\n",
    "    print('Errors for each outer CV fold: ' + str(error_outer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure()\n",
    "boxplot([error_KNN, error_dct])\n",
    "xlabel('K-Nearest Neighbors   vs.   Decision Tree')\n",
    "ylabel('Cross-validation error [%]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method selected: Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Re-estimate of the model on all data for a maximum depth of 12 \n",
    "\n",
    "# Fit decision tree classifier, Gini split criterion, maximum depth of 12 on all data\n",
    "dtc = tree.DecisionTreeClassifier(criterion='gini', max_depth=12) \n",
    "dtc.fit(X_classification, y_classification.ravel())\n",
    "\n",
    "# New data object\n",
    "new_data = np.array([1, 2, 4, -1, -1, 5, 0, -3]).reshape(1,-1) # Gives 0 - No SVI\n",
    "#new_data = np.array([1, 2, 4, -1, 2, 5, 0, -3]).reshape(1,-1) # Gives 1 - Yes SVI\n",
    "\n",
    "# Evalulate the decision tree for a new data object\n",
    "new_data_class = dtc.predict(new_data)[0]\n",
    "print(new_data_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lCaVol' 'lWeight' 'Age' 'lBPH' 'lCP' 'Gleason' 'pgg45' 'lPSA']\n"
     ]
    }
   ],
   "source": [
    "print(attributeNames_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: Tree Pages: 1 -->\r\n",
       "<svg width=\"632pt\" height=\"581pt\"\r\n",
       " viewBox=\"0.00 0.00 631.50 581.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 577)\">\r\n",
       "<title>Tree</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-577 627.5,-577 627.5,4 -4,4\"/>\r\n",
       "<!-- 0 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"442.5,-573 338.5,-573 338.5,-505 442.5,-505 442.5,-573\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"390.5\" y=\"-557.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">lCP &lt;= 1.416</text>\r\n",
       "<text text-anchor=\"middle\" x=\"390.5\" y=\"-542.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.339</text>\r\n",
       "<text text-anchor=\"middle\" x=\"390.5\" y=\"-527.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 97</text>\r\n",
       "<text text-anchor=\"middle\" x=\"390.5\" y=\"-512.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [76, 21]</text>\r\n",
       "</g>\r\n",
       "<!-- 1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"379,-469 278,-469 278,-401 379,-401 379,-469\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"328.5\" y=\"-453.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">lPSA &lt;= 0.448</text>\r\n",
       "<text text-anchor=\"middle\" x=\"328.5\" y=\"-438.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.191</text>\r\n",
       "<text text-anchor=\"middle\" x=\"328.5\" y=\"-423.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 84</text>\r\n",
       "<text text-anchor=\"middle\" x=\"328.5\" y=\"-408.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [75, 9]</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M370.37,-504.884C365.173,-496.332 359.508,-487.013 354.073,-478.072\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"356.926,-476.027 348.741,-469.299 350.944,-479.663 356.926,-476.027\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"342.817\" y=\"-489.887\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 12 -->\r\n",
       "<g id=\"node13\" class=\"node\"><title>12</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"504.5,-469 400.5,-469 400.5,-401 504.5,-401 504.5,-469\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"452.5\" y=\"-453.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">lCaVol &lt;= 1.05</text>\r\n",
       "<text text-anchor=\"middle\" x=\"452.5\" y=\"-438.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.142</text>\r\n",
       "<text text-anchor=\"middle\" x=\"452.5\" y=\"-423.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 13</text>\r\n",
       "<text text-anchor=\"middle\" x=\"452.5\" y=\"-408.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [1, 12]</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;12 -->\r\n",
       "<g id=\"edge12\" class=\"edge\"><title>0&#45;&gt;12</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M410.63,-504.884C415.827,-496.332 421.492,-487.013 426.927,-478.072\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"430.056,-479.663 432.259,-469.299 424.074,-476.027 430.056,-479.663\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"438.183\" y=\"-489.887\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 2 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"259.5,-357.5 161.5,-357.5 161.5,-304.5 259.5,-304.5 259.5,-357.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"210.5\" y=\"-342.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"210.5\" y=\"-327.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 66</text>\r\n",
       "<text text-anchor=\"middle\" x=\"210.5\" y=\"-312.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [66, 0]</text>\r\n",
       "</g>\r\n",
       "<!-- 1&#45;&gt;2 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>1&#45;&gt;2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M290.189,-400.884C276.575,-389.116 261.279,-375.894 247.755,-364.203\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"249.877,-361.412 240.023,-357.52 245.3,-366.707 249.877,-361.412\"/>\r\n",
       "</g>\r\n",
       "<!-- 3 -->\r\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"379,-365 278,-365 278,-297 379,-297 379,-365\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"328.5\" y=\"-349.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">lBPH &lt;= 1.038</text>\r\n",
       "<text text-anchor=\"middle\" x=\"328.5\" y=\"-334.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"328.5\" y=\"-319.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 18</text>\r\n",
       "<text text-anchor=\"middle\" x=\"328.5\" y=\"-304.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [9, 9]</text>\r\n",
       "</g>\r\n",
       "<!-- 1&#45;&gt;3 -->\r\n",
       "<g id=\"edge3\" class=\"edge\"><title>1&#45;&gt;3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M328.5,-400.884C328.5,-392.778 328.5,-383.982 328.5,-375.472\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"332,-375.299 328.5,-365.299 325,-375.299 332,-375.299\"/>\r\n",
       "</g>\r\n",
       "<!-- 4 -->\r\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"268.5,-261 158.5,-261 158.5,-193 268.5,-193 268.5,-261\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"213.5\" y=\"-245.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">lCaVol &lt;= 1.105</text>\r\n",
       "<text text-anchor=\"middle\" x=\"213.5\" y=\"-230.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.426</text>\r\n",
       "<text text-anchor=\"middle\" x=\"213.5\" y=\"-215.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 13</text>\r\n",
       "<text text-anchor=\"middle\" x=\"213.5\" y=\"-200.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [4, 9]</text>\r\n",
       "</g>\r\n",
       "<!-- 3&#45;&gt;4 -->\r\n",
       "<g id=\"edge4\" class=\"edge\"><title>3&#45;&gt;4</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M291.163,-296.884C280.819,-287.709 269.478,-277.65 258.736,-268.123\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"260.847,-265.317 251.043,-261.299 256.202,-270.553 260.847,-265.317\"/>\r\n",
       "</g>\r\n",
       "<!-- 11 -->\r\n",
       "<g id=\"node12\" class=\"node\"><title>11</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"378,-253.5 287,-253.5 287,-200.5 378,-200.5 378,-253.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"332.5\" y=\"-238.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"332.5\" y=\"-223.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"332.5\" y=\"-208.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [5, 0]</text>\r\n",
       "</g>\r\n",
       "<!-- 3&#45;&gt;11 -->\r\n",
       "<g id=\"edge11\" class=\"edge\"><title>3&#45;&gt;11</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M329.799,-296.884C330.217,-286.216 330.682,-274.352 331.107,-263.519\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"334.605,-263.649 331.499,-253.52 327.61,-263.375 334.605,-263.649\"/>\r\n",
       "</g>\r\n",
       "<!-- 5 -->\r\n",
       "<g id=\"node6\" class=\"node\"><title>5</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"209,-157 90,-157 90,-89 209,-89 209,-157\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"149.5\" y=\"-141.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">lWeight &lt;= &#45;1.115</text>\r\n",
       "<text text-anchor=\"middle\" x=\"149.5\" y=\"-126.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.219</text>\r\n",
       "<text text-anchor=\"middle\" x=\"149.5\" y=\"-111.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 8</text>\r\n",
       "<text text-anchor=\"middle\" x=\"149.5\" y=\"-96.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [1, 7]</text>\r\n",
       "</g>\r\n",
       "<!-- 4&#45;&gt;5 -->\r\n",
       "<g id=\"edge5\" class=\"edge\"><title>4&#45;&gt;5</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M192.721,-192.884C187.3,-184.243 181.387,-174.819 175.723,-165.793\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"178.673,-163.91 170.394,-157.299 172.744,-167.63 178.673,-163.91\"/>\r\n",
       "</g>\r\n",
       "<!-- 8 -->\r\n",
       "<g id=\"node9\" class=\"node\"><title>8</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"328,-157 227,-157 227,-89 328,-89 328,-157\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"277.5\" y=\"-141.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">lPSA &lt;= 1.506</text>\r\n",
       "<text text-anchor=\"middle\" x=\"277.5\" y=\"-126.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.48</text>\r\n",
       "<text text-anchor=\"middle\" x=\"277.5\" y=\"-111.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"277.5\" y=\"-96.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [3, 2]</text>\r\n",
       "</g>\r\n",
       "<!-- 4&#45;&gt;8 -->\r\n",
       "<g id=\"edge8\" class=\"edge\"><title>4&#45;&gt;8</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M234.279,-192.884C239.7,-184.243 245.613,-174.819 251.277,-165.793\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"254.256,-167.63 256.606,-157.299 248.327,-163.91 254.256,-167.63\"/>\r\n",
       "</g>\r\n",
       "<!-- 6 -->\r\n",
       "<g id=\"node7\" class=\"node\"><title>6</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"91,-53 0,-53 0,-0 91,-0 91,-53\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"45.5\" y=\"-37.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"45.5\" y=\"-22.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 1</text>\r\n",
       "<text text-anchor=\"middle\" x=\"45.5\" y=\"-7.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [1, 0]</text>\r\n",
       "</g>\r\n",
       "<!-- 5&#45;&gt;6 -->\r\n",
       "<g id=\"edge6\" class=\"edge\"><title>5&#45;&gt;6</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M113.126,-88.9485C102.857,-79.6175 91.6916,-69.4722 81.4478,-60.1641\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"83.5788,-57.3713 73.824,-53.2367 78.8713,-62.5521 83.5788,-57.3713\"/>\r\n",
       "</g>\r\n",
       "<!-- 7 -->\r\n",
       "<g id=\"node8\" class=\"node\"><title>7</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"200,-53 109,-53 109,-0 200,-0 200,-53\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"154.5\" y=\"-37.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"154.5\" y=\"-22.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 7</text>\r\n",
       "<text text-anchor=\"middle\" x=\"154.5\" y=\"-7.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [0, 7]</text>\r\n",
       "</g>\r\n",
       "<!-- 5&#45;&gt;7 -->\r\n",
       "<g id=\"edge7\" class=\"edge\"><title>5&#45;&gt;7</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M151.249,-88.9485C151.684,-80.7153 152.154,-71.848 152.596,-63.4814\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.105,-63.4077 153.138,-53.2367 149.115,-63.0378 156.105,-63.4077\"/>\r\n",
       "</g>\r\n",
       "<!-- 9 -->\r\n",
       "<g id=\"node10\" class=\"node\"><title>9</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"314,-53 223,-53 223,-0 314,-0 314,-53\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"268.5\" y=\"-37.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"268.5\" y=\"-22.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 3</text>\r\n",
       "<text text-anchor=\"middle\" x=\"268.5\" y=\"-7.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [3, 0]</text>\r\n",
       "</g>\r\n",
       "<!-- 8&#45;&gt;9 -->\r\n",
       "<g id=\"edge9\" class=\"edge\"><title>8&#45;&gt;9</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M274.352,-88.9485C273.559,-80.6238 272.705,-71.6509 271.9,-63.2027\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"275.384,-62.8598 270.951,-53.2367 268.415,-63.5235 275.384,-62.8598\"/>\r\n",
       "</g>\r\n",
       "<!-- 10 -->\r\n",
       "<g id=\"node11\" class=\"node\"><title>10</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"423,-53 332,-53 332,-0 423,-0 423,-53\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"377.5\" y=\"-37.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"377.5\" y=\"-22.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 2</text>\r\n",
       "<text text-anchor=\"middle\" x=\"377.5\" y=\"-7.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [0, 2]</text>\r\n",
       "</g>\r\n",
       "<!-- 8&#45;&gt;10 -->\r\n",
       "<g id=\"edge10\" class=\"edge\"><title>8&#45;&gt;10</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M312.475,-88.9485C322.349,-79.6175 333.085,-69.4722 342.935,-60.1641\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"345.401,-62.649 350.265,-53.2367 340.593,-57.5613 345.401,-62.649\"/>\r\n",
       "</g>\r\n",
       "<!-- 13 -->\r\n",
       "<g id=\"node14\" class=\"node\"><title>13</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"507.5,-365 397.5,-365 397.5,-297 507.5,-297 507.5,-365\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"452.5\" y=\"-349.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">lCaVol &lt;= 0.788</text>\r\n",
       "<text text-anchor=\"middle\" x=\"452.5\" y=\"-334.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.444</text>\r\n",
       "<text text-anchor=\"middle\" x=\"452.5\" y=\"-319.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 3</text>\r\n",
       "<text text-anchor=\"middle\" x=\"452.5\" y=\"-304.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [1, 2]</text>\r\n",
       "</g>\r\n",
       "<!-- 12&#45;&gt;13 -->\r\n",
       "<g id=\"edge13\" class=\"edge\"><title>12&#45;&gt;13</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M452.5,-400.884C452.5,-392.778 452.5,-383.982 452.5,-375.472\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"456,-375.299 452.5,-365.299 449,-375.299 456,-375.299\"/>\r\n",
       "</g>\r\n",
       "<!-- 16 -->\r\n",
       "<g id=\"node17\" class=\"node\"><title>16</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"623.5,-357.5 525.5,-357.5 525.5,-304.5 623.5,-304.5 623.5,-357.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"574.5\" y=\"-342.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"574.5\" y=\"-327.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 10</text>\r\n",
       "<text text-anchor=\"middle\" x=\"574.5\" y=\"-312.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [0, 10]</text>\r\n",
       "</g>\r\n",
       "<!-- 12&#45;&gt;16 -->\r\n",
       "<g id=\"edge16\" class=\"edge\"><title>12&#45;&gt;16</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M492.11,-400.884C506.185,-389.116 522,-375.894 535.982,-364.203\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"538.549,-366.619 543.976,-357.52 534.059,-361.249 538.549,-366.619\"/>\r\n",
       "</g>\r\n",
       "<!-- 14 -->\r\n",
       "<g id=\"node15\" class=\"node\"><title>14</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"494,-253.5 403,-253.5 403,-200.5 494,-200.5 494,-253.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"448.5\" y=\"-238.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"448.5\" y=\"-223.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 2</text>\r\n",
       "<text text-anchor=\"middle\" x=\"448.5\" y=\"-208.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [0, 2]</text>\r\n",
       "</g>\r\n",
       "<!-- 13&#45;&gt;14 -->\r\n",
       "<g id=\"edge14\" class=\"edge\"><title>13&#45;&gt;14</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M451.201,-296.884C450.783,-286.216 450.318,-274.352 449.893,-263.519\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"453.39,-263.375 449.501,-253.52 446.395,-263.649 453.39,-263.375\"/>\r\n",
       "</g>\r\n",
       "<!-- 15 -->\r\n",
       "<g id=\"node16\" class=\"node\"><title>15</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"603,-253.5 512,-253.5 512,-200.5 603,-200.5 603,-253.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"557.5\" y=\"-238.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"557.5\" y=\"-223.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 1</text>\r\n",
       "<text text-anchor=\"middle\" x=\"557.5\" y=\"-208.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [1, 0]</text>\r\n",
       "</g>\r\n",
       "<!-- 13&#45;&gt;15 -->\r\n",
       "<g id=\"edge15\" class=\"edge\"><title>13&#45;&gt;15</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M486.59,-296.884C498.591,-285.226 512.061,-272.141 524.012,-260.532\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"526.495,-262.998 531.229,-253.52 521.618,-257.977 526.495,-262.998\"/>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x14a3d223470>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export tree graph for visualization purposes:\n",
    "# (note: you can use i.e. Graphviz application to visualize the file)\n",
    "out = tree.export_graphviz(dtc, out_file='tree_gini.gvz', feature_names=attributeNames_classification)\n",
    "#graphviz.render('dot','png','tree_gini',quiet=False)\n",
    "src=graphviz.Source.from_file('tree_gini.gvz')\n",
    "## Comment in to automatically open pdf\n",
    "## Note. If you get an error (e.g. exit status 1), try closing the pdf file/viewer\n",
    "#src.render('../tree_gini', view=True)\n",
    "src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model\n",
    "# zero rule algorithm for classification\n",
    "def Baseline_model(y_train, y_test):\n",
    "    prediction = stats.mode(y_train)[0]\n",
    "    predicted = [int(prediction) for i in range(len(y_test))]\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-level-cross validation for dtc, KNN and baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV-fold 1 of 5\n",
      "Inner CV-fold 1 of 5\n",
      "Inner CV-fold 2 of 5\n",
      "Inner CV-fold 3 of 5\n",
      "Inner CV-fold 4 of 5\n",
      "Inner CV-fold 5 of 5\n",
      "Inner_error_values_ for dtc are:[16.833333333333332, 16.833333333333332, 15.5, 16.833333333333332, 15.583333333333334, 15.583333333333334, 19.5, 18.083333333333332, 18.166666666666668, 16.833333333333332, 19.583333333333336, 16.833333333333332, 19.5, 15.5, 19.5, 14.25, 20.833333333333336, 19.5, 22.083333333333336]\n",
      "The index of optimal tc value is: 0\n",
      "The optimal tc value across inner CV folds is: 17\n",
      "Errors for each outer tc fold: [15.0]\n",
      "Inner_error_values are:[22.0, 20.583333333333332, 18.083333333333332, 18.083333333333332, 15.583333333333334, 18.083333333333332, 16.75, 18.083333333333332, 15.583333333333334, 15.5, 16.833333333333332, 16.833333333333332, 15.583333333333334, 16.833333333333332, 15.583333333333334, 15.583333333333334, 15.583333333333334, 20.75, 22.0, 23.333333333333332, 22.0, 22.083333333333336, 23.333333333333336, 20.75, 23.333333333333332, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0]\n",
      "The index of optimal KNN value is: 0\n",
      "The optimal KNN value across inner CV folds is: 10\n",
      "Errors for each outer CV fold: [15.0]\n",
      "Errors for baseline outer fold[20.0]\n",
      "CV-fold 2 of 5\n",
      "Inner CV-fold 1 of 5\n",
      "Inner CV-fold 2 of 5\n",
      "Inner CV-fold 3 of 5\n",
      "Inner CV-fold 4 of 5\n",
      "Inner CV-fold 5 of 5\n",
      "Inner_error_values_ for dtc are:[18.25, 17.083333333333332, 15.75, 15.666666666666666, 15.75, 13.166666666666666, 14.416666666666666, 17.0, 14.5, 17.0, 13.083333333333334, 15.75, 15.666666666666666, 17.0, 17.0, 13.083333333333334, 15.75, 15.75, 14.416666666666666]\n",
      "The index of optimal tc value is: 0\n",
      "The optimal tc value across inner CV folds is: 12\n",
      "Errors for each outer tc fold: [15.0, 5.0]\n",
      "Inner_error_values are:[21.916666666666668, 19.583333333333336, 16.916666666666668, 20.833333333333336, 20.75, 17.083333333333336, 16.833333333333332, 15.75, 14.416666666666668, 15.75, 15.75, 17.083333333333336, 14.5, 15.75, 14.416666666666668, 15.75, 14.416666666666668, 17.166666666666668, 17.083333333333336, 18.416666666666668, 18.416666666666668, 21.083333333333336, 21.0, 23.75, 22.416666666666668, 22.416666666666668, 23.75, 23.666666666666668, 23.666666666666668, 23.666666666666668, 23.666666666666668, 24.916666666666668, 24.916666666666668, 24.916666666666668, 24.916666666666668, 24.916666666666668, 24.916666666666668, 24.916666666666668, 24.916666666666668, 24.916666666666668]\n",
      "The index of optimal KNN value is: 0\n",
      "The optimal KNN value across inner CV folds is: 9\n",
      "Errors for each outer CV fold: [15.0, 15.0]\n",
      "Errors for baseline outer fold[20.0, 10.0]\n",
      "CV-fold 3 of 5\n",
      "Inner CV-fold 1 of 5\n",
      "Inner CV-fold 2 of 5\n",
      "Inner CV-fold 3 of 5\n",
      "Inner CV-fold 4 of 5\n",
      "Inner CV-fold 5 of 5\n",
      "Inner_error_values_ for dtc are:[9.0, 12.916666666666666, 12.916666666666666, 14.166666666666666, 11.666666666666666, 11.666666666666666, 12.916666666666666, 14.166666666666666, 11.666666666666666, 12.916666666666666, 11.666666666666666, 14.166666666666666, 14.166666666666666, 12.916666666666666, 14.166666666666666, 11.666666666666666, 12.916666666666666, 14.166666666666666, 14.166666666666666]\n",
      "The index of optimal tc value is: 0\n",
      "The optimal tc value across inner CV folds is: 2\n",
      "Errors for each outer tc fold: [15.0, 5.0, 21.05263157894737]\n",
      "Inner_error_values are:[15.5, 15.5, 15.5, 16.75, 15.416666666666666, 11.666666666666666, 11.666666666666666, 9.166666666666666, 12.916666666666666, 9.166666666666666, 10.416666666666666, 10.5, 10.5, 10.5, 11.75, 11.75, 11.75, 14.333333333333334, 14.333333333333334, 18.166666666666668, 16.916666666666668, 18.166666666666668, 18.166666666666668, 19.416666666666668, 19.416666666666668, 19.416666666666668, 19.416666666666668, 19.416666666666668, 19.416666666666668, 19.416666666666668, 19.416666666666668, 19.416666666666668, 19.416666666666668, 19.416666666666668, 19.416666666666668, 19.416666666666668, 19.416666666666668, 19.416666666666668, 19.416666666666668, 19.416666666666668]\n",
      "The index of optimal KNN value is: 0\n",
      "The optimal KNN value across inner CV folds is: 8\n",
      "Errors for each outer CV fold: [15.0, 15.0, 15.789473684210526]\n",
      "Errors for baseline outer fold[20.0, 10.0, 31.57894736842105]\n",
      "CV-fold 4 of 5\n",
      "Inner CV-fold 1 of 5\n",
      "Inner CV-fold 2 of 5\n",
      "Inner CV-fold 3 of 5\n",
      "Inner CV-fold 4 of 5\n",
      "Inner CV-fold 5 of 5\n",
      "Inner_error_values_ for dtc are:[19.083333333333332, 16.583333333333332, 17.833333333333332, 20.5, 17.833333333333332, 21.833333333333332, 16.5, 21.833333333333332, 20.5, 20.5, 17.833333333333332, 20.5, 17.833333333333332, 21.833333333333332, 21.833333333333332, 20.5, 20.5, 17.833333333333332, 19.166666666666668]\n",
      "The index of optimal tc value is: 0\n",
      "The optimal tc value across inner CV folds is: 8\n",
      "Errors for each outer tc fold: [15.0, 5.0, 21.05263157894737, 15.789473684210526]\n",
      "Inner_error_values are:[17.916666666666668, 16.75, 12.916666666666666, 15.416666666666666, 14.25, 12.916666666666666, 15.583333333333334, 12.916666666666666, 13.0, 14.166666666666666, 14.166666666666666, 15.416666666666666, 14.166666666666666, 15.416666666666666, 14.166666666666666, 15.416666666666666, 16.75, 17.916666666666668, 19.25, 19.25, 19.25, 19.166666666666668, 17.916666666666668, 17.916666666666668, 17.916666666666668, 17.916666666666668, 17.916666666666668, 17.916666666666668, 17.916666666666668, 17.916666666666668, 17.916666666666668, 17.916666666666668, 17.916666666666668, 17.916666666666668, 17.916666666666668, 17.916666666666668, 17.916666666666668, 17.916666666666668, 17.916666666666668, 17.916666666666668]\n",
      "The index of optimal KNN value is: 0\n",
      "The optimal KNN value across inner CV folds is: 3\n",
      "Errors for each outer CV fold: [15.0, 15.0, 15.789473684210526, 15.789473684210526]\n",
      "Errors for baseline outer fold[20.0, 10.0, 31.57894736842105, 36.8421052631579]\n",
      "CV-fold 5 of 5\n",
      "Inner CV-fold 1 of 5\n",
      "Inner CV-fold 2 of 5\n",
      "Inner CV-fold 3 of 5\n",
      "Inner CV-fold 4 of 5\n",
      "Inner CV-fold 5 of 5\n",
      "Inner_error_values_ for dtc are:[10.5, 10.416666666666666, 10.416666666666666, 11.666666666666666, 9.083333333333334, 11.666666666666666, 11.666666666666666, 10.333333333333334, 9.083333333333334, 10.416666666666666, 10.416666666666666, 11.583333333333334, 11.666666666666666, 11.583333333333334, 11.583333333333334, 12.916666666666666, 10.333333333333334, 10.416666666666666, 11.583333333333334]\n",
      "The index of optimal tc value is: 0\n",
      "The optimal tc value across inner CV folds is: 6\n",
      "Errors for each outer tc fold: [15.0, 5.0, 21.05263157894737, 15.789473684210526, 21.05263157894737]\n",
      "Inner_error_values are:[16.75, 18.083333333333332, 9.083333333333334, 14.166666666666668, 11.583333333333334, 11.5, 10.25, 11.5, 11.5, 14.166666666666666, 12.833333333333334, 12.833333333333334, 11.583333333333334, 14.166666666666666, 14.166666666666666, 15.5, 14.166666666666666, 14.25, 12.916666666666666, 15.5, 15.5, 19.333333333333332, 16.75, 16.666666666666668, 14.166666666666666, 21.833333333333332, 20.583333333333332, 24.416666666666668, 24.416666666666668, 24.416666666666668, 23.166666666666668, 24.416666666666668, 24.416666666666668, 24.416666666666668, 24.416666666666668, 24.416666666666668, 24.416666666666668, 24.416666666666668, 24.416666666666668, 24.416666666666668]\n",
      "The index of optimal KNN value is: 0\n",
      "The optimal KNN value across inner CV folds is: 3\n",
      "Errors for each outer CV fold: [15.0, 15.0, 15.789473684210526, 15.789473684210526, 21.05263157894737]\n",
      "Errors for baseline outer fold[20.0, 10.0, 31.57894736842105, 36.8421052631579, 10.526315789473685]\n"
     ]
    }
   ],
   "source": [
    "# Results from the 2-level cross-validation for dtc \n",
    "# Errors for each outer tc fold: [15.0, 30.0, 10.526315789473685, 10.526315789473685, 15.789473684210526]\n",
    "# The errors for the best performing models are: 10.526315789473685, 15.0\n",
    "## Crossvalidation for decision trees\n",
    "## The selection of optimal model is based on an average of the inner errors for each model\n",
    "\n",
    "# Create crossvalidation partition for evaluation\n",
    "K_outer = 5\n",
    "K_inner = 5\n",
    "CV_outer = model_selection.KFold(n_splits=K_outer,shuffle=True)\n",
    "#CV = model_selection.StratifiedKFold(n_splits=K)\n",
    "\n",
    "# dtc\n",
    "index_min_lst_dtc = []\n",
    "min_indices_dtc = []\n",
    "error_outer_dtc = [] # List for the errors in outer CV fold\n",
    "dict_inner_dtc = {}\n",
    "error_inner_dtc = {} # Dict with the errors in the innter CV fold for each tested model\n",
    "# Tree complexity parameter - constraint on maximum depth\n",
    "tc = np.arange(2, 21, 1)\n",
    "classifier_lst_dtc = []\n",
    "\n",
    "for count, value in enumerate(tc):\n",
    "    error_inner_dtc['tc_of_{0}'.format(value)] = []\n",
    "\n",
    "# KNN\n",
    "index_min_lst_KNN = []\n",
    "min_indices_KNN = []\n",
    "error_outer_KNN = [] # List for the errors in outer CV fold\n",
    "dict_inner_KNN = {}\n",
    "error_inner_KNN = {} # Dict with the errors in the innter CV fold for each tested model\n",
    "K_KNN = range(1,41) # Change here for different nearest neighbour crossvalidation - test of K=1-40\n",
    "classifier_lst_KNN = []\n",
    "\n",
    "for count, value in enumerate(K_KNN):\n",
    "    error_inner_KNN['K_KNN_of_{0}'.format(value)] = []\n",
    "    \n",
    "# Baseline model\n",
    "error_baseline = []\n",
    "\n",
    "\n",
    "\n",
    "k=0\n",
    "\n",
    "for train_outer_index, test_outer_index in CV_outer.split(X_classification,y_classification):\n",
    "    print('CV-fold {0} of {1}'.format(k+1,K_outer))\n",
    "    k += 1\n",
    "    \n",
    "    # extract training and test set for current CV fold\n",
    "    X_train_outer = X_classification[train_outer_index,:]\n",
    "    y_train_outer = y_classification[train_outer_index]\n",
    "    X_test_outer = X_classification[test_outer_index,:]\n",
    "    y_test_outer = y_classification[test_outer_index]\n",
    "    \n",
    "    CV_inner = model_selection.KFold(n_splits=K_inner,shuffle=True)\n",
    "    \n",
    "    kk=0\n",
    "    for train_inner_index, test_inner_index in CV_inner.split(X_train_outer,y_train_outer):\n",
    "        print('Inner CV-fold {0} of {1}'.format(kk+1,K_inner))\n",
    "\n",
    "        # extract training and test set for current CV fold\n",
    "        X_train_inner = X_train_outer[train_inner_index,:]\n",
    "        y_train_inner = y_train_outer[train_inner_index]\n",
    "        X_test_inner = X_train_outer[test_inner_index,:]\n",
    "        y_test_inner = y_train_outer[test_inner_index]\n",
    "        \n",
    "#         print(len(X_train_inner))\n",
    "#         print(len(y_train_inner))\n",
    "        \n",
    "    # Decision tree classifier\n",
    "        for count, value in enumerate(tc):\n",
    "            dist=1\n",
    "            \n",
    "            # Fit decision tree classifier, Gini split criterion, different pruning levels\n",
    "            dtc = tree.DecisionTreeClassifier(criterion='gini', max_depth=value) \n",
    "            dtc.fit(X_train_inner, y_train_inner.ravel());\n",
    "            classifier_lst_dtc.append(dtc)\n",
    "            \n",
    "            y_dtc = dtc.predict(X_test_inner);\n",
    "            errordtc_inner = 100*(y_dtc!=y_test_inner).sum().astype(float)/len(y_test_inner)  \n",
    "            #index_min_lst.append(errorKNN_inner) #Append the error values to a list\n",
    "            error_inner_dtc['tc_of_{0}'.format(value)].append(errordtc_inner) # add errors for each fold to each model\n",
    "    \n",
    "    # KNN classifier\n",
    "            for count, value in enumerate(K_KNN):\n",
    "                dist=2 # euclidean_distance\n",
    "                       \n",
    "                knclassifier = KNeighborsClassifier(n_neighbors=value, p=dist);\n",
    "                knclassifier.fit(X_train_inner, y_train_inner);\n",
    "                classifier_lst_KNN.append(knclassifier)\n",
    "            \n",
    "                y_KNN = knclassifier.predict(X_test_inner);\n",
    "                errorKNN_inner = 100*(y_KNN!=y_test_inner).sum().astype(float)/len(y_test_inner)  \n",
    "                #index_min_lst.append(errorKNN_inner) #Append the error values to a list\n",
    "                error_inner_KNN['K_KNN_of_{0}'.format(value)].append(errorKNN_inner) # add errors for each fold to each model\n",
    "        \n",
    "        \n",
    "        kk += 1\n",
    "    #dtc\n",
    "    # Find the dtc value with minimum average error value\n",
    "    for key in error_inner_dtc.keys():\n",
    "        index_min_lst_dtc.append(mean(error_inner_dtc[key]))\n",
    "        \n",
    "    print('Inner_error_values_ for dtc are:' + str(index_min_lst_dtc))\n",
    "    index_min_dtc = np.argmin(index_min_lst_dtc) #Find the index of the minimum error value\n",
    "    top_count_dtc = index_min_dtc\n",
    "    min_indices_dtc.append(index_min_dtc) \n",
    "        \n",
    "    index_min_lst_dtc = [] # Clear for next CV fold\n",
    "    \n",
    "    for key in error_inner_dtc.keys():\n",
    "        error_inner_dtc[key] = [] # Clear for next CV fold\n",
    "        \n",
    "      \n",
    "    print('The index of optimal tc value is: ' + str(top_count))\n",
    "    \n",
    "    optimal_tc = tc[top_count_dtc]\n",
    "    \n",
    "    print('The optimal tc value across inner CV folds is: ' + str(optimal_tc))\n",
    "    \n",
    "    \n",
    "    dtcclassifierOuter = tree.DecisionTreeClassifier(criterion='gini', max_depth=optimal_tc);  #Uses optimal_tc, which was found in the inner CV loop\n",
    "    dtcclassifierOuter.fit(X_train_outer, y_train_outer.ravel());\n",
    "            \n",
    "    y_dtc_outer = dtcclassifierOuter.predict(X_test_outer);\n",
    "    errordtc_outer = 100*(y_dtc_outer!=y_test_outer).sum().astype(float)/len(y_test_outer)\n",
    "    error_outer_dtc.append(errordtc_outer)\n",
    "    print('Errors for each outer tc fold: ' + str(error_outer_dtc))\n",
    "    \n",
    "    \n",
    "    # KNN\n",
    "    # Find the KNN value with minimum average error value\n",
    "    for key in error_inner_KNN.keys():\n",
    "        index_min_lst_KNN.append(mean(error_inner_KNN[key]))\n",
    "        \n",
    "    print('Inner_error_values are:' + str(index_min_lst_KNN))\n",
    "    index_min_KNN = np.argmin(index_min_lst_KNN) #Find the index of the minimum error value\n",
    "    top_count_KNN = index_min_KNN\n",
    "    min_indices_KNN.append(index_min_KNN) \n",
    "        \n",
    "    index_min_lst_KNN = [] # Clear for next CV fold\n",
    "    \n",
    "    for key in error_inner_KNN.keys():\n",
    "        error_inner_KNN[key] = [] # Clear for next CV fold\n",
    "        \n",
    "      \n",
    "    print('The index of optimal KNN value is: ' + str(top_count))\n",
    "    \n",
    "    optimal_K = K_KNN[top_count_KNN]\n",
    "    \n",
    "    print('The optimal KNN value across inner CV folds is: ' + str(optimal_K))\n",
    "    \n",
    "    knclassifierOuter = KNeighborsClassifier(n_neighbors=optimal_K, p=dist); #Uses optimal_K, which was found in the inner CV loop\n",
    "    knclassifierOuter.fit(X_train_outer, y_train_outer);\n",
    "            \n",
    "    y_KNN_outer = knclassifierOuter.predict(X_test_outer);\n",
    "    errorKNN_outer = 100*(y_KNN_outer!=y_test_outer).sum().astype(float)/len(y_test_outer)\n",
    "    error_outer_KNN.append(errorKNN_outer)\n",
    "    print('Errors for each outer CV fold: ' + str(error_outer_KNN))\n",
    " \n",
    "    \n",
    "    \n",
    "    # Fit baseline model\n",
    "            \n",
    "    y_baselinemodel = Baseline_model(y_train_outer, y_test_outer);\n",
    "    error_baseline_outer = 100*(y_baselinemodel!=y_test_outer).sum().astype(float)/len(y_test_outer)  \n",
    "    error_baseline.append(error_baseline_outer)\n",
    "    print('Errors for baseline outer fold'+str(error_baseline))\n",
    "\n",
    "error_dct = error_outer_dtc\n",
    "error_KNN = error_outer_KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.257731958762886\n",
      "16.185567010309278\n"
     ]
    }
   ],
   "source": [
    "# Final generalization error for DCT\n",
    "DTC_error = (len(y_test_outer)/N*np.mat(error_dct)).sum()\n",
    "print(DTC_error)\n",
    "# Final generalization error for KNN\n",
    "KNN_error = (len(y_test_outer)/N*np.mat(error_KNN)).sum()\n",
    "print(KNN_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best performing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer error dct is[15.0, 5.0, 21.05263157894737, 15.789473684210526, 21.05263157894737]\n",
      "Outer error of KNN is[15.0, 15.0, 15.789473684210526, 15.789473684210526, 21.05263157894737]\n",
      "Outer error of baseline_model is[20.0, 10.0, 31.57894736842105, 36.8421052631579, 10.526315789473685]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Cross-validation error [%]')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAG+RJREFUeJzt3XuYHVWZ7/HvjyQQAgSCNBAcQkRurX0wYCOKEQygIgoyMwpkBFEa84znGC4zzsihVZABddRBOejgCYaLwLRwBAFRUYHm0gMEOiE3CAqOAhEk4SEggoEQ3vNHrYadnu7e1TtdtdOp3+d59rOrVl3Wu3fv3u9etapqKSIwM7Pq2qTZAZiZWXM5EZiZVZwTgZlZxTkRmJlVnBOBmVnFORGYmVWcE4GZWcU5EZiZVZwTgZlZxY1tdgB5bLfddjF16tRmh2FmNqrMnz//6YhoqbfeqEgEU6dOpbe3t9lhmJmNKpIezbOeDw2ZmVWcE4GZWcU5EZiZVZwTgZlZxTkRmJlVnBNBhXR1ddHW1saYMWNoa2ujq6ur2SGZ2QZgVJw+auuvq6uLzs5O5s6dy/Tp0+np6aGjowOAmTNnNjk6M2smjYahKtvb28PXEayftrY2LrjgAmbMmPFaWXd3N7Nnz2bp0qVNjMzMiiJpfkS0113PiaAaxowZw+rVqxk3btxrZWvWrGH8+PGsXbu2iZHZxmrbbbdl1apVhdczadIknnnmmcLrGY3yJgL3EVREa2srPT0965T19PTQ2trapIhsY7dq1SoiovBHGclmY+dEUBGdnZ10dHTQ3d3NmjVr6O7upqOjg87OzmaHZmZN5s7iiujrEJ49ezbLli2jtbWVc8891x3FZuY+AjMrhiTK+H4pq57RyH0EZmaWixOBmVnFORGYmVWcE4GZWcU5EZiZVVxhiUDSzpK6JS2T9ICkU1L5tpJ+Jenh9DypqBjMzKy+IlsErwD/GBGtwDuB/yXpLcDpwC0RsTtwS5o3M7MmKSwRRMSTEbEgTT8PLAPeCHwEuCytdhlwVFExmJlZfaX0EUiaCuwDzAN2iIgnIUsWwPaDbDNLUq+k3pUrV5YRpplZJRWeCCRtCVwDnBoRf8q7XUTMiYj2iGhvaWkpLkAzs4orNBFIGkeWBK6MiGtT8VOSJqflk4EVRcZgZmZDK/KsIQFzgWURcV7NohuAE9L0CcD1RcVgZmb1FXn30XcDxwNLJC1MZWcAXwOultQBPAZ8rMAYzMysjsISQUT0ABpk8SFF1WtmZsPjK4vNzCrOicDMrOKcCMzMKs6JwMys4pwIzMwqzonAzKzinAjMzCrOicDMrOKcCMzMKs6JwMys4pwIzMwqzonAzKzinAjMzCrOicDMrOKcCMzMKq7IEcoulrRC0tKasmmS7pG0MA1M/46i6jczs3yKbBFcChzWr+zrwJcjYhrwpTRvZmZNVFgiiIg7gGf6FwMT0/TWwBNF1W9mZvkMOVSlpBty7OOZiPhkzvpOBX4h6ZtkSeiAnNuZmVlB6o1Z3AqcNMRyAd8dRn2fAU6LiGskHQ3MBQ4dcMfSLGAWwJQpU4ZRhZmZDYciYvCF0tERcfWQOxhiHUlTgRsjoi3NPwdsExEhScBzETFxoG1rtbe3R29vb73VzGwDIomhvl9GWz2jkaT5EdFeb70h+wgG+oKXNF7SxKHWGcITwEFp+mDg4WFsa2ZmBah3aGgdkk4Cjgc2kXRnRJwxxLpdwHuB7SQtB84EPg2cL2kssJp06MfMzJqnXmfxERHxk5qiQyPioLRsETBoIoiImYMsevuwozQzs8LUO330bZKul/S2NL9Y0pWSrgAeKDg2MzMrwZAtgog4R9KOwNlZ3y5fArYEJkTE4hLiMzOzguXpI3iB7Pz/3YE5wH3AN4oMyszMyjPkoSFJ5wA/BW4BZkTEkcAi4KeSji8hPjMzK1i9PoIPR8SBZFcAfwIgIm4APgBsW3BsZmZWgnqHhpZKuhzYHLi9rzAiXgHOLzIwMzMrR73O4uMk/Q9gTUQ8VFJMZmZWonp9BPtGxJKhkoCkfUc+LDMzK0u9Q0OXSHov2c3lBjMX2GfEIjIzs1LVSwRbA/MZOhGsHLlwzMysbPX6CKaWFIeZmTWJB683M6s4JwIzs4qrmwiU2bmMYMzMrHx1E0FkQ/9cV0IsZmbWBHkPDd0jab9CIzEzs6bImwhmAHdL+q2kxZKWSBryNtSSLpa0QtLSfuWzJf1a0gOSvt5o4GZmNjLyDlX5wQb2fSnwHeAHfQWSZgAfAfaOiJckbd/Afs3MbATlahFExKPANsAR6bFNKhtqmzuAZ/oVfwb4WkS8lNZZMeyIzcxsROVKBJJOAa4Etk+PKyTNbqC+PYD3SJon6fah+h0kzZLUK6l35UpfvGxmVpS8h4Y6gP0j4gUASf8K3A1c0EB9k4B3AvsBV0vaNZ2ZtI6ImEM2Ihrt7e3/bbmZmY2MvJ3FAtbWzK9l6PsPDWY5cG1k7gVeBbZrYD9mZjZC8rYILgHmSfpxmj+K7K6jw3UdcDBwm6Q9gE2BpxvYj/U5a+uS6nmunHpsoxFnTizl8xlnTiy8jo2dBjgqM/CK2bgD08laAndExP111u8C3kv2i/8p4EzgcuBiYBrwMvC5iLi1Xt3t7e3R29ubK86qkUTev+GGXIdtfMr63PjzOThJ8yOivd56dVsEkjYBFkdEG7AgbwARMXOQRcfl3YeZmRUvzy0mXgUWSZpSQjxmZlayvH0Ek4EHJN0LvNBXGBFHFhKVmZmVJm8i+HKhUZiZWdPk6SMYA3wxIg4tIR4zMytZnj6CtcCLkko6T9HMzMqU99DQamCJpF+xbh/ByYVEZWZmpcmbCH6aHmZmtpHJlQgi4jJJmwNTIuLXBcdkwyQ1creP/CZNmlTo/m3jVfRnE/z5HAm5EoGkI4Bvkt0S4k2SpgFn+/TR5vMVlbah8mdz9Mh707mzgHcAzwJExELgTQXFZGZmJcqbCF6JiP53HXO6NzPbCOTtLF4q6e+AMZJ2B04G7iouLDMzK0veFsFs4K3AS8B/AM8BpxYVlJmZlSfvWUMvAp3pYWZmG5G8LQIzM9tIFZYIJF0saYWkpQMs+5ykkORhKs3MmqzIFsGlwGH9CyXtDLwPeKzAus3MLKe8F5S1AJ8GptZuExEnDrZNRNwhaeoAi74F/DNw/TDiNDOzguQ9ffR64E7gZmBto5VJOhL4Q0QsKuPSczMzqy9vIpgQEZ9fn4okTSA76+j9OdefBcwCmDLFo2SamRUlbx/BjZIOX8+63kx2W4pFkn4P/BWwQNKOA60cEXMioj0i2ltaWtazajMzG0zeFsEpwBmSXgbWpLKIiIl5K4qIJcD2ffMpGbRHxNN592FmZiMvV4sgIraKiE0iYnya3qpeEpDUBdwN7ClpuaSOkQjYzMxGVt4WQV9H74Fp9raIuHGo9SNiZp3lU/PWbWZmxcnVIpD0NbLDQw+mxympzMzMRrm8LYLDgWkR8SqApMuA+4HTiwrMzMzKMZwri7epmd56pAMxM7PmyNsi+Cpwv6RuQGR9Bf+7sKjMzKw0eW9D3SXpNmA/skTw+Yj4Y5GBmZlZOYY8NCRpr/S8LzAZWA48DuyUyszMbJSr1yL4B7LbPPzbAMsCOHjEIzIzs1INmQgiYlaa/GBErK5dJml8YVGZmVlp8p41NNBA9R683sxsIzBkiyDdEO6NwOaS9iHrKAaYCEwoODYzMytBvT6CDwCfJLtT6Hk15c8DZxQUk5mZlaheH8FlwGWS/jYirikpJjMzK1He6wiukfQh4K3A+Jrys4sKzMzMypH3pnPfA44BZpP1E3wM2KXAuMzMrCR5zxo6ICI+AayKiC8D7wJ2Li4sMzMrS95E8Jf0/KKknchGKXtTMSGZmVmZhjNm8TbAN4AFwO+BHw61gaSLJa2QtLSm7BuSHpK0WNKP0z7NzKyJ8g5V+S8R8Ww6c2gXYK+I+GKdzS4FDutX9iugLSL2Bn6D72BqZtZ09S4o+5shlhER1w62PCLukDS1X9kva2bvAT6aL0wzMytKvdNHj0jP2wMHALem+RnAbcCgiSCHE4Gr1mN7MzMbAfUuKPsUgKQbgbdExJNpfjLw3UYrldQJvAJcOcQ6s8jufMqUKVMarcrMzOrI21k8tS8JJE8BezRSoaQTgA8DH4+IGGy9iJgTEe0R0d7S0tJIVWZmlkPeoSpvk/QLoItsHIJjge7hVibpMODzwEER8eJwtzczs5GX9xYTn00dx+9JRXMi4sdDbSOpC3gvsJ2k5cCZZGcJbQb8ShLAPRHx9w3GbmZmIyBvi6DvDKHcncMRMXOA4rl5tzczs3LUO320JyKmS3qe7JDQa4uAiIiJhUZnZmaFq3fW0PT0vFU54ZiZWdnqtQi2HWp5RDwzsuGYmVnZ6vURzCc7JKQBlgWw64hHZGZmpap3aMh3GDUz28jlPmtI0iRgd9YdoeyOIoIyM7Py5EoEkk4CTiEbxH4h8E7gbuDg4kIzM7My5L3FxCnAfsCjETED2AdYWVhUZmZWmryJYHVErAaQtFlEPATsWVxYZmZWlrx9BMvTaGLXkd0eYhXwRHFhmZlZWfLea+iv0+RZkrqBrYGbCovKzMxKk7ez+Hzgqoi4KyJuLzgmMzMrUd4+ggXAFyQ9kgagby8yKDMzK0/ewesvi4jDgXeQDTr/r5IeLjQyMzMrRd4WQZ/dgL2AqcBDIx6NmZmVLlcikNTXAjgbeAB4e0QcUWczMzMbBfKePvo74F0R8XTeHUu6mGxs4hUR0ZbKtgWuImtR/B44OiJWDSdgMzMbWXn7CL7XlwQknZVz35cCh/UrOx24JSJ2B25J82Zm1kTD7SMAODLPSumGdP3HK/gIcFmavgw4qoH6zcxsBDWSCAYamyCvHSLiSYD0vP2glUizJPVK6l250rc1MjMrSiOJ4O0jHsUAImJORLRHRHtLS0sZVZqZVVLes4a+LmmipHFk9xp6WtJxDdT3lKTJaZ+TgRUN7MPMzEZQ3hbB+yPiT2RnAS0H9gD+qYH6bgBOSNMnANc3sA8zMxtBeRPBuPR8ONCVZ9B6SV1kg9fsKWm5pA7ga8D70jUJ70vzZmbWRHmvI/iJpIeAvwD/U1ILsHqoDSJi5iCLDhlGfGZmVrC81xGcDrwLaI+INcALZKeCmpnZKJe3s/hjwCsRsVbSF4ArgJ0KjczMzEqRt4/gixHxvKTpwAfILga7sLiwzMysLHkTwdr0/CHgwoi4Hti0mJDMzKxMeRPBHyT9X+Bo4GeSNhvGtmZmtgHL+2V+NPAL4LCIeBbYlsauIzAzsw1M3rOGXgR+C3xA0meB7SPil4VGZmZmpch71tApwJVkN4nbHrhC0uwiAzMzs3LkvaCsA9g/Il6AbMQysquGLygqMDMzK0fePgLx+plDpOn1uR21mZltIPK2CC4B5kn6cZo/CphbTEhmZlamXIkgIs6TdBswnawl8KmIuL/IwMzMrBx1E4GkTYDFaQD6BcWHZGZmZarbRxARrwKLJE0pIR4zMytZ3j6CycADku4lu/MoABGRayB7MzPbcOVNBF8eyUolnQacBASwhKzPYcjxDczMrBhDJgJJuwE7RMTt/coPBP7QSIWS3gicDLwlIv4i6WrgWODSRvZnZmbrp14fwbeB5wcofzEta9RYYHNJY4EJwBPrsS8zM1sP9RLB1IhY3L8wInqBqY1UGBF/AL4JPAY8CTzn+xaZmTVPvUQwfohlmzdSoaRJZMNcvolslLMtJB03wHqzJPVK6l25cmUjVZmZWQ71EsF9kj7dv1BSBzC/wToPBX4XESvT+MfXAgf0Xyki5kREe0S0t7S0NFiVmZnVU++soVOBH0v6OK9/8beTjU721w3W+RjwTkkTgL8AhwC9De7LzMzW05CJICKeAg6QNANoS8U/jYhbG60wIuZJ+hHZVcqvAPcDcxrdn5mZrZ+89xrqBrpHqtKIOBM4c6T2Z2ZmjfO4w2ZmFedEYGZWcU4EZmYV50RgZlZxTgRmZhXnRGBmVnFOBGZmFedEYGZWcU4EZmYV50RgZlZxTgRmZhXnRGBmVnFOBGZmFedEYGZWcU4EZmYV50RgZlZxTUkEkraR9CNJD0laJuldzYjDzMxyjlBWgPOBmyLio5I2BSY0KQ4zs8orPRFImggcCHwSICJeBl4uOw4zM8s049DQrsBK4BJJ90v6vqQtmhCHmZnRnEQwFtgXuDAi9gFeAE7vv5KkWZJ6JfWuXLmy7BjNzCqjGYlgObA8Iual+R+RJYZ1RMSciGiPiPaWlpZSAzQzq5LSE0FE/BF4XNKeqegQ4MGy4zAzs0yzzhqaDVyZzhj6L+BTTYrDzKzympIIImIh0N6Mus3MbF2+stjMrOKcCMzMKs6JwMys4pwIzMwqzonAzJquq6uLtrY2xowZQ1tbG11dXc0OqVKadfqomRmQJYHOzk7mzp3L9OnT6enpoaOjA4CZM2c2ObpqUEQ0O4a62tvbo7e3t9lhmFkB2trauOCCC5gxY8ZrZd3d3cyePZulS5c2MbLRT9L8iKh7qr4TgZk11ZgxY1i9ejXjxo17rWzNmjWMHz+etWvXNjGy0S9vInAfgZk1VWtrKz09PeuU9fT00Nra2qSIqseJwMyaqrOzk46ODrq7u1mzZg3d3d10dHTQ2dnZ7NAqw53FZtZUfR3Cs2fPZtmyZbS2tnLuuee6o7hE7iMwM9tIuY/AzMxycSIwM6s4JwIzs4pzIjAzqzgnAjOzihsVZw1JWgk82uw4NiLbAU83OwizAfizObJ2iYiWeiuNikRgI0tSb55TyszK5s9mc/jQkJlZxTkRmJlVnBNBNc1pdgBmg/BnswncR2BmVnFuEZiZVZwTQYVIuljSCkke9sk2KJJ2ltQtaZmkBySd0uyYqsSHhipE0oHAn4EfRERbs+Mx6yNpMjA5IhZI2gqYDxwVEQ82ObRKcIugQiLiDuCZZsdh1l9EPBkRC9L088Ay4I3Njao6nAjMbIMiaSqwDzCvuZFUhxOBmW0wJG0JXAOcGhF/anY8VeFEYGYbBEnjyJLAlRFxbbPjqRInAjNrOkkC5gLLIuK8ZsdTNU4EFSKpC7gb2FPSckkdzY7JLHk3cDxwsKSF6XF4s4OqCp8+amZWcW4RmJlVnBOBmVnFORGYmVWcE4GZWcU5EZiZVZwTwSgi6c8104dLeljSlAHWWy7pqpr5YyV9v6w4+8VyoqQdB1l2haTHJW2a5neU9Eid/Y2RdGeOepdL2maA8nMknZo3/tEkvZ+/k7RI0m8kXSZppwb3tb+kbw2xfOfaz1ijJN2QThV9RNJzNaeO7r+++7b8nAhGIUmHABcAh0XEY4Ostr+kPUe43rENbHYiMGAiSAI4Ie/OImJtRLyngTjWW4Ovv2ynRcTbgL2AJcCt6YrdYYmIeRFx2hDLH4+IY9Yjzr79HBkR04C/B7ojYlp6rHOfIUlj1rcuG5wTwSgj6T3ARcCHIuK3Q6z6b8AZA2y/paRLJd0r6X5JR6TyN0u6M5XN7/tFJulQSTdL+iFwfyo7IW2/UNK/S9pE0lhJl0taImmppJMlHQNMA65K6246QJzfAj430D+6pNNTPYslfSmVjZX0bJoeI+l76f71P5F0k6SjanZxano9iyXtUVO+T7r3/cOSTkz72kTSeSn2JZI+OtDrl7SVpJ+nX91L+9arJ8X6qKSJaV6S/kvSdqnFtjTtszvP/uqJiFcj4ptkd5t9f6rzg5LulrRA0lWStkjl+6fyRZLmSZqQXvd1afnBadnCtO0WknaTtDAt3zy1Ppak5Qem8pMk/UjSL9J7/dXhvAZJf5T0BUl3AUdK2kPSL9Pn8zZJu6X1dpR0naT7UvzvGIn3sFIiwo9R8gDWkP1j711nveXAdsCvgTcBxwLfT8u+DhybpicBvwHGAxOA8al8L2Bemj6UbAyDKWm+DbgOGJvm5wB/B+wP/Lwmhm3Scw8wbZA4rwCOAn5AdlXpjsAjadnhwL8DIvvBchNwADAWeDatcyzwk7R8J+A5snvY970Hn0nTJwPfS9PnAAvSa94+rbcDcEyqY0yK4/G0vP/rPwa4sOY1bD2Mv993gePT9LuBm9L0MmCH2vetwc/HFX2vv6bsO8A/ptdyOzAhlXeS/VAYD/wO2Lfv9aT34FDgulT2c2D/NL1lWr4bsDCVfR64KE2/FXgU2BQ4CXgY2ArYPL2nOw0S+2v11ZT9ETi5Zv52YGqaPgj4WZq+BtgvTe8KLG72/+poe7hFMLqsAe4C8twa4hWyVsHp/crfD3SmX3PdZF8EU4DNgLnKRi/7IfCWmm3ujtcPQR0K7Af0pn0cBLwZeITs1hXnS/oA2ZdyXl8h+zKp/Ty+H/ggWStkAdkXzx79tpsOXB3Zr98nyL4oavXduGw+MLWm/LqIWB0RK4A70uuZDvxHZIee/kiWwNoHeP2LgcMkfU3SuyNiOK/zKrJEAlkS6zvG/p/ADySdxMi30pWeDyD7m96V/m4fJ3tPWoHH4vWxAJ6LiLX99vGfwLclzQYmDrB8OnB52v4B4AmyvxfAzRHxfET8BXiI7LM2HFcBSNqO7O90XYr/fLLkD3AIcFEqvxZ4wyCtTxvEaDjmaa97FTgauFnSGRHxlfSBvzctvzYizq5Z/1Lgn8l+9fcR2a/GdQ4rSTqH7BfbccA4sl/BfV7ot/3FEfHF/sFJ2pvsy/tk4G+BWXleVEQ8JOlB4G/61XNORMztV8fYfusM5aX0vJZ1P+v976sSdfb12uuPiGWS2slaLN+QdGNEfKVOHH3uBC6V9AbgSKDvPfw0WYvqw8AiSXtHxKqc+6xnGvBTskR/U0QcX7tQ0r789/djHRFxjqQbgA8B90l6b79thnrvXqqZ7v93yKPvvRfwVGT9Ca9XLPXV3R4Rrwxz35a4RTDKRMSLZF8YH5fUEREvx+sdbGf3W/dl4P8AteO//oLsixoASfukya2BJyNrX5/A4P/cNwNHp19oSHqDpCmSWsjuXfX/gDOBfdP6z5MdGqjnXOCf+sXZUXMc+6/66qzRA3w0HW+fDByYox6AoyRtlvb3HqCXrGVwbDqWvwPZoZve/htKeiPw54i4HDiv5nXWld7b64FvA4si4tm0aNeIuIcsMaxiBEbmSu/JacAbgF+RtSQPkrRrWr6FpN2BB4BdUkJA0kT166+R9OaIWBwRXyVrofU/CeEOshYGklqByWQtxBETESuBVZKOTPVskhJmALcCn6mJd9ogu7FBOBGMQhHxDHAY8AVJH6mz+kVkx2v7fBmYkDr2HgDOSuXfAU6SdA+wC+v+kqute0nax82SFgO/JDvGvjNwR2qeX8TrHdWXAN/X4J3FfftdBCyqmf8Z8CPgHklLgKvJjk/XuhpYASwlO/4+j3yHpO4jO+59N3BmRDyV6nooxXAz8A/p0FF/byP7VbyQrLWVtzXQ5yqyVlftqZffSq9xCdmhlKXKTs+8YZj77tvXIrL+oWnAwRGxJr3GDrKO+0VkiWGPiHgJmAlcmMp/SdZ6qPW51Jm9GHg2rVPrAmDz9BquBD6RfoSMtKOBz6Y4l5K1yiBLAjOUnRTwINmZajYMvvuojWqStoyIP6cWyTyyTs2VzY7LbDRxH4GNdj9XdkrmOLJf904CZsPkFoGZWcW5j8DMrOKcCMzMKs6JwMys4pwIzMwqzonAzKzinAjMzCru/wOJCGiPzlsqvgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14a3d23aef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The two best performing models are from dct\n",
    "print('Outer error dct is' + str(error_dct))\n",
    "print('Outer error of KNN is' + str(error_KNN))\n",
    "print('Outer error of baseline_model is' + str(error_baseline))\n",
    "\n",
    "figure()\n",
    "boxplot([error_KNN, error_dct])\n",
    "xlabel('K-Nearest Neighbors   vs.   Decision Tree')\n",
    "ylabel('Cross-validation error [%]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of methods (based on all the different models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 and baseline model are not significantly different\n",
      "-7.973368972967936\n",
      "20.394421604546878\n",
      "Baseline model and Model 2 are not significantly different\n",
      "-11.335478981899378\n",
      "21.861794771373056\n",
      "Model 1 and Model 2 are not significantly different\n",
      "-7.838695795598217\n",
      "5.9439589534929524\n"
     ]
    }
   ],
   "source": [
    "# The best performing classifiers are KNN and dtc\n",
    "\n",
    "## Crossvalidation\n",
    "# Create crossvalidation partition for evaluation\n",
    "K = K_outer\n",
    "CV = model_selection.KFold(n_splits=K,shuffle=True)\n",
    "#CV = model_selection.StratifiedKFold(n_splits=K)\n",
    "\n",
    "# Initialize variables\n",
    "Error_base_line = np.asarray(error_baseline)\n",
    "Error_model_1 = np.asarray(error_dct)\n",
    "Error_model_2 = np.asarray(error_KNN)\n",
    "\n",
    "# Comparison of Baseline model with model 1\n",
    "\n",
    "# Test if classifiers are significantly different using methods in section 9.3.3\n",
    "# by computing credibility interval. Notice this can also be accomplished by computing the p-value using\n",
    "# [tstatistic, pvalue] = stats.ttest_ind(Error_logreg,Error_dectree)\n",
    "# and test if the p-value is less than alpha=0.05. \n",
    "z = (Error_base_line-Error_model_1)\n",
    "zb = z.mean()\n",
    "nu = K-1\n",
    "sig =  (z-zb).std()  / np.sqrt(K-1)\n",
    "alpha = 0.05\n",
    "\n",
    "zL = zb + sig * stats.t.ppf(alpha/2, nu);\n",
    "zH = zb + sig * stats.t.ppf(1-alpha/2, nu);\n",
    "\n",
    "if zL <= 0 and zH >= 0 :\n",
    "    print('Model 1 and baseline model are not significantly different')        \n",
    "else:\n",
    "    print('Model 1 and baseline model are significantly different.')\n",
    "print(zL)\n",
    "print(zH)\n",
    "\n",
    "# Comparison of Baseline model with model 2\n",
    "\n",
    "# Test if classifiers are significantly different using methods in section 9.3.3\n",
    "# by computing credibility interval. Notice this can also be accomplished by computing the p-value using\n",
    "# [tstatistic, pvalue] = stats.ttest_ind(Error_logreg,Error_dectree)\n",
    "# and test if the p-value is less than alpha=0.05. \n",
    "z = (Error_base_line-Error_model_2)\n",
    "zb = z.mean()\n",
    "nu = K-1\n",
    "sig =  (z-zb).std()  / np.sqrt(K-1)\n",
    "alpha = 0.05\n",
    "\n",
    "zL = zb + sig * stats.t.ppf(alpha/2, nu);\n",
    "zH = zb + sig * stats.t.ppf(1-alpha/2, nu);\n",
    "\n",
    "if zL <= 0 and zH >= 0 :\n",
    "    print('Baseline model and Model 2 are not significantly different')        \n",
    "else:\n",
    "    print('Baseline model and Model 2 are significantly different.')\n",
    "print(zL)\n",
    "print(zH)\n",
    "\n",
    "# Comparison of model 1 with model 2\n",
    "\n",
    "# Test if classifiers are significantly different using methods in section 9.3.3\n",
    "# by computing credibility interval. Notice this can also be accomplished by computing the p-value using\n",
    "# [tstatistic, pvalue] = stats.ttest_ind(Error_logreg,Error_dectree)\n",
    "# and test if the p-value is less than alpha=0.05. \n",
    "z = (Error_model_1-Error_model_2)\n",
    "zb = z.mean()\n",
    "nu = K-1\n",
    "sig =  (z-zb).std()  / np.sqrt(K-1)\n",
    "alpha = 0.05\n",
    "\n",
    "zL = zb + sig * stats.t.ppf(alpha/2, nu);\n",
    "zH = zb + sig * stats.t.ppf(1-alpha/2, nu);\n",
    "\n",
    "if zL <= 0 and zH >= 0 :\n",
    "    print('Model 1 and Model 2 are not significantly different')        \n",
    "else:\n",
    "    print('Model 1 and Model 2 are significantly different.')\n",
    "print(zL)\n",
    "print(zH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of two single models (selected based on error) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV-fold 1 of 10\n",
      "CV-fold 2 of 10\n",
      "CV-fold 3 of 10\n",
      "CV-fold 4 of 10\n",
      "CV-fold 5 of 10\n",
      "CV-fold 6 of 10\n",
      "CV-fold 7 of 10\n",
      "CV-fold 8 of 10\n",
      "CV-fold 9 of 10\n",
      "CV-fold 10 of 10\n",
      "Model 1 and baseline model are significantly different.\n",
      "22.752118136756003\n",
      "47.025659641021775\n",
      "Baseline model and Model 2 are significantly different.\n",
      "31.743820015413128\n",
      "50.4784022068091\n",
      "Model 1 and Model 2 are not significantly different\n",
      "-0.11660365964019714\n",
      "12.561048104084641\n"
     ]
    }
   ],
   "source": [
    "# The best performing classifiers are KNN with K = 2 and a dtc with tc=10\n",
    "\n",
    "## Crossvalidation\n",
    "# Create crossvalidation partition for evaluation\n",
    "K = 10\n",
    "CV = model_selection.KFold(n_splits=K,shuffle=True)\n",
    "#CV = model_selection.StratifiedKFold(n_splits=K)\n",
    "\n",
    "# Initialize variables\n",
    "Error_base_line = np.empty((K,1))\n",
    "Error_model_1 = np.empty((K,1))\n",
    "Error_model_2 = np.empty((K,1))\n",
    "\n",
    "n_tested=0\n",
    "\n",
    "k=0\n",
    "for train_index, test_index in CV.split(X_classification,y_classification):\n",
    "    print('CV-fold {0} of {1}'.format(k+1,K))\n",
    "    \n",
    "    # extract training and test set for current CV fold\n",
    "    X_train = X[train_index,:]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index,:]\n",
    "    y_test = y[test_index]\n",
    "\n",
    "    # Fit and evaluate KNN\n",
    "    model1 = KNeighborsClassifier(n_neighbors=2, p=dist);\n",
    "    model1 = model1.fit(X_train, y_train)\n",
    "    y_model1 = model1.predict(X_test)\n",
    "    Error_model_1[k] = 100*(y_model1!=y_test).sum().astype(float)/len(y_test)\n",
    "    \n",
    "    # Fit and evaluate Decision Tree classifier\n",
    "    model2 = tree.DecisionTreeClassifier(criterion='gini', max_depth=10);\n",
    "    model2 = model2.fit(X_train, y_train.ravel())\n",
    "    y_model2 = model2.predict(X_test)\n",
    "    Error_model_2[k] = 100*(y_model2!=y_test).sum().astype(float)/len(y_test)  \n",
    "    \n",
    "    # Fit and evaluate baseline model classifier\n",
    "    y_baseline = Baseline_model(y_train, y_test);\n",
    "    Error_base_line[k] = 100*(y_baseline!=y_test).sum().astype(float)/len(y_test)\n",
    "  \n",
    "    k+=1\n",
    "\n",
    "# Comparison of Baseline model with model 1\n",
    "\n",
    "# Test if classifiers are significantly different using methods in section 9.3.3\n",
    "# by computing credibility interval. Notice this can also be accomplished by computing the p-value using\n",
    "# [tstatistic, pvalue] = stats.ttest_ind(Error_logreg,Error_dectree)\n",
    "# and test if the p-value is less than alpha=0.05. \n",
    "z = (Error_base_line-Error_model_1)\n",
    "zb = z.mean()\n",
    "nu = K-1\n",
    "sig =  (z-zb).std()  / np.sqrt(K-1)\n",
    "alpha = 0.05\n",
    "\n",
    "zL = zb + sig * stats.t.ppf(alpha/2, nu);\n",
    "zH = zb + sig * stats.t.ppf(1-alpha/2, nu);\n",
    "\n",
    "if zL <= 0 and zH >= 0 :\n",
    "    print('Model 1 and baseline model are not significantly different')        \n",
    "else:\n",
    "    print('Model 1 and baseline model are significantly different.')\n",
    "print(zL)\n",
    "print(zH)\n",
    "\n",
    "# Comparison of Baseline model with model 2\n",
    "\n",
    "# Test if classifiers are significantly different using methods in section 9.3.3\n",
    "# by computing credibility interval. Notice this can also be accomplished by computing the p-value using\n",
    "# [tstatistic, pvalue] = stats.ttest_ind(Error_logreg,Error_dectree)\n",
    "# and test if the p-value is less than alpha=0.05. \n",
    "z = (Error_base_line-Error_model_2)\n",
    "zb = z.mean()\n",
    "nu = K-1\n",
    "sig =  (z-zb).std()  / np.sqrt(K-1)\n",
    "alpha = 0.05\n",
    "\n",
    "zL = zb + sig * stats.t.ppf(alpha/2, nu);\n",
    "zH = zb + sig * stats.t.ppf(1-alpha/2, nu);\n",
    "\n",
    "if zL <= 0 and zH >= 0 :\n",
    "    print('Baseline model and Model 2 are not significantly different')        \n",
    "else:\n",
    "    print('Baseline model and Model 2 are significantly different.')\n",
    "print(zL)\n",
    "print(zH)\n",
    "\n",
    "# Comparison of model 1 with model 2\n",
    "\n",
    "# Test if classifiers are significantly different using methods in section 9.3.3\n",
    "# by computing credibility interval. Notice this can also be accomplished by computing the p-value using\n",
    "# [tstatistic, pvalue] = stats.ttest_ind(Error_logreg,Error_dectree)\n",
    "# and test if the p-value is less than alpha=0.05. \n",
    "z = (Error_model_1-Error_model_2)\n",
    "zb = z.mean()\n",
    "nu = K-1\n",
    "sig =  (z-zb).std()  / np.sqrt(K-1)\n",
    "alpha = 0.05\n",
    "\n",
    "zL = zb + sig * stats.t.ppf(alpha/2, nu);\n",
    "zH = zb + sig * stats.t.ppf(1-alpha/2, nu);\n",
    "\n",
    "if zL <= 0 and zH >= 0 :\n",
    "    print('Model 1 and Model 2 are not significantly different')        \n",
    "else:\n",
    "    print('Model 1 and Model 2 are significantly different.')\n",
    "print(zL)\n",
    "print(zH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gammel kode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 and baseline model are significantly different.\n",
      "5.0\n",
      "5.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_parse_args() missing 1 required positional argument: 'df'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-3aadf24c2f4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0mzL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzb\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msig\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mppf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[0mzH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzb\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msig\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mppf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py\u001b[0m in \u001b[0;36mppf\u001b[1;34m(self, q, *args, **kwds)\u001b[0m\n\u001b[0;32m   1898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1899\u001b[0m         \"\"\"\n\u001b[1;32m-> 1900\u001b[1;33m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1901\u001b[0m         \u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1902\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: _parse_args() missing 1 required positional argument: 'df'"
     ]
    }
   ],
   "source": [
    "# Initialize variables\n",
    "Error_base_line = min(error_baseline)\n",
    "Error_model_1 = min(error_dct) # Gives the best performing model\n",
    "Error_model_2 = sorted(error_dct)[1] # Gives the second best performing model\n",
    "\n",
    "# Comparison of Baseline model with model 1\n",
    "\n",
    "# Test if classifiers are significantly different using methods in section 9.3.3\n",
    "# by computing credibility interval. Notice this can also be accomplished by computing the p-value using\n",
    "# [tstatistic, pvalue] = stats.ttest_ind(Error_logreg,Error_dectree)\n",
    "# and test if the p-value is less than alpha=0.05. \n",
    "z = (Error_base_line-Error_model_1)\n",
    "zb = z.mean()\n",
    "nu = K-1\n",
    "sig =  (z-zb).std()  / np.sqrt(K-1)\n",
    "alpha = 0.05\n",
    "\n",
    "zL = zb + sig * stats.t.ppf(alpha/2, nu);\n",
    "zH = zb + sig * stats.t.ppf(1-alpha/2, nu);\n",
    "\n",
    "if zL <= 0 and zH >= 0 :\n",
    "    print('Model 1 and baseline model are not significantly different')        \n",
    "else:\n",
    "    print('Model 1 and baseline model are significantly different.')\n",
    "print(zL)\n",
    "print(zH)\n",
    "\n",
    "# Comparison of Baseline model with model 2\n",
    "\n",
    "# Test if classifiers are significantly different using methods in section 9.3.3\n",
    "# by computing credibility interval. Notice this can also be accomplished by computing the p-value using\n",
    "# [tstatistic, pvalue] = stats.ttest_ind(Error_logreg,Error_dectree)\n",
    "# and test if the p-value is less than alpha=0.05. \n",
    "z = (Error_base_line-Error_model_2)\n",
    "zb = z.mean()\n",
    "nu = K-1\n",
    "sig =  (z-zb).std()  / np.sqrt(K-1)\n",
    "alpha = 0.05\n",
    "\n",
    "zL = zb + sig * stats.t.ppf(alpha/2);\n",
    "zH = zb + sig * stats.t.ppf(1-alpha/2);\n",
    "\n",
    "if zL <= 0 and zH >= 0 :\n",
    "    print('Model 2 and baseline model are not significantly different')        \n",
    "else:\n",
    "    print('Model 2 and baseline model are significantly different.')\n",
    "print(zL)\n",
    "print(zH)\n",
    "\n",
    "# Comparison of model 1 with model 2\n",
    "\n",
    "# Test if classifiers are significantly different using methods in section 9.3.3\n",
    "# by computing credibility interval. Notice this can also be accomplished by computing the p-value using\n",
    "# [tstatistic, pvalue] = stats.ttest_ind(Error_logreg,Error_dectree)\n",
    "# and test if the p-value is less than alpha=0.05. \n",
    "z = (Error_model_1-Error_model_2)\n",
    "zb = z.mean()\n",
    "nu = K-1\n",
    "sig =  (z-zb).std()  / np.sqrt(K-1)\n",
    "alpha = 0.05\n",
    "\n",
    "zL = zb + sig * stats.t.ppf(alpha/2, nu);\n",
    "zH = zb + sig * stats.t.ppf(1-alpha/2, nu);\n",
    "\n",
    "if zL <= 0 and zH >= 0 :\n",
    "    print('Model 1 and Model 2 are not significantly different')        \n",
    "else:\n",
    "    print('Model 1 and Model 2 are significantly different.')\n",
    "print(zL)\n",
    "print(zH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two level cross validation for KNN - Greta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Crossvalidation for KNN\n",
    "# Create crossvalidation partition for evaluation\n",
    "K_outer = 5\n",
    "K_inner = 5\n",
    "CV_outer = model_selection.KFold(n_splits=K_outer,shuffle=True)\n",
    "#CV = model_selection.StratifiedKFold(n_splits=K)\n",
    "index_min_lst = []\n",
    "min_indices = []\n",
    "error_outer = [] # List for the errors in outer CV fold\n",
    "\n",
    "# Initialize variables\n",
    "#Error_logreg = np.empty((K_outer,1))\n",
    "#Error_KNN_inner = np.empty((K_inner,3))\n",
    "#Error_NaveB = np.empty((K_outer,1))\n",
    "#n_tested=0\n",
    "\n",
    "K_KNN = [1, 2, 3] # Change here for different nearest neighbour crossvalidation\n",
    "\n",
    "k=0\n",
    "classifier_lst = []\n",
    "\n",
    "for train_outer_index, test_outer_index in CV_outer.split(X_classification,y_classification):\n",
    "    print('CV-fold {0} of {1}'.format(k+1,K_outer))\n",
    "    k += 1\n",
    "    \n",
    "    # extract training and test set for current CV fold\n",
    "    X_train_outer = X_classification[train_outer_index,:]\n",
    "    y_train_outer = y_classification[train_outer_index]\n",
    "    X_test_outer = X_classification[test_outer_index,:]\n",
    "    y_test_outer = y_classification[test_outer_index]\n",
    "    \n",
    "    CV_inner = model_selection.KFold(n_splits=K_inner,shuffle=True)\n",
    "    \n",
    "    kk=0\n",
    "    for train_inner_index, test_inner_index in CV_inner.split(X_train_outer,y_train_outer):\n",
    "        print('Inner CV-fold {0} of {1}'.format(kk+1,K_inner))\n",
    "\n",
    "        # extract training and test set for current CV fold\n",
    "        X_train_inner = X_train_outer[train_inner_index,:]\n",
    "        y_train_inner = y_train_outer[train_inner_index]\n",
    "        X_test_inner = X_train_outer[test_inner_index,:]\n",
    "        y_test_inner = y_train_outer[test_inner_index]\n",
    "        \n",
    "#         print(len(X_train_inner))\n",
    "#         print(len(y_train_inner))\n",
    "        \n",
    "        for count, value in enumerate(K_KNN):\n",
    "            dist=1\n",
    "    \n",
    "            knclassifier = KNeighborsClassifier(n_neighbors=value, p=dist);\n",
    "            knclassifier.fit(X_train_inner, y_train_inner);\n",
    "            classifier_lst.append(knclassifier)\n",
    "            \n",
    "            y_KNN = knclassifier.predict(X_test_inner);\n",
    "            errorKNN_inner = 100*(y_KNN!=y_test_inner).sum().astype(float)/len(y_test_inner)  \n",
    "            index_min_lst.append(errorKNN_inner) #Append the error values to a list\n",
    "            \n",
    "        # Find the KNN value with least error value\n",
    "        index_min = np.argmin(index_min_lst) #Find the index of the minimum error value\n",
    "        min_indices.append(index_min) \n",
    "        \n",
    "        index_min_lst = [] # Clear for next CV fold\n",
    "        \n",
    "        kk += 1\n",
    "\n",
    "        counts = np.bincount(min_indices) # Count which index appears the maximum number of times = i.e. the must be the least error value\n",
    "        top_count = np.argmax(counts)\n",
    "        # Only use the count from the last iteration! \n",
    "        optimal_K = K_KNN[top_count]\n",
    "        \n",
    "    print('The index of optimal KNN value is: ' + str(top_count))\n",
    "\n",
    "    print('The optimal KNN value across inner CV folds is: ' + str(optimal_K))\n",
    "    \n",
    "    knclassifierOuter = KNeighborsClassifier(n_neighbors=optimal_K, p=dist); #Uses optimal_K, which was found in the inner CV loop\n",
    "    knclassifierOuter.fit(X_train_outer, y_train_outer);\n",
    "            \n",
    "    y_KNN_outer = knclassifier.predict(X_test_outer);\n",
    "    errorKNN_outer = 100*(y_KNN_outer!=y_test_outer).sum().astype(float)/len(y_test_outer)\n",
    "    error_outer.append(errorKNN_outer)\n",
    "    print('Errors for each outer CV fold: ' + str(error_outer))\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Crossvalidation for KNN\n",
    "# Create crossvalidation partition for evaluation\n",
    "K_outer = 5\n",
    "K_inner = 5\n",
    "CV_outer = model_selection.KFold(n_splits=K_outer,shuffle=True)\n",
    "#CV = model_selection.StratifiedKFold(n_splits=K)\n",
    "errorKNN_inner_GT=[]\n",
    "errorKNN_outer_GT = []\n",
    "index_min_lst = []\n",
    "min_indices = []\n",
    "error_outer = []\n",
    "\n",
    "# Initialize variables\n",
    "#Error_logreg = np.empty((K_outer,1))\n",
    "#Error_KNN_inner = np.empty((K_inner,3))\n",
    "#Error_NaveB = np.empty((K_outer,1))\n",
    "#n_tested=0\n",
    "\n",
    "K_KNN = range(1,41)\n",
    "\n",
    "k=0\n",
    "classifier_lst = []\n",
    "\n",
    "for train_outer_index, test_outer_index in CV_outer.split(X_classification,y_classification):\n",
    "    print('CV-fold {0} of {1}'.format(k+1,K_outer))\n",
    "    k += 1\n",
    "    \n",
    "    # extract training and test set for current CV fold\n",
    "    X_train_outer = X_classification[train_outer_index,:]\n",
    "    y_train_outer = y_classification[train_outer_index]\n",
    "    X_test_outer = X_classification[test_outer_index,:]\n",
    "    y_test_outer = y_classification[test_outer_index]\n",
    "    \n",
    "    CV_inner = model_selection.KFold(n_splits=K_inner,shuffle=True)\n",
    "    \n",
    "    kk=0\n",
    "    for train_inner_index, test_inner_index in CV_inner.split(X_train_outer,y_train_outer):\n",
    "        print('Inner CV-fold {0} of {1}'.format(kk+1,K_inner))\n",
    "\n",
    "        # extract training and test set for current CV fold\n",
    "        X_train_inner = X_train_outer[train_inner_index,:]\n",
    "        y_train_inner = y_train_outer[train_inner_index]\n",
    "        X_test_inner = X_train_outer[test_inner_index,:]\n",
    "        y_test_inner = y_train_outer[test_inner_index]\n",
    "        \n",
    "#         print(len(X_train_inner))\n",
    "#         print(len(y_train_inner))\n",
    "        \n",
    "        for count, value in enumerate(K_KNN):\n",
    "            dist=1\n",
    "    \n",
    "            knclassifier = KNeighborsClassifier(n_neighbors=value, p=dist);\n",
    "            knclassifier.fit(X_train_inner, y_train_inner);\n",
    "            classifier_lst.append(knclassifier)\n",
    "            \n",
    "            y_KNN = knclassifier.predict(X_test_inner);\n",
    "            errorKNN_inner_GT = 100*(y_KNN!=y_test_inner).sum().astype(float)/len(y_test_inner)  \n",
    "            index_min_lst.append(errorKNN_inner_GT) #Append the error values to a list\n",
    "            \n",
    "        # Find the KNN value with least error value\n",
    "        index_min = np.argmin(index_min_lst) #Find the index of the minimum error value\n",
    "        min_indices.append(index_min) \n",
    "        \n",
    "        index_min_lst = [] # Clear for next CV fold\n",
    "        \n",
    "        kk += 1\n",
    "\n",
    "    print(min_indices)\n",
    "    counts = np.bincount(min_indices) # Count which index appears the maximum number of times = i.e. the must be the least error value\n",
    "    top_count = np.argmax(counts)\n",
    "    print('The index of optimal KNN value is: ' + str(top_count))\n",
    "    \n",
    "    # Only use the count from the last iteration! \n",
    "    optimal_K = K_KNN[top_count]\n",
    "    print('The optimal KNN value across inner CV folds is: ' + str(optimal_K))\n",
    "    \n",
    "    knclassifierOuter = KNeighborsClassifier(n_neighbors=1, p=dist);\n",
    "    knclassifierOuter.fit(X_train_outer, y_train_outer);\n",
    "            \n",
    "    y_KNN_outer = knclassifier.predict(X_test_outer);\n",
    "    errorKNN_outer_GT = 100*(y_KNN_outer!=y_test_outer).sum().astype(float)/len(y_test_outer)\n",
    "    error_outer.append(errorKNN_outer_GT)\n",
    "    print(error_outer)\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
