{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "The purpose of this application is to solve relevant classification and regression problems for the prostate dataset for use in the project in 02450 Intro to Machine Learning\n",
    "\n",
    "Author: Naia Wright\n",
    "\n",
    "Reviewed by:  \n",
    "\n",
    "Last modified: 28/10/18, 09:39\n",
    "\n",
    "#### Change-log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import of required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from scipy.linalg import svd\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn.linear_model as lm\n",
    "from matplotlib.pylab import figure, plot, xlabel, ylabel, legend, ylim, show\n",
    "\n",
    "from matplotlib.pyplot import figure, boxplot, xlabel, ylabel, show\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn import model_selection, tree\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import model_selection\n",
    "\n",
    "from matplotlib.pyplot import figure, plot, subplot, title, xlabel, ylabel, show, clim\n",
    "from scipy.io import loadmat\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn import model_selection\n",
    "from toolbox_02450 import feature_selector_lr, bmplot\n",
    "import numpy as np\n",
    "\n",
    "from statistics import mean\n",
    "import graphviz\n",
    "from numpy import array\n",
    "from scipy import stats\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a method for importing a spread_sheet using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataLoader(path, sheet):\n",
    "    \"\"\"\n",
    "    Method for importing data from a spreadsheet.\n",
    "\n",
    "    :param path: full path to the spreadsheet to load\n",
    "    :param sheet: name of the sheet in the workbook that is loaded\n",
    "    :return: pandas dataFrame with imported data\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    out = pd.read_excel(path, sheet_name=sheet)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify path and sheet name in the prostate workbook\n",
    "base_dir = os.getcwd()\n",
    "filePath = base_dir + '/Data/Prostate.xlsx'\n",
    "#filePath = 'C:/Users/PeterBakke/Documents/git/ML_fall2018/Data/Prostate.xlsx'\n",
    "#filePath = 'C:/Users/Greta/Documents/Github/ML_fall2018/Data/Prostate.xlsx'\n",
    "#filePath = 'C:/Users/narisa/Documents/GitHub/ML_fall2018/Data/Prostate.xlsx'\n",
    "sheet = 'Sheet1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load prostate data into dataFrame\n",
    "myData = DataLoader(path=filePath, sheet=sheet)\n",
    "\n",
    "# delete irrelevant columns\n",
    "del myData['ID']\n",
    "del myData['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lCaVol', 'lWeight', 'Age', 'lBPH', 'SVI', 'lCP', 'Gleason', 'pgg45', 'lPSA']\n"
     ]
    }
   ],
   "source": [
    "# Extract class names \n",
    "attributeNames = list(myData.columns.values)\n",
    "\n",
    "# Convert dataFrame to numpy array\n",
    "X = myData.values\n",
    "\n",
    "# Compute values of N (observations) and M (features)\n",
    "M = len(attributeNames)\n",
    "N = X.shape[0]\n",
    "\n",
    "print(attributeNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting and deleting PGG45 and Gleason from X, as well as SVI (to not normalize SVI)\n",
    "X_orig = np.copy(X)\n",
    "\n",
    "gleason = X_orig[:,6]\n",
    "pgg = X_orig[:,7]\n",
    "svi = X_orig[:,4]\n",
    "\n",
    "# print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBS - only run this once\n",
    "X = np.delete(X,6,1) # Deletes Gleason\n",
    "X = np.delete(X,6,1) # Deletes PGG\n",
    "X = np.delete(X,4,1) # Deletes SVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z score all other variables \n",
    "X_z = zscore(X)\n",
    "# print(X_z)\n",
    "\n",
    "# Current order: lCavol, lWeight, Age, lBPH, lCP, lPSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One out of K coding for PGG and Gleason\n",
    "\n",
    "from categoric2numeric import *\n",
    "\n",
    "[X_Gleason, attribute_names_Gleason]=categoric2numeric(gleason)\n",
    "[X_PGG45, attribute_names_PGG45]=categoric2numeric(pgg)\n",
    "\n",
    "#print(X_Gleason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97, 1)\n",
      "(97, 11)\n",
      "[-1.58702059 -2.20015441  1.36823439 -1.03002898 -0.86765522 -2.29971238\n",
      "  0.          1.          0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Add one out of K coded Gleason and PGG columns, as well as SVI \n",
    "svi = np.reshape(svi,[97,1])\n",
    "print(svi.shape)\n",
    "# X_k = np.concatenate((X,X_Gleason,X_PGG45,svi),axis=1)\n",
    "X_k = np.concatenate((X_z,X_Gleason,svi),axis=1)\n",
    "\n",
    "print(X_k.shape)\n",
    "print(X_k[2])\n",
    "\n",
    "# Order: lCavol, lWeight, Age, lBPH, lCP, lPSA, Gleason (4 columns), (PGG (19 columns)), SVI (1 column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove attribute 10 (SVI) and Gleason from X\n",
    "X_classification = X_k[:,[0,1,2,3,4,5,6,7,8,9]]\n",
    "\n",
    "# Use attribute 5 (SVI) as y\n",
    "y_classification = X_k[:,10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "N, M = X_classification.shape\n",
    "\n",
    "print(N)\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lCaVol', 'lWeight', 'Age', 'lBPH', 'SVI', 'lCP', 'Gleason', 'pgg45', 'lPSA']\n",
      "{6: 0, 7: 1, 8: 2, 9: 3}\n"
     ]
    }
   ],
   "source": [
    "# extract class names and encode with integers (dict)\n",
    "\n",
    "#classLabels = myData['Gleason'].values.tolist()\n",
    "#classNames = sorted(set(classLabels))\n",
    "#classDict = dict(zip(classNames, range(4)))\n",
    "\n",
    "#del myData['Gleason']\n",
    "\n",
    "#attributeNames = list(myData.columns.values)\n",
    "\n",
    "#print(attributeNames)\n",
    "#print(classDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract vector y, convert to NumPy array\n",
    "#y = np.asarray([classDict[value] for value in classLabels])\n",
    "\n",
    "# Convert dataFrame to numpy array\n",
    "#X = myData.values\n",
    "\n",
    "# Compute values of N, M and C\n",
    "#N = len(y)\n",
    "#M = len(attributeNames)\n",
    "#C = len(classNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1.]\n"
     ]
    }
   ],
   "source": [
    "# Normalize data with mean and std\n",
    "#Y = (X - np.ones((N,1))*X.mean(axis=0)) / X.std(axis=0)\n",
    "print(y_classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lCavol' 'lWeight' 'Age' 'lBPH' 'lCP' 'lPSA' 'Gleason 1' 'Gleason 2'\n",
      " 'Gleason 3' 'Gleason 4']\n"
     ]
    }
   ],
   "source": [
    "# Remove attribute 5 (SVI) from X\n",
    "#X_classification = Y[:,[0,1,2,3,5,6,7,8]]\n",
    "#print(X_classification)\n",
    "# Use attribute 5 (SVI) as y\n",
    "#y_classification = X[:,4]\n",
    "#print(y_classification)\n",
    "# Remove attribute 5 (SVI) from attribute names\n",
    "\n",
    "#lCavol, lWeight, Age, lBPH, lCP, lPSA, Gleason (4 columns), (PGG (19 columns)), SVI (1 column)\n",
    "\n",
    "attributeNames_classification = np.array(['lCavol', 'lWeight', 'Age', 'lBPH', 'lCP', 'lPSA', 'Gleason 1', 'Gleason 2', 'Gleason 3', 'Gleason 4'])\n",
    "print(attributeNames_classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two level cross validation for KNN - Naia 2018-11-03 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV-fold 1 of 5\n",
      "Inner CV-fold 1 of 5\n",
      "Inner CV-fold 2 of 5\n",
      "Inner CV-fold 3 of 5\n",
      "Inner CV-fold 4 of 5\n",
      "Inner CV-fold 5 of 5\n",
      "Inner_error_values are:[14.333333333333334, 10.5, 11.75, 9.083333333333334, 13.0, 14.416666666666668, 13.083333333333334, 14.416666666666668, 13.083333333333334, 14.416666666666668, 14.416666666666668, 14.416666666666668, 14.416666666666668, 13.083333333333334, 13.083333333333334, 14.416666666666668, 14.416666666666668, 13.083333333333334, 15.75, 14.333333333333334, 13.0, 15.583333333333334, 15.583333333333334, 16.916666666666668, 16.916666666666668, 19.5, 18.25, 22.083333333333336, 22.083333333333336, 22.083333333333336, 22.083333333333336, 22.083333333333336, 22.083333333333336, 22.083333333333336, 22.083333333333336, 22.083333333333336, 22.083333333333336, 22.083333333333336, 22.083333333333336, 22.083333333333336]\n",
      "The index of optimal KNN value is: 3\n",
      "The optimal KNN value across inner CV folds is: 4\n",
      "Errors for each outer CV fold: [25.0]\n",
      "CV-fold 2 of 5\n",
      "Inner CV-fold 1 of 5\n",
      "Inner CV-fold 2 of 5\n",
      "Inner CV-fold 3 of 5\n",
      "Inner CV-fold 4 of 5\n",
      "Inner CV-fold 5 of 5\n",
      "Inner_error_values are:[15.75, 19.5, 19.5, 15.5, 13.0, 13.0, 14.333333333333334, 14.333333333333334, 15.583333333333334, 15.583333333333334, 19.583333333333336, 15.666666666666666, 14.333333333333334, 15.583333333333334, 13.0, 11.666666666666668, 13.0, 13.0, 13.0, 16.916666666666668, 15.666666666666668, 17.0, 15.666666666666668, 18.25, 18.25, 22.0, 22.0, 24.666666666666668, 23.333333333333332, 24.666666666666668, 24.666666666666668, 23.333333333333332, 23.333333333333332, 23.333333333333332, 23.333333333333332, 23.333333333333332, 23.333333333333332, 23.333333333333332, 23.333333333333332, 23.333333333333332]\n",
      "The index of optimal KNN value is: 15\n",
      "The optimal KNN value across inner CV folds is: 16\n",
      "Errors for each outer CV fold: [25.0, 10.0]\n",
      "CV-fold 3 of 5\n",
      "Inner CV-fold 1 of 5\n",
      "Inner CV-fold 2 of 5\n",
      "Inner CV-fold 3 of 5\n",
      "Inner CV-fold 4 of 5\n",
      "Inner CV-fold 5 of 5\n",
      "Inner_error_values are:[14.083333333333334, 17.916666666666668, 15.333333333333334, 17.916666666666668, 14.0, 16.5, 17.833333333333332, 17.833333333333332, 16.583333333333332, 19.166666666666668, 17.833333333333332, 16.583333333333332, 15.333333333333334, 15.333333333333334, 15.333333333333334, 15.333333333333334, 15.333333333333334, 16.583333333333332, 16.583333333333332, 15.25, 15.333333333333334, 15.333333333333334, 15.333333333333334, 16.583333333333332, 16.583333333333332, 20.416666666666668, 20.416666666666668, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75]\n",
      "The index of optimal KNN value is: 4\n",
      "The optimal KNN value across inner CV folds is: 5\n",
      "Errors for each outer CV fold: [25.0, 10.0, 15.789473684210526]\n",
      "CV-fold 4 of 5\n",
      "Inner CV-fold 1 of 5\n",
      "Inner CV-fold 2 of 5\n",
      "Inner CV-fold 3 of 5\n",
      "Inner CV-fold 4 of 5\n",
      "Inner CV-fold 5 of 5\n",
      "Inner_error_values are:[18.083333333333336, 19.416666666666668, 19.416666666666668, 18.0, 15.5, 16.833333333333336, 15.5, 16.833333333333336, 14.25, 16.75, 16.75, 15.5, 15.5, 16.833333333333332, 15.5, 19.416666666666668, 16.75, 19.333333333333336, 16.75, 19.416666666666668, 20.666666666666668, 19.416666666666668, 20.666666666666668, 19.416666666666668, 19.416666666666668, 21.916666666666668, 20.666666666666668, 21.916666666666668, 21.916666666666668, 20.583333333333332, 20.583333333333332, 20.583333333333332, 20.583333333333332, 20.583333333333332, 20.583333333333332, 20.583333333333332, 20.583333333333332, 20.583333333333332, 20.583333333333332, 20.583333333333332]\n",
      "The index of optimal KNN value is: 8\n",
      "The optimal KNN value across inner CV folds is: 9\n",
      "Errors for each outer CV fold: [25.0, 10.0, 15.789473684210526, 15.789473684210526]\n",
      "CV-fold 5 of 5\n",
      "Inner CV-fold 1 of 5\n",
      "Inner CV-fold 2 of 5\n",
      "Inner CV-fold 3 of 5\n",
      "Inner CV-fold 4 of 5\n",
      "Inner CV-fold 5 of 5\n",
      "Inner_error_values are:[16.833333333333336, 18.166666666666668, 15.583333333333334, 16.916666666666668, 15.666666666666668, 14.333333333333334, 15.583333333333334, 15.583333333333334, 16.916666666666668, 16.916666666666668, 18.166666666666668, 16.916666666666668, 14.416666666666668, 16.833333333333332, 11.75, 14.333333333333334, 11.833333333333334, 13.166666666666668, 11.833333333333334, 17.0, 18.25, 18.25, 17.0, 19.5, 19.5, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75]\n",
      "The index of optimal KNN value is: 14\n",
      "The optimal KNN value across inner CV folds is: 15\n",
      "Errors for each outer CV fold: [25.0, 10.0, 15.789473684210526, 15.789473684210526, 21.05263157894737]\n"
     ]
    }
   ],
   "source": [
    "## Crossvalidation for KNN\n",
    "## The selection of optimal model is based on an average of the inner errors for each model\n",
    "\n",
    "# Create crossvalidation partition for evaluation\n",
    "K_outer = 5\n",
    "K_inner = 5\n",
    "CV_outer = model_selection.KFold(n_splits=K_outer,shuffle=True)\n",
    "#CV = model_selection.StratifiedKFold(n_splits=K)\n",
    "index_min_lst = []\n",
    "min_indices = []\n",
    "error_outer = [] # List for the errors in outer CV fold\n",
    "dict_inner = {}\n",
    "error_inner = {} # Dict with the errors in the innter CV fold for each tested model\n",
    "K_KNN = range(1,41) # Change here for different nearest neighbour crossvalidation - test of K=1-40\n",
    "\n",
    "for count, value in enumerate(K_KNN):\n",
    "    error_inner['K_KNN_of_{0}'.format(value)] = []\n",
    "\n",
    "k=0\n",
    "classifier_lst = []\n",
    "\n",
    "for train_outer_index, test_outer_index in CV_outer.split(X_classification,y_classification):\n",
    "    print('CV-fold {0} of {1}'.format(k+1,K_outer))\n",
    "    k += 1\n",
    "    \n",
    "    # extract training and test set for current CV fold\n",
    "    X_train_outer = X_classification[train_outer_index,:]\n",
    "    y_train_outer = y_classification[train_outer_index]\n",
    "    X_test_outer = X_classification[test_outer_index,:]\n",
    "    y_test_outer = y_classification[test_outer_index]\n",
    "    \n",
    "    CV_inner = model_selection.KFold(n_splits=K_inner,shuffle=True)\n",
    "    \n",
    "    kk=0\n",
    "    for train_inner_index, test_inner_index in CV_inner.split(X_train_outer,y_train_outer):\n",
    "        print('Inner CV-fold {0} of {1}'.format(kk+1,K_inner))\n",
    "\n",
    "        # extract training and test set for current CV fold\n",
    "        X_train_inner = X_train_outer[train_inner_index,:]\n",
    "        y_train_inner = y_train_outer[train_inner_index]\n",
    "        X_test_inner = X_train_outer[test_inner_index,:]\n",
    "        y_test_inner = y_train_outer[test_inner_index]\n",
    "        \n",
    "#         print(len(X_train_inner))\n",
    "#         print(len(y_train_inner))\n",
    "        \n",
    "        for count, value in enumerate(K_KNN):\n",
    "            dist=2 #euclidean_distance\n",
    "                       \n",
    "            knclassifier = KNeighborsClassifier(n_neighbors=value, p=dist);\n",
    "            knclassifier.fit(X_train_inner, y_train_inner);\n",
    "            classifier_lst.append(knclassifier)\n",
    "            \n",
    "            y_KNN = knclassifier.predict(X_test_inner);\n",
    "            errorKNN_inner = 100*(y_KNN!=y_test_inner).sum().astype(float)/len(y_test_inner)  \n",
    "            #index_min_lst.append(errorKNN_inner) #Append the error values to a list\n",
    "            error_inner['K_KNN_of_{0}'.format(value)].append(errorKNN_inner) # add errors for each fold to each model\n",
    "            \n",
    "        kk += 1\n",
    "        \n",
    "    # Find the KNN value with minimum average error value\n",
    "    for key in error_inner.keys():\n",
    "        index_min_lst.append(mean(error_inner[key]))\n",
    "        \n",
    "    print('Inner_error_values are:' + str(index_min_lst))\n",
    "    index_min = np.argmin(index_min_lst) #Find the index of the minimum error value\n",
    "    top_count = index_min\n",
    "    min_indices.append(index_min) \n",
    "        \n",
    "    index_min_lst = [] # Clear for next CV fold\n",
    "    \n",
    "    for key in error_inner.keys():\n",
    "        error_inner[key] = [] # Clear for next CV fold\n",
    "        \n",
    "      \n",
    "    print('The index of optimal KNN value is: ' + str(top_count))\n",
    "    \n",
    "    optimal_K = K_KNN[top_count]\n",
    "    \n",
    "    print('The optimal KNN value across inner CV folds is: ' + str(optimal_K))\n",
    "    \n",
    "    knclassifierOuter = KNeighborsClassifier(n_neighbors=optimal_K, p=dist); #Uses optimal_K, which was found in the inner CV loop\n",
    "    knclassifierOuter.fit(X_train_outer, y_train_outer);\n",
    "            \n",
    "    y_KNN_outer = knclassifierOuter.predict(X_test_outer);\n",
    "    errorKNN_outer = 100*(y_KNN_outer!=y_test_outer).sum().astype(float)/len(y_test_outer)\n",
    "    error_outer.append(errorKNN_outer)\n",
    "    print('Errors for each outer CV fold: ' + str(error_outer))\n",
    "error_KNN = error_outer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two level cross validation for decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "CV-fold 1 of 5\n",
      "Inner CV-fold 1 of 5\n",
      "Inner CV-fold 2 of 5\n",
      "Inner CV-fold 3 of 5\n",
      "Inner CV-fold 4 of 5\n",
      "Inner CV-fold 5 of 5\n",
      "Inner_error_values are:[16.666666666666668, 18.0, 13.916666666666666, 19.166666666666668, 17.833333333333332, 16.5, 15.25, 15.25, 17.833333333333332, 19.166666666666668, 19.083333333333332, 16.5, 16.583333333333332, 19.166666666666668, 19.083333333333332, 17.833333333333332, 16.5, 17.916666666666668]\n",
      "The index of optimal tc value is: 2\n",
      "The optimal tc value across inner CV folds is: 4\n",
      "Errors for each outer tc fold: [10.0]\n",
      "CV-fold 2 of 5\n",
      "Inner CV-fold 1 of 5\n",
      "Inner CV-fold 2 of 5\n",
      "Inner CV-fold 3 of 5\n",
      "Inner CV-fold 4 of 5\n",
      "Inner CV-fold 5 of 5\n",
      "Inner_error_values are:[18.416666666666668, 19.75, 18.416666666666668, 19.666666666666668, 20.916666666666668, 19.666666666666668, 18.416666666666668, 18.416666666666668, 18.416666666666668, 18.416666666666668, 18.416666666666668, 18.416666666666668, 20.916666666666668, 18.416666666666668, 18.416666666666668, 20.916666666666668, 19.666666666666668, 18.416666666666668]\n",
      "The index of optimal tc value is: 0\n",
      "The optimal tc value across inner CV folds is: 2\n",
      "Errors for each outer tc fold: [10.0, 10.0]\n",
      "CV-fold 3 of 5\n",
      "Inner CV-fold 1 of 5\n",
      "Inner CV-fold 2 of 5\n",
      "Inner CV-fold 3 of 5\n",
      "Inner CV-fold 4 of 5\n",
      "Inner CV-fold 5 of 5\n",
      "Inner_error_values are:[20.666666666666668, 15.5, 15.583333333333334, 18.166666666666668, 15.5, 15.5, 16.833333333333336, 15.583333333333334, 16.833333333333336, 14.25, 15.5, 18.166666666666668, 15.5, 15.5, 15.5, 15.5, 16.833333333333336, 16.833333333333336]\n",
      "The index of optimal tc value is: 9\n",
      "The optimal tc value across inner CV folds is: 11\n",
      "Errors for each outer tc fold: [10.0, 10.0, 15.789473684210526]\n",
      "CV-fold 4 of 5\n",
      "Inner CV-fold 1 of 5\n",
      "Inner CV-fold 2 of 5\n",
      "Inner CV-fold 3 of 5\n",
      "Inner CV-fold 4 of 5\n",
      "Inner CV-fold 5 of 5\n",
      "Inner_error_values are:[17.916666666666668, 16.666666666666668, 15.25, 14.083333333333334, 12.75, 15.333333333333334, 11.416666666666666, 11.5, 14.083333333333334, 12.75, 15.333333333333334, 15.333333333333334, 11.5, 12.75, 16.666666666666668, 12.666666666666666, 12.75, 15.333333333333334]\n",
      "The index of optimal tc value is: 6\n",
      "The optimal tc value across inner CV folds is: 8\n",
      "Errors for each outer tc fold: [10.0, 10.0, 15.789473684210526, 15.789473684210526]\n",
      "CV-fold 5 of 5\n",
      "Inner CV-fold 1 of 5\n",
      "Inner CV-fold 2 of 5\n",
      "Inner CV-fold 3 of 5\n",
      "Inner CV-fold 4 of 5\n",
      "Inner CV-fold 5 of 5\n",
      "Inner_error_values are:[11.75, 7.833333333333334, 9.166666666666666, 10.416666666666666, 11.75, 10.5, 10.416666666666666, 11.75, 10.5, 11.75, 11.75, 9.166666666666666, 11.75, 9.166666666666666, 10.416666666666666, 10.5, 11.75, 11.75]\n",
      "The index of optimal tc value is: 1\n",
      "The optimal tc value across inner CV folds is: 3\n",
      "Errors for each outer tc fold: [10.0, 10.0, 15.789473684210526, 15.789473684210526, 15.789473684210526]\n"
     ]
    }
   ],
   "source": [
    "## Crossvalidation for decision trees\n",
    "## The selection of optimal model is based on an average of the inner errors for each model\n",
    "\n",
    "# Create crossvalidation partition for evaluation\n",
    "K_outer = 5\n",
    "K_inner = 5\n",
    "CV_outer = model_selection.KFold(n_splits=K_outer,shuffle=True)\n",
    "#CV = model_selection.StratifiedKFold(n_splits=K)\n",
    "index_min_lst = []\n",
    "min_indices = []\n",
    "error_outer = [] # List for the errors in outer CV fold\n",
    "dict_inner = {}\n",
    "error_inner = {} # Dict with the errors in the innter CV fold for each tested model\n",
    "# Tree complexity parameter - constraint on maximum depth\n",
    "tc = np.arange(2, 20, 1)\n",
    "print(tc)\n",
    "\n",
    "\n",
    "for count, value in enumerate(tc):\n",
    "    error_inner['tc_of_{0}'.format(value)] = []\n",
    "\n",
    "k=0\n",
    "classifier_lst = []\n",
    "\n",
    "for train_outer_index, test_outer_index in CV_outer.split(X_classification,y_classification):\n",
    "    print('CV-fold {0} of {1}'.format(k+1,K_outer))\n",
    "    k += 1\n",
    "    \n",
    "    # extract training and test set for current CV fold\n",
    "    X_train_outer = X_classification[train_outer_index,:]\n",
    "    y_train_outer = y_classification[train_outer_index]\n",
    "    X_test_outer = X_classification[test_outer_index,:]\n",
    "    y_test_outer = y_classification[test_outer_index]\n",
    "    \n",
    "    CV_inner = model_selection.KFold(n_splits=K_inner,shuffle=True)\n",
    "    \n",
    "    kk=0\n",
    "    for train_inner_index, test_inner_index in CV_inner.split(X_train_outer,y_train_outer):\n",
    "        print('Inner CV-fold {0} of {1}'.format(kk+1,K_inner))\n",
    "\n",
    "        # extract training and test set for current CV fold\n",
    "        X_train_inner = X_train_outer[train_inner_index,:]\n",
    "        y_train_inner = y_train_outer[train_inner_index]\n",
    "        X_test_inner = X_train_outer[test_inner_index,:]\n",
    "        y_test_inner = y_train_outer[test_inner_index]\n",
    "        \n",
    "#         print(len(X_train_inner))\n",
    "#         print(len(y_train_inner))\n",
    "        \n",
    "        for count, value in enumerate(tc):\n",
    "            dist=1\n",
    "            \n",
    "            # Fit decision tree classifier, Gini split criterion, different pruning levels\n",
    "            dtc = tree.DecisionTreeClassifier(criterion='gini', max_depth=value) \n",
    "            dtc.fit(X_train_inner, y_train_inner.ravel());\n",
    "            classifier_lst.append(dtc)\n",
    "            \n",
    "            y_dtc = dtc.predict(X_test_inner);\n",
    "            errordtc_inner = 100*(y_dtc!=y_test_inner).sum().astype(float)/len(y_test_inner)  \n",
    "            #index_min_lst.append(errorKNN_inner) #Append the error values to a list\n",
    "            error_inner['tc_of_{0}'.format(value)].append(errordtc_inner) # add errors for each fold to each model\n",
    "            \n",
    "        kk += 1\n",
    "        \n",
    "    # Find the KNN value with minimum average error value\n",
    "    for key in error_inner.keys():\n",
    "        index_min_lst.append(mean(error_inner[key]))\n",
    "        \n",
    "    print('Inner_error_values are:' + str(index_min_lst))\n",
    "    index_min = np.argmin(index_min_lst) #Find the index of the minimum error value\n",
    "    top_count = index_min\n",
    "    min_indices.append(index_min) \n",
    "        \n",
    "    index_min_lst = [] # Clear for next CV fold\n",
    "    \n",
    "    for key in error_inner.keys():\n",
    "        error_inner[key] = [] # Clear for next CV fold\n",
    "        \n",
    "      \n",
    "    print('The index of optimal tc value is: ' + str(top_count))\n",
    "    \n",
    "    optimal_tc = tc[top_count]\n",
    "    \n",
    "    print('The optimal tc value across inner CV folds is: ' + str(optimal_tc))\n",
    "    \n",
    "    \n",
    "    dtcclassifierOuter = tree.DecisionTreeClassifier(criterion='gini', max_depth=optimal_tc);  #Uses optimal_tc, which was found in the inner CV loop\n",
    "    dtcclassifierOuter.fit(X_train_outer, y_train_outer.ravel());\n",
    "            \n",
    "    y_dtc_outer = dtcclassifierOuter.predict(X_test_outer);\n",
    "    errordtc_outer = 100*(y_dtc_outer!=y_test_outer).sum().astype(float)/len(y_test_outer)\n",
    "    error_outer.append(errordtc_outer)\n",
    "    print('Errors for each outer tc fold: ' + str(error_outer))\n",
    "\n",
    "error_dct = error_outer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two level cross validation for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV-fold 1 of 5\n",
      "Inner CV-fold 1 of 5\n",
      "[[-1.9993129  -0.72575948 -0.79198919 -1.03002898 -0.86765522 -2.29971238\n",
      "   1.          0.          0.          0.        ]\n",
      " [-1.58702059 -2.20015441  1.36823439 -1.03002898 -0.86765522 -2.29971238\n",
      "   0.          1.          0.          0.        ]\n",
      " [-0.5105128  -0.46121762 -0.25193329 -1.03002898 -0.86765522 -1.83463099\n",
      "   1.          0.          0.          0.        ]\n",
      " [-2.04670586 -0.93880639 -1.87210098 -1.03002898 -0.86765522 -1.49161747\n",
      "   1.          0.          0.          0.        ]\n",
      " [-0.5226677  -0.3646778   0.01809466  0.35670122 -0.86765522 -1.49161747\n",
      "   1.          0.          0.          0.        ]\n",
      " [-0.56020767 -0.20984103 -0.79198919  0.99529051 -0.86765522 -1.4141616\n",
      "   1.          0.          0.          0.        ]\n",
      " [-1.81362657 -0.20984103 -2.2771429  -1.03002898 -0.86765522 -1.24618021\n",
      "   1.          0.          0.          0.        ]\n",
      " [-0.9610521  -0.90192675 -0.11691932 -1.03002898 -0.86765522 -1.24618021\n",
      "   1.          0.          0.          0.        ]\n",
      " [-0.93418834 -0.05819996  0.15310863 -1.03002898 -0.86765522 -1.05492666\n",
      "   1.          0.          0.          0.        ]\n",
      " [-0.12284403 -0.4385849  -0.92700316 -1.03002898 -0.1807427  -0.94018137\n",
      "   0.          1.          0.          0.        ]\n",
      " [ 0.16302259 -1.33245984  0.2881226  -1.03002898 -0.86765522 -0.89820677\n",
      "   1.          0.          0.          0.        ]\n",
      " [ 0.80038343  0.04790351  0.2881226  -1.03002898  0.39606027 -0.85816274\n",
      "   1.          0.          0.          0.        ]\n",
      " [-1.63076627 -0.84767487 -3.08722674 -1.03002898 -0.86765522 -0.80135103\n",
      "   1.          0.          0.          0.        ]\n",
      " [-0.9958673   0.46089542  0.8281785   1.07937567 -0.86765522 -0.76543644\n",
      "   1.          0.          0.          0.        ]\n",
      " [-0.17279428 -0.4917387  -0.65697521 -1.03002898 -0.86765522 -0.73094466\n",
      "   1.          0.          0.          0.        ]\n",
      " [ 0.60486894 -0.30009502 -0.52196124  0.95226146  1.09806826 -0.71419787\n",
      "   0.          1.          0.          0.        ]\n",
      " [-1.61593366 -0.59376893 -0.65697521 -0.62277959 -0.86765522 -0.68164068\n",
      "   1.          0.          0.          0.        ]\n",
      " [-0.82278841  0.09023368  0.69316453  1.03860789 -0.86765522 -0.65025697\n",
      "   1.          0.          0.          0.        ]\n",
      " [ 0.08264956 -1.18343728  0.55815055  0.13839656 -0.86765522 -0.619965\n",
      "   1.          0.          0.          0.        ]\n",
      " [-0.71399732  0.21283185  0.15310863 -1.03002898 -0.44509826 -0.59069151\n",
      "   0.          1.          0.          0.        ]\n",
      " [-1.49290981  0.55616587  0.42313658  1.18900155 -0.86765522 -0.57641572\n",
      "   0.          1.          0.          0.        ]\n",
      " [-0.0636628  -1.38806321  0.96319247  0.80827605 -0.86765522 -0.40942861\n",
      "   1.          0.          0.          0.        ]\n",
      " [-1.14287478 -0.84767487 -1.33204508 -1.03002898 -0.86765522 -0.39781765\n",
      "   1.          0.          0.          0.        ]\n",
      " [-1.15993242 -0.96684975 -0.11691932 -1.03002898 -0.44509826 -0.3750503\n",
      "   1.          0.          0.          0.        ]\n",
      " [-0.03554419  1.15183144  0.01809466  1.43488428 -0.86765522 -0.34197776\n",
      "   0.          1.          0.          0.        ]\n",
      " [-0.76124438 -2.94238594  0.01809466 -1.03002898 -0.86765522 -0.24968869\n",
      "   0.          1.          0.          0.        ]\n",
      " [-0.47120382 -1.44501572 -1.06201713  0.5790429   0.01211097 -0.17513581\n",
      "   0.          1.          0.          0.        ]\n",
      " [-0.65481608  0.55616587 -0.25193329  1.11787737 -0.1807427  -0.13158654\n",
      "   1.          0.          0.          0.        ]\n",
      " [ 0.35951816  0.62873791 -0.38694727 -1.03002898  0.71191882 -0.09011178\n",
      "   0.          1.          0.          0.        ]\n",
      " [ 0.1160991  -0.51489465  0.2881226   1.14240568 -0.1807427   0.0377352\n",
      "   0.          1.          0.          0.        ]\n",
      " [ 0.26772493 -0.55400096 -0.38694727  0.35670122 -0.86765522  0.06527282\n",
      "   0.          1.          0.          0.        ]\n",
      " [-0.15936323  0.95303849  0.55815055  1.11787737 -0.1807427   0.07872178\n",
      "   0.          1.          0.          0.        ]\n",
      " [-0.11017138 -0.14270309  0.8281785   0.8822505  -0.44509826  0.09851369\n",
      "   1.          0.          0.          0.        ]\n",
      " [ 0.26448829  1.421615    0.01809466  1.36687051 -0.86765522  0.17346783\n",
      "   1.          0.          0.          0.        ]\n",
      " [-0.71399732  0.0110004   0.01809466  0.96483055  0.16402005  0.17943223\n",
      "   0.          1.          0.          0.        ]\n",
      " [ 0.66269388  1.15563954  0.55815055  1.15435171  1.16912805  0.18535613\n",
      "   0.          1.          0.          0.        ]\n",
      " [ 1.53819093 -0.26497044 -0.65697521 -1.03002898 -0.86765522  0.1970843\n",
      "   0.          1.          0.          0.        ]\n",
      " [-0.07083973  1.52790617  0.2881226   1.40088236 -0.86765522  0.2086566\n",
      "   0.          1.          0.          0.        ]\n",
      " [-0.75586358  0.31848951 -2.00711495  0.91647239 -0.86765522  0.27503575\n",
      "   1.          0.          0.          0.        ]\n",
      " [-0.24626419  0.52151525 -0.38694727  0.82752319 -0.86765522  0.29086898\n",
      "   0.          1.          0.          0.        ]\n",
      " [-0.76124438  2.10127925  1.23322042  1.54225202 -0.86765522  0.31663434\n",
      "   1.          0.          0.          0.        ]\n",
      " [ 1.2159132  -0.2441444   1.09820645 -1.03002898  1.24908762  0.32673071\n",
      "   0.          0.          0.          1.        ]\n",
      " [-1.53197866  1.82921036  0.69316453 -1.03002898 -0.86765522  0.42173538\n",
      "   1.          0.          0.          0.        ]\n",
      " [-0.1331195   2.70166094  1.09820645  1.54225202 -0.44509826  0.43068977\n",
      "   0.          1.          0.          0.        ]\n",
      " [-0.16203258 -0.67539077  1.77327632  1.14240568 -0.86765522  0.48675094\n",
      "   0.          1.          0.          0.        ]\n",
      " [-0.11521787  0.46089542  0.69316453 -1.03002898  0.28936185  0.50329884\n",
      "   0.          1.          0.          0.        ]\n",
      " [ 1.40654082  0.51652225  0.69316453 -1.03002898  1.50170584  0.69391731\n",
      "   0.          1.          0.          0.        ]\n",
      " [ 0.56363872  1.88843646  1.09820645  1.40088236  0.48894996  0.79630031\n",
      "   0.          1.          0.          0.        ]\n",
      " [ 1.01288993  1.70306453  1.90829029  1.54225202 -0.86765522  0.83354436\n",
      "   0.          1.          0.          0.        ]\n",
      " [ 0.10052143 -1.31058265  0.2881226   0.31819952  0.28936185  0.90356948\n",
      "   0.          1.          0.          0.        ]\n",
      " [ 0.99242046 -0.3646778  -0.92700316  0.23411435  1.80201365  0.91641341\n",
      "   0.          1.          0.          0.        ]\n",
      " [ 1.077152    0.60960358  1.77327632 -0.43510323  0.5312501   0.94648734\n",
      "   0.          1.          0.          0.        ]\n",
      " [ 1.132233    0.49140008  0.15310863  0.7030969   1.38643629  0.95140024\n",
      "   0.          0.          0.          1.        ]\n",
      " [ 0.18109221  0.1899692  -0.52196124  1.10527971  0.71191882  0.96597463\n",
      "   0.          1.          0.          0.        ]\n",
      " [ 1.66548696 -0.25800887  0.01809466 -1.03002898  1.80201365  1.00368795\n",
      "   0.          1.          0.          0.        ]\n",
      " [ 0.18109221  0.15525053  1.63826234  0.5790429   0.71191882  1.31945687\n",
      "   0.          1.          0.          0.        ]\n",
      " [ 1.61742159  1.10952004  0.55815055 -1.03002898 -0.86765522  1.3509826\n",
      "   1.          0.          0.          0.        ]\n",
      " [ 1.26244405  0.58060761  0.55815055 -1.03002898  1.07914887  1.66041493\n",
      "   0.          1.          0.          0.        ]\n",
      " [ 2.10739693  0.62873791 -2.68218482 -1.03002898  1.68826718  1.92104373\n",
      "   0.          1.          0.          0.        ]\n",
      " [ 1.30704467  0.34014146  0.55815055  1.0100326   1.24908762  2.61164875\n",
      "   0.          1.          0.          0.        ]\n",
      " [ 1.80971922  0.81196061  0.55815055  0.23411435  2.21673517  2.70345173\n",
      "   0.          1.          0.          0.        ]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X must be non-negative",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-af75c218e0c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mnbclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_prior\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mest_prior\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[0mnbclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_inner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_inner\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m             \u001b[0mclassifier_lst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbclassifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    602\u001b[0m         self.feature_count_ = np.zeros((n_effective_classes, n_features),\n\u001b[0;32m    603\u001b[0m                                        dtype=np.float64)\n\u001b[1;32m--> 604\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    605\u001b[0m         \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    706\u001b[0m         \u001b[1;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 708\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input X must be non-negative\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    709\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input X must be non-negative"
     ]
    }
   ],
   "source": [
    "### OBS. method can not use X values < 0\n",
    "\n",
    "## Crossvalidation for Naive Bayes\n",
    "## The selection of optimal model is based on an average of the inner errors for each model\n",
    "\n",
    "# Create crossvalidation partition for evaluation\n",
    "K_outer = 5\n",
    "K_inner = 5\n",
    "CV_outer = model_selection.KFold(n_splits=K_outer,shuffle=True)\n",
    "#CV = model_selection.StratifiedKFold(n_splits=K)\n",
    "index_min_lst = []\n",
    "min_indices = []\n",
    "error_outer = np.empty((1,1)) # List for the errors in outer CV fold\n",
    "dict_inner = {}\n",
    "error_inner = {} # Dict with the errors in the innter CV fold for each tested model\n",
    "\n",
    "# Naive Bayes classifier parameters\n",
    "alpha = [1]         # additive parameter (e.g. Laplace correction), lÃ¦gges til da log bliver taget i MultinormilNB\n",
    "\n",
    "for count, value in enumerate(K_KNN):\n",
    "    error_inner['K_KNN_of_{0}'.format(value)] = []\n",
    "\n",
    "k=0\n",
    "classifier_lst = []\n",
    "\n",
    "for train_outer_index, test_outer_index in CV_outer.split(X_classification,y_classification):\n",
    "    print('CV-fold {0} of {1}'.format(k+1,K_outer))\n",
    "    k += 1\n",
    "    \n",
    "    # extract training and test set for current CV fold\n",
    "    X_train_outer = X_classification[train_outer_index,:]\n",
    "    y_train_outer = y_classification[train_outer_index]\n",
    "    X_test_outer = X_classification[test_outer_index,:]\n",
    "    y_test_outer = y_classification[test_outer_index]\n",
    "    \n",
    "    CV_inner = model_selection.KFold(n_splits=K_inner,shuffle=True)\n",
    "    \n",
    "    kk=0\n",
    "    for train_inner_index, test_inner_index in CV_inner.split(X_train_outer,y_train_outer):\n",
    "        print('Inner CV-fold {0} of {1}'.format(kk+1,K_inner))\n",
    "\n",
    "        # extract training and test set for current CV fold\n",
    "        X_train_inner = X_train_outer[train_inner_index,:]\n",
    "        y_train_inner = y_train_outer[train_inner_index]\n",
    "        X_test_inner = X_train_outer[test_inner_index,:]\n",
    "        y_test_inner = y_train_outer[test_inner_index]\n",
    "        \n",
    "#         print(len(X_train_inner))\n",
    "#         print(len(y_train_inner))\n",
    "        print(X_train_inner) \n",
    "        for count, value in enumerate(alpha):\n",
    "            est_prior = True  # uniform prior (change to True to estimate prior from data)    \n",
    "                       \n",
    "            nbclassifier = MultinomialNB(alpha=alpha, fit_prior=est_prior);\n",
    "            nbclassifier.fit(X_train_inner, y_train_inner);\n",
    "            classifier_lst.append(nbclassifier)\n",
    " \n",
    "            y_NB = nbclassifier.predict(X_test_inner);\n",
    "            errorNB_inner = 100*(y_NB!=y_test_inner).sum().astype(float)/len(y_test_inner)  \n",
    "            #index_min_lst.append(errorKNN_inner) #Append the error values to a list\n",
    "            error_inner['alpha_of_{0}'.format(value)].append(errorNB_inner) # add errors for each fold to each model\n",
    "            \n",
    "        kk += 1\n",
    "        \n",
    "    # Find the alpha value with minimum average error value\n",
    "    for key in error_inner.keys():\n",
    "        index_min_lst.append(mean(error_inner[key]))\n",
    "        \n",
    "    print('Inner_error_values are:' + str(index_min_lst))\n",
    "    index_min = np.argmin(index_min_lst) #Find the index of the minimum error value\n",
    "    top_count = index_min\n",
    "    min_indices.append(index_min) \n",
    "        \n",
    "    index_min_lst = [] # Clear for next CV fold\n",
    "    \n",
    "    for key in error_inner.keys():\n",
    "        error_inner[key] = [] # Clear for next CV fold\n",
    "        \n",
    "      \n",
    "    print('The index of optimal KNN value is: ' + str(top_count))\n",
    "    \n",
    "    optimal_alpha = alpha[top_count]\n",
    "    \n",
    "    print('The optimal KNN value across inner CV folds is: ' + str(optimal_K))\n",
    "    \n",
    "    nbclassifierOuter = MultinomialNB(alpha=optimal_alpha, fit_prior=est_prior); #Uses optimal_alpha, which was found in the inner CV loop\n",
    "    nbclassifierOuter.fit(X_train_outer, y_train_outer);\n",
    "            \n",
    "    y_NB_outer = nbclassifier.predict(X_test_outer);\n",
    "    errorNB_outer = 100*(y_NB_outer!=y_test_outer).sum().astype(float)/len(y_test_outer)\n",
    "    error_outer.append(errorNB_outer)\n",
    "    print('Errors for each outer CV fold: ' + str(error_outer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Cross-validation error [%]')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure()\n",
    "boxplot([error_KNN, error_dct])\n",
    "xlabel('K-Nearest Neighbors   vs.   Decision Tree')\n",
    "ylabel('Cross-validation error [%]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method selected: Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Re-estimate of the model on all data for a maximum depth of 12 \n",
    "\n",
    "# Fit decision tree classifier, Gini split criterion, maximum depth of 12 on all data\n",
    "dtc = tree.DecisionTreeClassifier(criterion='gini', max_depth=4) \n",
    "dtc.fit(X_classification, y_classification.ravel())\n",
    "\n",
    "# New data object\n",
    "new_data = np.array([1, 2, 4, -1, -1, 5, 0, -3, 2, 4]).reshape(1,-1) # Gives 0 - No SVI\n",
    "#new_data = np.array([1, 2, 4, -1, 2, 5, 0, -3]).reshape(1,-1) # Gives 1 - Yes SVI\n",
    "\n",
    "# Evalulate the decision tree for a new data object\n",
    "new_data_class = dtc.predict(new_data)[0]\n",
    "print(new_data_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lCavol' 'lWeight' 'Age' 'lBPH' 'lCP' 'lPSA' 'Gleason 1' 'Gleason 2'\n",
      " 'Gleason 3' 'Gleason 4']\n"
     ]
    }
   ],
   "source": [
    "print(attributeNames_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: Tree Pages: 1 -->\r\n",
       "<svg width=\"509pt\" height=\"477pt\"\r\n",
       " viewBox=\"0.00 0.00 508.50 477.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 473)\">\r\n",
       "<title>Tree</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-473 504.5,-473 504.5,4 -4,4\"/>\r\n",
       "<!-- 0 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"325.5,-469 221.5,-469 221.5,-401 325.5,-401 325.5,-469\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"273.5\" y=\"-453.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">lCP &lt;= 1.416</text>\r\n",
       "<text text-anchor=\"middle\" x=\"273.5\" y=\"-438.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.339</text>\r\n",
       "<text text-anchor=\"middle\" x=\"273.5\" y=\"-423.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 97</text>\r\n",
       "<text text-anchor=\"middle\" x=\"273.5\" y=\"-408.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [76, 21]</text>\r\n",
       "</g>\r\n",
       "<!-- 1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"265,-365 164,-365 164,-297 265,-297 265,-365\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"214.5\" y=\"-349.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">lPSA &lt;= 0.448</text>\r\n",
       "<text text-anchor=\"middle\" x=\"214.5\" y=\"-334.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.191</text>\r\n",
       "<text text-anchor=\"middle\" x=\"214.5\" y=\"-319.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 84</text>\r\n",
       "<text text-anchor=\"middle\" x=\"214.5\" y=\"-304.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [75, 9]</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M254.344,-400.884C249.398,-392.332 244.008,-383.013 238.836,-374.072\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"241.798,-372.203 233.761,-365.299 235.739,-375.708 241.798,-372.203\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"227.31\" y=\"-385.753\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 8 -->\r\n",
       "<g id=\"node9\" class=\"node\"><title>8</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"384,-365 283,-365 283,-297 384,-297 384,-365\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"333.5\" y=\"-349.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">lCavol &lt;= 1.05</text>\r\n",
       "<text text-anchor=\"middle\" x=\"333.5\" y=\"-334.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.142</text>\r\n",
       "<text text-anchor=\"middle\" x=\"333.5\" y=\"-319.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 13</text>\r\n",
       "<text text-anchor=\"middle\" x=\"333.5\" y=\"-304.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [1, 12]</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;8 -->\r\n",
       "<g id=\"edge8\" class=\"edge\"><title>0&#45;&gt;8</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M292.98,-400.884C298.01,-392.332 303.492,-383.013 308.752,-374.072\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"311.859,-375.693 313.912,-365.299 305.825,-372.144 311.859,-375.693\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"320.186\" y=\"-385.799\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 2 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"145.5,-253.5 47.5,-253.5 47.5,-200.5 145.5,-200.5 145.5,-253.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"96.5\" y=\"-238.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"96.5\" y=\"-223.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 66</text>\r\n",
       "<text text-anchor=\"middle\" x=\"96.5\" y=\"-208.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [66, 0]</text>\r\n",
       "</g>\r\n",
       "<!-- 1&#45;&gt;2 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>1&#45;&gt;2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M176.189,-296.884C162.575,-285.116 147.279,-271.894 133.755,-260.203\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"135.877,-257.412 126.023,-253.52 131.3,-262.707 135.877,-257.412\"/>\r\n",
       "</g>\r\n",
       "<!-- 3 -->\r\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"265,-261 164,-261 164,-193 265,-193 265,-261\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"214.5\" y=\"-245.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">lBPH &lt;= 1.038</text>\r\n",
       "<text text-anchor=\"middle\" x=\"214.5\" y=\"-230.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"214.5\" y=\"-215.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 18</text>\r\n",
       "<text text-anchor=\"middle\" x=\"214.5\" y=\"-200.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [9, 9]</text>\r\n",
       "</g>\r\n",
       "<!-- 1&#45;&gt;3 -->\r\n",
       "<g id=\"edge3\" class=\"edge\"><title>1&#45;&gt;3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M214.5,-296.884C214.5,-288.778 214.5,-279.982 214.5,-271.472\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"218,-271.299 214.5,-261.299 211,-271.299 218,-271.299\"/>\r\n",
       "</g>\r\n",
       "<!-- 4 -->\r\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"153,-157 46,-157 46,-89 153,-89 153,-157\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"99.5\" y=\"-141.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">lCavol &lt;= 1.105</text>\r\n",
       "<text text-anchor=\"middle\" x=\"99.5\" y=\"-126.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.426</text>\r\n",
       "<text text-anchor=\"middle\" x=\"99.5\" y=\"-111.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 13</text>\r\n",
       "<text text-anchor=\"middle\" x=\"99.5\" y=\"-96.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [4, 9]</text>\r\n",
       "</g>\r\n",
       "<!-- 3&#45;&gt;4 -->\r\n",
       "<g id=\"edge4\" class=\"edge\"><title>3&#45;&gt;4</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M177.163,-192.884C166.819,-183.709 155.478,-173.65 144.736,-164.123\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"146.847,-161.317 137.043,-157.299 142.202,-166.553 146.847,-161.317\"/>\r\n",
       "</g>\r\n",
       "<!-- 7 -->\r\n",
       "<g id=\"node8\" class=\"node\"><title>7</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"262,-149.5 171,-149.5 171,-96.5 262,-96.5 262,-149.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"216.5\" y=\"-134.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"216.5\" y=\"-119.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"216.5\" y=\"-104.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [5, 0]</text>\r\n",
       "</g>\r\n",
       "<!-- 3&#45;&gt;7 -->\r\n",
       "<g id=\"edge7\" class=\"edge\"><title>3&#45;&gt;7</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M215.149,-192.884C215.359,-182.216 215.591,-170.352 215.804,-159.519\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"219.303,-159.587 216,-149.52 212.304,-159.449 219.303,-159.587\"/>\r\n",
       "</g>\r\n",
       "<!-- 5 -->\r\n",
       "<g id=\"node6\" class=\"node\"><title>5</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"91,-53 0,-53 0,-0 91,-0 91,-53\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"45.5\" y=\"-37.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.219</text>\r\n",
       "<text text-anchor=\"middle\" x=\"45.5\" y=\"-22.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 8</text>\r\n",
       "<text text-anchor=\"middle\" x=\"45.5\" y=\"-7.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [1, 7]</text>\r\n",
       "</g>\r\n",
       "<!-- 4&#45;&gt;5 -->\r\n",
       "<g id=\"edge5\" class=\"edge\"><title>4&#45;&gt;5</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M80.6134,-88.9485C75.6474,-80.2579 70.2776,-70.8608 65.2667,-62.0917\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"68.207,-60.1826 60.2067,-53.2367 62.1293,-63.6557 68.207,-60.1826\"/>\r\n",
       "</g>\r\n",
       "<!-- 6 -->\r\n",
       "<g id=\"node7\" class=\"node\"><title>6</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"200,-53 109,-53 109,-0 200,-0 200,-53\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"154.5\" y=\"-37.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.48</text>\r\n",
       "<text text-anchor=\"middle\" x=\"154.5\" y=\"-22.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"154.5\" y=\"-7.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [3, 2]</text>\r\n",
       "</g>\r\n",
       "<!-- 4&#45;&gt;6 -->\r\n",
       "<g id=\"edge6\" class=\"edge\"><title>4&#45;&gt;6</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M118.736,-88.9485C123.794,-80.2579 129.264,-70.8608 134.367,-62.0917\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"137.516,-63.6401 139.521,-53.2367 131.466,-60.1189 137.516,-63.6401\"/>\r\n",
       "</g>\r\n",
       "<!-- 9 -->\r\n",
       "<g id=\"node10\" class=\"node\"><title>9</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"384,-261 283,-261 283,-193 384,-193 384,-261\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"333.5\" y=\"-245.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">lPSA &lt;= 0.634</text>\r\n",
       "<text text-anchor=\"middle\" x=\"333.5\" y=\"-230.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.444</text>\r\n",
       "<text text-anchor=\"middle\" x=\"333.5\" y=\"-215.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 3</text>\r\n",
       "<text text-anchor=\"middle\" x=\"333.5\" y=\"-200.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [1, 2]</text>\r\n",
       "</g>\r\n",
       "<!-- 8&#45;&gt;9 -->\r\n",
       "<g id=\"edge9\" class=\"edge\"><title>8&#45;&gt;9</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M333.5,-296.884C333.5,-288.778 333.5,-279.982 333.5,-271.472\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"337,-271.299 333.5,-261.299 330,-271.299 337,-271.299\"/>\r\n",
       "</g>\r\n",
       "<!-- 12 -->\r\n",
       "<g id=\"node13\" class=\"node\"><title>12</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"500.5,-253.5 402.5,-253.5 402.5,-200.5 500.5,-200.5 500.5,-253.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"451.5\" y=\"-238.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"451.5\" y=\"-223.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 10</text>\r\n",
       "<text text-anchor=\"middle\" x=\"451.5\" y=\"-208.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [0, 10]</text>\r\n",
       "</g>\r\n",
       "<!-- 8&#45;&gt;12 -->\r\n",
       "<g id=\"edge12\" class=\"edge\"><title>8&#45;&gt;12</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M371.811,-296.884C385.425,-285.116 400.721,-271.894 414.245,-260.203\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"416.7,-262.707 421.977,-253.52 412.123,-257.412 416.7,-262.707\"/>\r\n",
       "</g>\r\n",
       "<!-- 10 -->\r\n",
       "<g id=\"node11\" class=\"node\"><title>10</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"376,-149.5 285,-149.5 285,-96.5 376,-96.5 376,-149.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"330.5\" y=\"-134.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"330.5\" y=\"-119.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 2</text>\r\n",
       "<text text-anchor=\"middle\" x=\"330.5\" y=\"-104.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [0, 2]</text>\r\n",
       "</g>\r\n",
       "<!-- 9&#45;&gt;10 -->\r\n",
       "<g id=\"edge10\" class=\"edge\"><title>9&#45;&gt;10</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M332.526,-192.884C332.212,-182.216 331.863,-170.352 331.545,-159.519\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"335.043,-159.413 331.251,-149.52 328.046,-159.619 335.043,-159.413\"/>\r\n",
       "</g>\r\n",
       "<!-- 11 -->\r\n",
       "<g id=\"node12\" class=\"node\"><title>11</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"485,-149.5 394,-149.5 394,-96.5 485,-96.5 485,-149.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"439.5\" y=\"-134.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"439.5\" y=\"-119.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 1</text>\r\n",
       "<text text-anchor=\"middle\" x=\"439.5\" y=\"-104.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [1, 0]</text>\r\n",
       "</g>\r\n",
       "<!-- 9&#45;&gt;11 -->\r\n",
       "<g id=\"edge11\" class=\"edge\"><title>9&#45;&gt;11</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M367.915,-192.884C380.03,-181.226 393.628,-168.141 405.693,-156.532\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"408.2,-158.976 412.979,-149.52 403.347,-153.932 408.2,-158.976\"/>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x1a10b0cef98>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export tree graph for visualization purposes:\n",
    "# (note: you can use i.e. Graphviz application to visualize the file)\n",
    "out = tree.export_graphviz(dtc, out_file='tree_gini.gvz', feature_names=attributeNames_classification)\n",
    "#graphviz.render('dot','png','tree_gini',quiet=False)\n",
    "src=graphviz.Source.from_file('tree_gini.gvz')\n",
    "## Comment in to automatically open pdf\n",
    "## Note. If you get an error (e.g. exit status 1), try closing the pdf file/viewer\n",
    "#src.render('../tree_gini', view=True)\n",
    "src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model\n",
    "# zero rule algorithm for classification\n",
    "def Baseline_model(y_train, y_test):\n",
    "    prediction = stats.mode(y_train)[0]\n",
    "    predicted = [int(prediction) for i in range(len(y_test))]\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-level-cross validation for dtc, KNN and baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV-fold 1 of 5\n",
      "Inner CV-fold 1 of 5\n",
      "Inner CV-fold 2 of 5\n",
      "Inner CV-fold 3 of 5\n",
      "Inner CV-fold 4 of 5\n",
      "Inner CV-fold 5 of 5\n",
      "Inner_error_values_ for dtc are:[17.083333333333332, 15.75, 13.083333333333334, 11.916666666666666, 15.666666666666668, 15.666666666666668, 14.5, 14.5, 17.0, 11.916666666666668, 17.0, 13.166666666666668, 11.833333333333334, 15.666666666666666, 14.416666666666666, 13.083333333333334, 13.166666666666666, 15.75, 14.416666666666666]\n",
      "The index of optimal tc value is: 1\n",
      "The optimal tc value across inner CV folds is: 14\n",
      "Errors for each outer tc fold: [25.0]\n",
      "Inner_error_values are:[15.75, 14.416666666666666, 13.166666666666666, 11.833333333333334, 11.833333333333334, 13.083333333333334, 10.583333333333334, 15.666666666666668, 17.083333333333332, 16.916666666666668, 19.666666666666668, 14.333333333333334, 14.416666666666668, 14.416666666666666, 14.416666666666666, 14.416666666666666, 13.083333333333334, 15.666666666666666, 15.666666666666666, 17.0, 17.0, 18.25, 17.0, 19.5, 19.5, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75]\n",
      "The index of optimal KNN value is: 1\n",
      "The optimal KNN value across inner CV folds is: 7\n",
      "Errors for each outer CV fold: [10.0]\n",
      "Errors for baseline outer fold[25.0]\n",
      "CV-fold 2 of 5\n",
      "Inner CV-fold 1 of 5\n",
      "Inner CV-fold 2 of 5\n",
      "Inner CV-fold 3 of 5\n",
      "Inner CV-fold 4 of 5\n",
      "Inner CV-fold 5 of 5\n",
      "Inner_error_values_ for dtc are:[18.25, 16.916666666666668, 14.25, 16.916666666666668, 15.583333333333334, 15.583333333333334, 15.583333333333334, 15.583333333333334, 15.583333333333334, 16.916666666666668, 15.583333333333334, 14.25, 15.583333333333334, 14.25, 14.25, 15.583333333333334, 14.25, 16.916666666666668, 16.916666666666668]\n",
      "The index of optimal tc value is: 1\n",
      "The optimal tc value across inner CV folds is: 4\n",
      "Errors for each outer tc fold: [25.0, 15.0]\n",
      "Inner_error_values are:[15.583333333333334, 19.5, 19.5, 20.75, 18.083333333333332, 13.0, 10.333333333333334, 13.0, 11.75, 11.666666666666668, 11.666666666666668, 11.666666666666666, 13.0, 13.0, 13.0, 15.666666666666666, 10.333333333333334, 15.666666666666666, 11.666666666666668, 14.333333333333334, 13.0, 16.833333333333332, 16.833333333333332, 19.5, 19.5, 19.5, 19.5, 20.833333333333336, 20.833333333333336, 20.833333333333336, 20.833333333333336, 20.833333333333336, 20.833333333333336, 20.833333333333336, 20.833333333333336, 20.833333333333336, 20.833333333333336, 20.833333333333336, 20.833333333333336, 20.833333333333336]\n",
      "The index of optimal KNN value is: 1\n",
      "The optimal KNN value across inner CV folds is: 7\n",
      "Errors for each outer CV fold: [10.0, 20.0]\n",
      "Errors for baseline outer fold[25.0, 25.0]\n",
      "CV-fold 3 of 5\n",
      "Inner CV-fold 1 of 5\n",
      "Inner CV-fold 2 of 5\n",
      "Inner CV-fold 3 of 5\n",
      "Inner CV-fold 4 of 5\n",
      "Inner CV-fold 5 of 5\n",
      "Inner_error_values_ for dtc are:[14.083333333333334, 11.583333333333334, 11.583333333333334, 11.583333333333334, 15.416666666666666, 12.916666666666666, 12.833333333333334, 12.916666666666666, 14.166666666666666, 15.416666666666666, 12.916666666666666, 12.916666666666666, 10.333333333333334, 12.916666666666666, 12.916666666666666, 12.916666666666666, 14.166666666666666, 12.916666666666666, 14.166666666666666]\n",
      "The index of optimal tc value is: 1\n",
      "The optimal tc value across inner CV folds is: 14\n",
      "Errors for each outer tc fold: [25.0, 15.0, 15.789473684210526]\n",
      "Inner_error_values are:[12.916666666666668, 15.416666666666666, 12.75, 12.75, 12.75, 14.083333333333334, 15.333333333333334, 14.083333333333334, 15.333333333333334, 14.083333333333334, 12.75, 15.416666666666666, 14.083333333333334, 14.083333333333334, 12.75, 14.083333333333334, 14.083333333333334, 14.083333333333334, 14.083333333333334, 14.083333333333334, 15.416666666666666, 15.333333333333334, 16.666666666666668, 19.25, 19.25, 18.0, 16.666666666666668, 19.333333333333332, 19.333333333333332, 21.833333333333332, 21.833333333333332, 23.083333333333332, 23.083333333333332, 24.333333333333332, 24.333333333333332, 24.333333333333332, 24.333333333333332, 24.333333333333332, 24.333333333333332, 24.333333333333332]\n",
      "The index of optimal KNN value is: 1\n",
      "The optimal KNN value across inner CV folds is: 3\n",
      "Errors for each outer CV fold: [10.0, 20.0, 21.05263157894737]\n",
      "Errors for baseline outer fold[25.0, 25.0, 10.526315789473685]\n",
      "CV-fold 4 of 5\n",
      "Inner CV-fold 1 of 5\n",
      "Inner CV-fold 2 of 5\n",
      "Inner CV-fold 3 of 5\n",
      "Inner CV-fold 4 of 5\n",
      "Inner CV-fold 5 of 5\n",
      "Inner_error_values_ for dtc are:[22.0, 18.083333333333336, 19.416666666666668, 18.083333333333336, 16.75, 18.0, 18.083333333333332, 19.416666666666668, 16.833333333333332, 18.083333333333336, 18.083333333333336, 18.083333333333336, 20.666666666666668, 15.5, 16.75, 18.083333333333336, 18.0, 16.833333333333336, 19.416666666666668]\n",
      "The index of optimal tc value is: 1\n",
      "The optimal tc value across inner CV folds is: 15\n",
      "Errors for each outer tc fold: [25.0, 15.0, 15.789473684210526, 0.0]\n",
      "Inner_error_values are:[15.166666666666666, 16.75, 14.0, 15.416666666666668, 14.083333333333334, 16.75, 16.666666666666668, 18.083333333333332, 15.333333333333334, 18.0, 16.666666666666668, 20.583333333333332, 19.333333333333332, 16.833333333333332, 18.083333333333332, 16.833333333333332, 18.083333333333332, 16.916666666666668, 16.833333333333332, 16.916666666666668, 18.166666666666668, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5]\n",
      "The index of optimal KNN value is: 1\n",
      "The optimal KNN value across inner CV folds is: 3\n",
      "Errors for each outer CV fold: [10.0, 20.0, 21.05263157894737, 21.05263157894737]\n",
      "Errors for baseline outer fold[25.0, 25.0, 10.526315789473685, 31.57894736842105]\n",
      "CV-fold 5 of 5\n",
      "Inner CV-fold 1 of 5\n",
      "Inner CV-fold 2 of 5\n",
      "Inner CV-fold 3 of 5\n",
      "Inner CV-fold 4 of 5\n",
      "Inner CV-fold 5 of 5\n",
      "Inner_error_values_ for dtc are:[13.0, 11.75, 15.583333333333334, 16.916666666666668, 16.916666666666668, 15.666666666666668, 16.916666666666668, 15.583333333333334, 16.916666666666668, 15.666666666666668, 16.916666666666668, 15.583333333333334, 15.583333333333334, 17.0, 15.583333333333334, 16.916666666666668, 16.916666666666668, 15.583333333333334, 14.333333333333334]\n",
      "The index of optimal tc value is: 1\n",
      "The optimal tc value across inner CV folds is: 3\n",
      "Errors for each outer tc fold: [25.0, 15.0, 15.789473684210526, 0.0, 15.789473684210526]\n",
      "Inner_error_values are:[15.333333333333334, 18.083333333333336, 14.083333333333334, 17.916666666666668, 16.666666666666668, 15.416666666666666, 16.666666666666668, 16.666666666666668, 15.416666666666666, 15.416666666666666, 15.416666666666666, 15.416666666666666, 15.333333333333334, 14.083333333333334, 9.0, 15.5, 15.416666666666666, 15.5, 14.166666666666666, 15.5, 14.166666666666666, 20.666666666666668, 18.166666666666668, 23.166666666666668, 21.916666666666668, 21.916666666666668, 21.916666666666668, 21.916666666666668, 21.916666666666668, 23.166666666666668, 23.166666666666668, 23.166666666666668, 23.166666666666668, 23.166666666666668, 23.166666666666668, 23.166666666666668, 23.166666666666668, 23.166666666666668, 23.166666666666668, 23.166666666666668]\n",
      "The index of optimal KNN value is: 1\n",
      "The optimal KNN value across inner CV folds is: 15\n",
      "Errors for each outer CV fold: [10.0, 20.0, 21.05263157894737, 21.05263157894737, 0.0]\n",
      "Errors for baseline outer fold[25.0, 25.0, 10.526315789473685, 31.57894736842105, 15.789473684210526]\n"
     ]
    }
   ],
   "source": [
    "# Results from the 2-level cross-validation for dtc \n",
    "# Errors for each outer tc fold: [15.0, 30.0, 10.526315789473685, 10.526315789473685, 15.789473684210526]\n",
    "# The errors for the best performing models are: 10.526315789473685, 15.0\n",
    "## Crossvalidation for decision trees\n",
    "## The selection of optimal model is based on an average of the inner errors for each model\n",
    "\n",
    "# Create crossvalidation partition for evaluation\n",
    "K_outer = 5\n",
    "K_inner = 5\n",
    "CV_outer = model_selection.KFold(n_splits=K_outer,shuffle=True)\n",
    "#CV = model_selection.StratifiedKFold(n_splits=K)\n",
    "\n",
    "# dtc\n",
    "index_min_lst_dtc = []\n",
    "min_indices_dtc = []\n",
    "error_outer_dtc = [] # List for the errors in outer CV fold\n",
    "dict_inner_dtc = {}\n",
    "error_inner_dtc = {} # Dict with the errors in the innter CV fold for each tested model\n",
    "# Tree complexity parameter - constraint on maximum depth\n",
    "tc = np.arange(2, 21, 1)\n",
    "classifier_lst_dtc = []\n",
    "\n",
    "for count, value in enumerate(tc):\n",
    "    error_inner_dtc['tc_of_{0}'.format(value)] = []\n",
    "\n",
    "# KNN\n",
    "index_min_lst_KNN = []\n",
    "min_indices_KNN = []\n",
    "error_outer_KNN = [] # List for the errors in outer CV fold\n",
    "dict_inner_KNN = {}\n",
    "error_inner_KNN = {} # Dict with the errors in the innter CV fold for each tested model\n",
    "K_KNN = range(1,41) # Change here for different nearest neighbour crossvalidation - test of K=1-40\n",
    "classifier_lst_KNN = []\n",
    "\n",
    "for count, value in enumerate(K_KNN):\n",
    "    error_inner_KNN['K_KNN_of_{0}'.format(value)] = []\n",
    "    \n",
    "# Baseline model\n",
    "error_baseline = []\n",
    "\n",
    "\n",
    "\n",
    "k=0\n",
    "\n",
    "for train_outer_index, test_outer_index in CV_outer.split(X_classification,y_classification):\n",
    "    print('CV-fold {0} of {1}'.format(k+1,K_outer))\n",
    "    k += 1\n",
    "    \n",
    "    # extract training and test set for current CV fold\n",
    "    X_train_outer = X_classification[train_outer_index,:]\n",
    "    y_train_outer = y_classification[train_outer_index]\n",
    "    X_test_outer = X_classification[test_outer_index,:]\n",
    "    y_test_outer = y_classification[test_outer_index]\n",
    "    \n",
    "    CV_inner = model_selection.KFold(n_splits=K_inner,shuffle=True)\n",
    "    \n",
    "    kk=0\n",
    "    for train_inner_index, test_inner_index in CV_inner.split(X_train_outer,y_train_outer):\n",
    "        print('Inner CV-fold {0} of {1}'.format(kk+1,K_inner))\n",
    "\n",
    "        # extract training and test set for current CV fold\n",
    "        X_train_inner = X_train_outer[train_inner_index,:]\n",
    "        y_train_inner = y_train_outer[train_inner_index]\n",
    "        X_test_inner = X_train_outer[test_inner_index,:]\n",
    "        y_test_inner = y_train_outer[test_inner_index]\n",
    "        \n",
    "#         print(len(X_train_inner))\n",
    "#         print(len(y_train_inner))\n",
    "        \n",
    "    # Decision tree classifier\n",
    "        for count, value in enumerate(tc):\n",
    "            dist=1\n",
    "            \n",
    "            # Fit decision tree classifier, Gini split criterion, different pruning levels\n",
    "            dtc = tree.DecisionTreeClassifier(criterion='gini', max_depth=value) \n",
    "            dtc.fit(X_train_inner, y_train_inner.ravel());\n",
    "            classifier_lst_dtc.append(dtc)\n",
    "            \n",
    "            y_dtc = dtc.predict(X_test_inner);\n",
    "            errordtc_inner = 100*(y_dtc!=y_test_inner).sum().astype(float)/len(y_test_inner)  \n",
    "            #index_min_lst.append(errorKNN_inner) #Append the error values to a list\n",
    "            error_inner_dtc['tc_of_{0}'.format(value)].append(errordtc_inner) # add errors for each fold to each model\n",
    "    \n",
    "    # KNN classifier\n",
    "            for count, value in enumerate(K_KNN):\n",
    "                dist=2 # euclidean_distance\n",
    "                       \n",
    "                knclassifier = KNeighborsClassifier(n_neighbors=value, p=dist);\n",
    "                knclassifier.fit(X_train_inner, y_train_inner);\n",
    "                classifier_lst_KNN.append(knclassifier)\n",
    "            \n",
    "                y_KNN = knclassifier.predict(X_test_inner);\n",
    "                errorKNN_inner = 100*(y_KNN!=y_test_inner).sum().astype(float)/len(y_test_inner)  \n",
    "                #index_min_lst.append(errorKNN_inner) #Append the error values to a list\n",
    "                error_inner_KNN['K_KNN_of_{0}'.format(value)].append(errorKNN_inner) # add errors for each fold to each model\n",
    "        \n",
    "        \n",
    "        kk += 1\n",
    "    #dtc\n",
    "    # Find the dtc value with minimum average error value\n",
    "    for key in error_inner_dtc.keys():\n",
    "        index_min_lst_dtc.append(mean(error_inner_dtc[key]))\n",
    "        \n",
    "    print('Inner_error_values_ for dtc are:' + str(index_min_lst_dtc))\n",
    "    index_min_dtc = np.argmin(index_min_lst_dtc) #Find the index of the minimum error value\n",
    "    top_count_dtc = index_min_dtc\n",
    "    min_indices_dtc.append(index_min_dtc) \n",
    "        \n",
    "    index_min_lst_dtc = [] # Clear for next CV fold\n",
    "    \n",
    "    for key in error_inner_dtc.keys():\n",
    "        error_inner_dtc[key] = [] # Clear for next CV fold\n",
    "        \n",
    "      \n",
    "    print('The index of optimal tc value is: ' + str(top_count))\n",
    "    \n",
    "    optimal_tc = tc[top_count_dtc]\n",
    "    \n",
    "    print('The optimal tc value across inner CV folds is: ' + str(optimal_tc))\n",
    "    \n",
    "    \n",
    "    dtcclassifierOuter = tree.DecisionTreeClassifier(criterion='gini', max_depth=optimal_tc);  #Uses optimal_tc, which was found in the inner CV loop\n",
    "    dtcclassifierOuter.fit(X_train_outer, y_train_outer.ravel());\n",
    "            \n",
    "    y_dtc_outer = dtcclassifierOuter.predict(X_test_outer);\n",
    "    errordtc_outer = 100*(y_dtc_outer!=y_test_outer).sum().astype(float)/len(y_test_outer)\n",
    "    error_outer_dtc.append(errordtc_outer)\n",
    "    print('Errors for each outer tc fold: ' + str(error_outer_dtc))\n",
    "    \n",
    "    \n",
    "    # KNN\n",
    "    # Find the KNN value with minimum average error value\n",
    "    for key in error_inner_KNN.keys():\n",
    "        index_min_lst_KNN.append(mean(error_inner_KNN[key]))\n",
    "        \n",
    "    print('Inner_error_values are:' + str(index_min_lst_KNN))\n",
    "    index_min_KNN = np.argmin(index_min_lst_KNN) #Find the index of the minimum error value\n",
    "    top_count_KNN = index_min_KNN\n",
    "    min_indices_KNN.append(index_min_KNN) \n",
    "        \n",
    "    index_min_lst_KNN = [] # Clear for next CV fold\n",
    "    \n",
    "    for key in error_inner_KNN.keys():\n",
    "        error_inner_KNN[key] = [] # Clear for next CV fold\n",
    "        \n",
    "      \n",
    "    print('The index of optimal KNN value is: ' + str(top_count))\n",
    "    \n",
    "    optimal_K = K_KNN[top_count_KNN]\n",
    "    \n",
    "    print('The optimal KNN value across inner CV folds is: ' + str(optimal_K))\n",
    "    \n",
    "    knclassifierOuter = KNeighborsClassifier(n_neighbors=optimal_K, p=dist); #Uses optimal_K, which was found in the inner CV loop\n",
    "    knclassifierOuter.fit(X_train_outer, y_train_outer);\n",
    "            \n",
    "    y_KNN_outer = knclassifierOuter.predict(X_test_outer);\n",
    "    errorKNN_outer = 100*(y_KNN_outer!=y_test_outer).sum().astype(float)/len(y_test_outer)\n",
    "    error_outer_KNN.append(errorKNN_outer)\n",
    "    print('Errors for each outer CV fold: ' + str(error_outer_KNN))\n",
    " \n",
    "    \n",
    "    \n",
    "    # Fit baseline model\n",
    "            \n",
    "    y_baselinemodel = Baseline_model(y_train_outer, y_test_outer);\n",
    "    error_baseline_outer = 100*(y_baselinemodel!=y_test_outer).sum().astype(float)/len(y_test_outer)  \n",
    "    error_baseline.append(error_baseline_outer)\n",
    "    print('Errors for baseline outer fold'+str(error_baseline))\n",
    "\n",
    "error_dct = error_outer_dtc\n",
    "error_KNN = error_outer_KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.02061855670103\n",
      "14.123711340206185\n"
     ]
    }
   ],
   "source": [
    "# Final generalization error for DCT\n",
    "DTC_error = (len(y_test_outer)/N*np.mat(error_dct)).sum()\n",
    "print(DTC_error)\n",
    "# Final generalization error for KNN\n",
    "KNN_error = (len(y_test_outer)/N*np.mat(error_KNN)).sum()\n",
    "print(KNN_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best performing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer error dct is[25.0, 15.0, 15.789473684210526, 0.0, 15.789473684210526]\n",
      "Outer error of KNN is[10.0, 20.0, 21.05263157894737, 21.05263157894737, 0.0]\n",
      "Outer error of baseline_model is[25.0, 25.0, 10.526315789473685, 31.57894736842105, 15.789473684210526]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Cross-validation error [%]')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The two best performing models are from dct and KNN\n",
    "print('Outer error dct is' + str(error_dct))\n",
    "print('Outer error of KNN is' + str(error_KNN))\n",
    "print('Outer error of baseline_model is' + str(error_baseline))\n",
    "\n",
    "figure()\n",
    "boxplot([error_KNN, error_dct])\n",
    "xlabel('K-Nearest Neighbors   vs.   Decision Tree')\n",
    "ylabel('Cross-validation error [%]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of methods (based on all the different models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 and baseline model are not significantly different\n",
      "-10.955521108607925\n",
      "25.481836898081603\n",
      "Baseline model and Model 2 are not significantly different\n",
      "-6.225849993025792\n",
      "20.541639466709995\n",
      "Model 1 and Model 2 are not significantly different\n",
      "-19.44262235602408\n",
      "19.2320960402346\n"
     ]
    }
   ],
   "source": [
    "# The best performing classifiers are KNN and dtc\n",
    "\n",
    "## Crossvalidation\n",
    "# Create crossvalidation partition for evaluation\n",
    "K = K_outer\n",
    "CV = model_selection.KFold(n_splits=K,shuffle=True)\n",
    "#CV = model_selection.StratifiedKFold(n_splits=K)\n",
    "\n",
    "# Initialize variables\n",
    "Error_base_line = np.asarray(error_baseline)\n",
    "Error_model_1 = np.asarray(error_dct)\n",
    "Error_model_2 = np.asarray(error_KNN)\n",
    "\n",
    "# Comparison of Baseline model with model 1\n",
    "\n",
    "# Test if classifiers are significantly different using methods in section 9.3.3\n",
    "# by computing credibility interval. Notice this can also be accomplished by computing the p-value using\n",
    "# [tstatistic, pvalue] = stats.ttest_ind(Error_logreg,Error_dectree)\n",
    "# and test if the p-value is less than alpha=0.05. \n",
    "z = (Error_base_line-Error_model_1)\n",
    "zb = z.mean()\n",
    "nu = K-1\n",
    "sig =  (z-zb).std()  / np.sqrt(K-1)\n",
    "alpha = 0.05\n",
    "\n",
    "zL = zb + sig * stats.t.ppf(alpha/2, nu);\n",
    "zH = zb + sig * stats.t.ppf(1-alpha/2, nu);\n",
    "\n",
    "if zL <= 0 and zH >= 0 :\n",
    "    print('Model 1 and baseline model are not significantly different')        \n",
    "else:\n",
    "    print('Model 1 and baseline model are significantly different.')\n",
    "print(zL)\n",
    "print(zH)\n",
    "\n",
    "# Comparison of Baseline model with model 2\n",
    "\n",
    "# Test if classifiers are significantly different using methods in section 9.3.3\n",
    "# by computing credibility interval. Notice this can also be accomplished by computing the p-value using\n",
    "# [tstatistic, pvalue] = stats.ttest_ind(Error_logreg,Error_dectree)\n",
    "# and test if the p-value is less than alpha=0.05. \n",
    "z = (Error_base_line-Error_model_2)\n",
    "zb = z.mean()\n",
    "nu = K-1\n",
    "sig =  (z-zb).std()  / np.sqrt(K-1)\n",
    "alpha = 0.05\n",
    "\n",
    "zL = zb + sig * stats.t.ppf(alpha/2, nu);\n",
    "zH = zb + sig * stats.t.ppf(1-alpha/2, nu);\n",
    "\n",
    "if zL <= 0 and zH >= 0 :\n",
    "    print('Baseline model and Model 2 are not significantly different')        \n",
    "else:\n",
    "    print('Baseline model and Model 2 are significantly different.')\n",
    "print(zL)\n",
    "print(zH)\n",
    "\n",
    "# Comparison of model 1 with model 2\n",
    "\n",
    "# Test if classifiers are significantly different using methods in section 9.3.3\n",
    "# by computing credibility interval. Notice this can also be accomplished by computing the p-value using\n",
    "# [tstatistic, pvalue] = stats.ttest_ind(Error_logreg,Error_dectree)\n",
    "# and test if the p-value is less than alpha=0.05. \n",
    "z = (Error_model_1-Error_model_2)\n",
    "zb = z.mean()\n",
    "nu = K-1\n",
    "sig =  (z-zb).std()  / np.sqrt(K-1)\n",
    "alpha = 0.05\n",
    "\n",
    "zL = zb + sig * stats.t.ppf(alpha/2, nu);\n",
    "zH = zb + sig * stats.t.ppf(1-alpha/2, nu);\n",
    "\n",
    "if zL <= 0 and zH >= 0 :\n",
    "    print('Model 1 and Model 2 are not significantly different')        \n",
    "else:\n",
    "    print('Model 1 and Model 2 are significantly different.')\n",
    "print(zL)\n",
    "print(zH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of two single models (selected based on error) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV-fold 1 of 10\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-7efc38443c84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# extract training and test set for current CV fold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "# The best performing classifiers are KNN with K = 2 and a dtc with tc=10\n",
    "\n",
    "## Crossvalidation\n",
    "# Create crossvalidation partition for evaluation\n",
    "K = 10\n",
    "CV = model_selection.KFold(n_splits=K,shuffle=True)\n",
    "#CV = model_selection.StratifiedKFold(n_splits=K)\n",
    "\n",
    "# Initialize variables\n",
    "Error_base_line = np.empty((K,1))\n",
    "Error_model_1 = np.empty((K,1))\n",
    "Error_model_2 = np.empty((K,1))\n",
    "\n",
    "n_tested=0\n",
    "\n",
    "k=0\n",
    "for train_index, test_index in CV.split(X_classification,y_classification):\n",
    "    print('CV-fold {0} of {1}'.format(k+1,K))\n",
    "    \n",
    "    # extract training and test set for current CV fold\n",
    "    X_train = X[train_index,:]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index,:]\n",
    "    y_test = y[test_index]\n",
    "\n",
    "    # Fit and evaluate KNN\n",
    "    model1 = KNeighborsClassifier(n_neighbors=2, p=dist);\n",
    "    model1 = model1.fit(X_train, y_train)\n",
    "    y_model1 = model1.predict(X_test)\n",
    "    Error_model_1[k] = 100*(y_model1!=y_test).sum().astype(float)/len(y_test)\n",
    "    \n",
    "    # Fit and evaluate Decision Tree classifier\n",
    "    model2 = tree.DecisionTreeClassifier(criterion='gini', max_depth=10);\n",
    "    model2 = model2.fit(X_train, y_train.ravel())\n",
    "    y_model2 = model2.predict(X_test)\n",
    "    Error_model_2[k] = 100*(y_model2!=y_test).sum().astype(float)/len(y_test)  \n",
    "    \n",
    "    # Fit and evaluate baseline model classifier\n",
    "    y_baseline = Baseline_model(y_train, y_test);\n",
    "    Error_base_line[k] = 100*(y_baseline!=y_test).sum().astype(float)/len(y_test)\n",
    "  \n",
    "    k+=1\n",
    "\n",
    "# Comparison of Baseline model with model 1\n",
    "\n",
    "# Test if classifiers are significantly different using methods in section 9.3.3\n",
    "# by computing credibility interval. Notice this can also be accomplished by computing the p-value using\n",
    "# [tstatistic, pvalue] = stats.ttest_ind(Error_logreg,Error_dectree)\n",
    "# and test if the p-value is less than alpha=0.05. \n",
    "z = (Error_base_line-Error_model_1)\n",
    "zb = z.mean()\n",
    "nu = K-1\n",
    "sig =  (z-zb).std()  / np.sqrt(K-1)\n",
    "alpha = 0.05\n",
    "\n",
    "zL = zb + sig * stats.t.ppf(alpha/2, nu);\n",
    "zH = zb + sig * stats.t.ppf(1-alpha/2, nu);\n",
    "\n",
    "if zL <= 0 and zH >= 0 :\n",
    "    print('Model 1 and baseline model are not significantly different')        \n",
    "else:\n",
    "    print('Model 1 and baseline model are significantly different.')\n",
    "print(zL)\n",
    "print(zH)\n",
    "\n",
    "# Comparison of Baseline model with model 2\n",
    "\n",
    "# Test if classifiers are significantly different using methods in section 9.3.3\n",
    "# by computing credibility interval. Notice this can also be accomplished by computing the p-value using\n",
    "# [tstatistic, pvalue] = stats.ttest_ind(Error_logreg,Error_dectree)\n",
    "# and test if the p-value is less than alpha=0.05. \n",
    "z = (Error_base_line-Error_model_2)\n",
    "zb = z.mean()\n",
    "nu = K-1\n",
    "sig =  (z-zb).std()  / np.sqrt(K-1)\n",
    "alpha = 0.05\n",
    "\n",
    "zL = zb + sig * stats.t.ppf(alpha/2, nu);\n",
    "zH = zb + sig * stats.t.ppf(1-alpha/2, nu);\n",
    "\n",
    "if zL <= 0 and zH >= 0 :\n",
    "    print('Baseline model and Model 2 are not significantly different')        \n",
    "else:\n",
    "    print('Baseline model and Model 2 are significantly different.')\n",
    "print(zL)\n",
    "print(zH)\n",
    "\n",
    "# Comparison of model 1 with model 2\n",
    "\n",
    "# Test if classifiers are significantly different using methods in section 9.3.3\n",
    "# by computing credibility interval. Notice this can also be accomplished by computing the p-value using\n",
    "# [tstatistic, pvalue] = stats.ttest_ind(Error_logreg,Error_dectree)\n",
    "# and test if the p-value is less than alpha=0.05. \n",
    "z = (Error_model_1-Error_model_2)\n",
    "zb = z.mean()\n",
    "nu = K-1\n",
    "sig =  (z-zb).std()  / np.sqrt(K-1)\n",
    "alpha = 0.05\n",
    "\n",
    "zL = zb + sig * stats.t.ppf(alpha/2, nu);\n",
    "zH = zb + sig * stats.t.ppf(1-alpha/2, nu);\n",
    "\n",
    "if zL <= 0 and zH >= 0 :\n",
    "    print('Model 1 and Model 2 are not significantly different')        \n",
    "else:\n",
    "    print('Model 1 and Model 2 are significantly different.')\n",
    "print(zL)\n",
    "print(zH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gammel kode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 and baseline model are significantly different.\n",
      "5.0\n",
      "5.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_parse_args() missing 1 required positional argument: 'df'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-3aadf24c2f4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0mzL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzb\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msig\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mppf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[0mzH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzb\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msig\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mppf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py\u001b[0m in \u001b[0;36mppf\u001b[1;34m(self, q, *args, **kwds)\u001b[0m\n\u001b[0;32m   1898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1899\u001b[0m         \"\"\"\n\u001b[1;32m-> 1900\u001b[1;33m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1901\u001b[0m         \u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1902\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: _parse_args() missing 1 required positional argument: 'df'"
     ]
    }
   ],
   "source": [
    "# Initialize variables\n",
    "Error_base_line = min(error_baseline)\n",
    "Error_model_1 = min(error_dct) # Gives the best performing model\n",
    "Error_model_2 = sorted(error_dct)[1] # Gives the second best performing model\n",
    "\n",
    "# Comparison of Baseline model with model 1\n",
    "\n",
    "# Test if classifiers are significantly different using methods in section 9.3.3\n",
    "# by computing credibility interval. Notice this can also be accomplished by computing the p-value using\n",
    "# [tstatistic, pvalue] = stats.ttest_ind(Error_logreg,Error_dectree)\n",
    "# and test if the p-value is less than alpha=0.05. \n",
    "z = (Error_base_line-Error_model_1)\n",
    "zb = z.mean()\n",
    "nu = K-1\n",
    "sig =  (z-zb).std()  / np.sqrt(K-1)\n",
    "alpha = 0.05\n",
    "\n",
    "zL = zb + sig * stats.t.ppf(alpha/2, nu);\n",
    "zH = zb + sig * stats.t.ppf(1-alpha/2, nu);\n",
    "\n",
    "if zL <= 0 and zH >= 0 :\n",
    "    print('Model 1 and baseline model are not significantly different')        \n",
    "else:\n",
    "    print('Model 1 and baseline model are significantly different.')\n",
    "print(zL)\n",
    "print(zH)\n",
    "\n",
    "# Comparison of Baseline model with model 2\n",
    "\n",
    "# Test if classifiers are significantly different using methods in section 9.3.3\n",
    "# by computing credibility interval. Notice this can also be accomplished by computing the p-value using\n",
    "# [tstatistic, pvalue] = stats.ttest_ind(Error_logreg,Error_dectree)\n",
    "# and test if the p-value is less than alpha=0.05. \n",
    "z = (Error_base_line-Error_model_2)\n",
    "zb = z.mean()\n",
    "nu = K-1\n",
    "sig =  (z-zb).std()  / np.sqrt(K-1)\n",
    "alpha = 0.05\n",
    "\n",
    "zL = zb + sig * stats.t.ppf(alpha/2);\n",
    "zH = zb + sig * stats.t.ppf(1-alpha/2);\n",
    "\n",
    "if zL <= 0 and zH >= 0 :\n",
    "    print('Model 2 and baseline model are not significantly different')        \n",
    "else:\n",
    "    print('Model 2 and baseline model are significantly different.')\n",
    "print(zL)\n",
    "print(zH)\n",
    "\n",
    "# Comparison of model 1 with model 2\n",
    "\n",
    "# Test if classifiers are significantly different using methods in section 9.3.3\n",
    "# by computing credibility interval. Notice this can also be accomplished by computing the p-value using\n",
    "# [tstatistic, pvalue] = stats.ttest_ind(Error_logreg,Error_dectree)\n",
    "# and test if the p-value is less than alpha=0.05. \n",
    "z = (Error_model_1-Error_model_2)\n",
    "zb = z.mean()\n",
    "nu = K-1\n",
    "sig =  (z-zb).std()  / np.sqrt(K-1)\n",
    "alpha = 0.05\n",
    "\n",
    "zL = zb + sig * stats.t.ppf(alpha/2, nu);\n",
    "zH = zb + sig * stats.t.ppf(1-alpha/2, nu);\n",
    "\n",
    "if zL <= 0 and zH >= 0 :\n",
    "    print('Model 1 and Model 2 are not significantly different')        \n",
    "else:\n",
    "    print('Model 1 and Model 2 are significantly different.')\n",
    "print(zL)\n",
    "print(zH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two level cross validation for KNN - Greta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Crossvalidation for KNN\n",
    "# Create crossvalidation partition for evaluation\n",
    "K_outer = 5\n",
    "K_inner = 5\n",
    "CV_outer = model_selection.KFold(n_splits=K_outer,shuffle=True)\n",
    "#CV = model_selection.StratifiedKFold(n_splits=K)\n",
    "index_min_lst = []\n",
    "min_indices = []\n",
    "error_outer = [] # List for the errors in outer CV fold\n",
    "\n",
    "# Initialize variables\n",
    "#Error_logreg = np.empty((K_outer,1))\n",
    "#Error_KNN_inner = np.empty((K_inner,3))\n",
    "#Error_NaÃ¯veB = np.empty((K_outer,1))\n",
    "#n_tested=0\n",
    "\n",
    "K_KNN = [1, 2, 3] # Change here for different nearest neighbour crossvalidation\n",
    "\n",
    "k=0\n",
    "classifier_lst = []\n",
    "\n",
    "for train_outer_index, test_outer_index in CV_outer.split(X_classification,y_classification):\n",
    "    print('CV-fold {0} of {1}'.format(k+1,K_outer))\n",
    "    k += 1\n",
    "    \n",
    "    # extract training and test set for current CV fold\n",
    "    X_train_outer = X_classification[train_outer_index,:]\n",
    "    y_train_outer = y_classification[train_outer_index]\n",
    "    X_test_outer = X_classification[test_outer_index,:]\n",
    "    y_test_outer = y_classification[test_outer_index]\n",
    "    \n",
    "    CV_inner = model_selection.KFold(n_splits=K_inner,shuffle=True)\n",
    "    \n",
    "    kk=0\n",
    "    for train_inner_index, test_inner_index in CV_inner.split(X_train_outer,y_train_outer):\n",
    "        print('Inner CV-fold {0} of {1}'.format(kk+1,K_inner))\n",
    "\n",
    "        # extract training and test set for current CV fold\n",
    "        X_train_inner = X_train_outer[train_inner_index,:]\n",
    "        y_train_inner = y_train_outer[train_inner_index]\n",
    "        X_test_inner = X_train_outer[test_inner_index,:]\n",
    "        y_test_inner = y_train_outer[test_inner_index]\n",
    "        \n",
    "#         print(len(X_train_inner))\n",
    "#         print(len(y_train_inner))\n",
    "        \n",
    "        for count, value in enumerate(K_KNN):\n",
    "            dist=1\n",
    "    \n",
    "            knclassifier = KNeighborsClassifier(n_neighbors=value, p=dist);\n",
    "            knclassifier.fit(X_train_inner, y_train_inner);\n",
    "            classifier_lst.append(knclassifier)\n",
    "            \n",
    "            y_KNN = knclassifier.predict(X_test_inner);\n",
    "            errorKNN_inner = 100*(y_KNN!=y_test_inner).sum().astype(float)/len(y_test_inner)  \n",
    "            index_min_lst.append(errorKNN_inner) #Append the error values to a list\n",
    "            \n",
    "        # Find the KNN value with least error value\n",
    "        index_min = np.argmin(index_min_lst) #Find the index of the minimum error value\n",
    "        min_indices.append(index_min) \n",
    "        \n",
    "        index_min_lst = [] # Clear for next CV fold\n",
    "        \n",
    "        kk += 1\n",
    "\n",
    "        counts = np.bincount(min_indices) # Count which index appears the maximum number of times = i.e. the must be the least error value\n",
    "        top_count = np.argmax(counts)\n",
    "        # Only use the count from the last iteration! \n",
    "        optimal_K = K_KNN[top_count]\n",
    "        \n",
    "    print('The index of optimal KNN value is: ' + str(top_count))\n",
    "\n",
    "    print('The optimal KNN value across inner CV folds is: ' + str(optimal_K))\n",
    "    \n",
    "    knclassifierOuter = KNeighborsClassifier(n_neighbors=optimal_K, p=dist); #Uses optimal_K, which was found in the inner CV loop\n",
    "    knclassifierOuter.fit(X_train_outer, y_train_outer);\n",
    "            \n",
    "    y_KNN_outer = knclassifier.predict(X_test_outer);\n",
    "    errorKNN_outer = 100*(y_KNN_outer!=y_test_outer).sum().astype(float)/len(y_test_outer)\n",
    "    error_outer.append(errorKNN_outer)\n",
    "    print('Errors for each outer CV fold: ' + str(error_outer))\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Crossvalidation for KNN\n",
    "# Create crossvalidation partition for evaluation\n",
    "K_outer = 5\n",
    "K_inner = 5\n",
    "CV_outer = model_selection.KFold(n_splits=K_outer,shuffle=True)\n",
    "#CV = model_selection.StratifiedKFold(n_splits=K)\n",
    "errorKNN_inner_GT=[]\n",
    "errorKNN_outer_GT = []\n",
    "index_min_lst = []\n",
    "min_indices = []\n",
    "error_outer = []\n",
    "\n",
    "# Initialize variables\n",
    "#Error_logreg = np.empty((K_outer,1))\n",
    "#Error_KNN_inner = np.empty((K_inner,3))\n",
    "#Error_NaÃ¯veB = np.empty((K_outer,1))\n",
    "#n_tested=0\n",
    "\n",
    "K_KNN = range(1,41)\n",
    "\n",
    "k=0\n",
    "classifier_lst = []\n",
    "\n",
    "for train_outer_index, test_outer_index in CV_outer.split(X_classification,y_classification):\n",
    "    print('CV-fold {0} of {1}'.format(k+1,K_outer))\n",
    "    k += 1\n",
    "    \n",
    "    # extract training and test set for current CV fold\n",
    "    X_train_outer = X_classification[train_outer_index,:]\n",
    "    y_train_outer = y_classification[train_outer_index]\n",
    "    X_test_outer = X_classification[test_outer_index,:]\n",
    "    y_test_outer = y_classification[test_outer_index]\n",
    "    \n",
    "    CV_inner = model_selection.KFold(n_splits=K_inner,shuffle=True)\n",
    "    \n",
    "    kk=0\n",
    "    for train_inner_index, test_inner_index in CV_inner.split(X_train_outer,y_train_outer):\n",
    "        print('Inner CV-fold {0} of {1}'.format(kk+1,K_inner))\n",
    "\n",
    "        # extract training and test set for current CV fold\n",
    "        X_train_inner = X_train_outer[train_inner_index,:]\n",
    "        y_train_inner = y_train_outer[train_inner_index]\n",
    "        X_test_inner = X_train_outer[test_inner_index,:]\n",
    "        y_test_inner = y_train_outer[test_inner_index]\n",
    "        \n",
    "#         print(len(X_train_inner))\n",
    "#         print(len(y_train_inner))\n",
    "        \n",
    "        for count, value in enumerate(K_KNN):\n",
    "            dist=1\n",
    "    \n",
    "            knclassifier = KNeighborsClassifier(n_neighbors=value, p=dist);\n",
    "            knclassifier.fit(X_train_inner, y_train_inner);\n",
    "            classifier_lst.append(knclassifier)\n",
    "            \n",
    "            y_KNN = knclassifier.predict(X_test_inner);\n",
    "            errorKNN_inner_GT = 100*(y_KNN!=y_test_inner).sum().astype(float)/len(y_test_inner)  \n",
    "            index_min_lst.append(errorKNN_inner_GT) #Append the error values to a list\n",
    "            \n",
    "        # Find the KNN value with least error value\n",
    "        index_min = np.argmin(index_min_lst) #Find the index of the minimum error value\n",
    "        min_indices.append(index_min) \n",
    "        \n",
    "        index_min_lst = [] # Clear for next CV fold\n",
    "        \n",
    "        kk += 1\n",
    "\n",
    "    print(min_indices)\n",
    "    counts = np.bincount(min_indices) # Count which index appears the maximum number of times = i.e. the must be the least error value\n",
    "    top_count = np.argmax(counts)\n",
    "    print('The index of optimal KNN value is: ' + str(top_count))\n",
    "    \n",
    "    # Only use the count from the last iteration! \n",
    "    optimal_K = K_KNN[top_count]\n",
    "    print('The optimal KNN value across inner CV folds is: ' + str(optimal_K))\n",
    "    \n",
    "    knclassifierOuter = KNeighborsClassifier(n_neighbors=1, p=dist);\n",
    "    knclassifierOuter.fit(X_train_outer, y_train_outer);\n",
    "            \n",
    "    y_KNN_outer = knclassifier.predict(X_test_outer);\n",
    "    errorKNN_outer_GT = 100*(y_KNN_outer!=y_test_outer).sum().astype(float)/len(y_test_outer)\n",
    "    error_outer.append(errorKNN_outer_GT)\n",
    "    print(error_outer)\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
